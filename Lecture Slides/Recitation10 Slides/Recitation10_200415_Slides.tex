\documentclass{beamer}

\mode<presentation>
\usetheme{Madrid}
\definecolor{Columbia}{RGB}{185,217,235}
\definecolor{Columbia2}{RGB}{0,51,160}
\definecolor{Columbia3}{RGB}{0,114,206}
\setbeamercolor{title}{fg=Columbia2}
\setbeamercolor{frametitle}{fg=Columbia2}
\setbeamercolor{block title}{bg=Columbia, fg=Columbia2}
%\setbeamercolor{block body}{fg=Columbia2}
\setbeamercolor{structure}{fg=Columbia}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Columbia2}
\setbeamercolor{subitem}{fg=Columbia2}
\setbeamercolor{section in toc}{fg=Columbia}
\setbeamercolor{description item}{fg=Columbia}
\setbeamercolor{caption name}{fg=Columbia}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{hyperref}

%\hypersetup{
%colorlinks=true,
%linkcolor=black,
%filecolor=green, 
%urlcolor=blue,
%}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[page number]
\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{headline}{}
\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Econometrics 2]{Introduction to Econometrics 2: Recitation 10} % Change this regularly
\author{Seung-hun Lee}
\institute{Columbia University}

\date{April 15th, 2020\footnote{Fun fact: After this recitation, my gov't mandated self-isolation is officially over.}}

\begin{document}
\begin{frame}
\titlepage
\end{frame}
%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Semiparametric Regression}
Semiparametric Regression: Framework
\begin{itemize}
\item Semiparametrics can be thought of as a middle ground between nonparametric and parametric regression.
\item Suppose that we partition the covariates into two unoverlapping spaces - $X$ and $W$. 
\item Also assume that $E(\epsilon|X,W)=0$. 
\item One example of a semiparametric regression is a \textbf{partially linear regression} which has the following form
\[
Y_i = X_i\beta + g(W_i)+\epsilon_i 
\]
where $\beta\in\dim{(X_i)} $ represents a coefficient for the linearly regressed terms and $g(\cdot)$ is a nonparametric portion of the regression.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Semiparametric Regression}
Semiparametric Regression: Estimation
\begin{itemize}
\item To estimate $\beta$, we use the fact that
\begin{align*}
E[Y_i|W_i]& = E[X_i|W_i]\beta +g(W_i)
\end{align*}\par
\item Given this, we can write
\[
Y_i-E[Y_i|W_i]=\{X_i-E[X_i|W_i]\}\beta +\epsilon_i
\]\par
\item Then we follow this procedure
\begin{enumerate}
\item Nonparametrically estimate $E[X_i|W_i]$ and $E[Y_i|W_i]$. Then define $\tilde{X}_i=X_i-\widehat{E}[X_i|W_i]$ and $\tilde{Y}_i = Y_i-\widehat{E}[Y_i|W_i]$, where $\widehat{E}$ are nonparametric estimators
\item Regress $\tilde{Y}$ onto $\tilde{X}$ to get an estimate of $\beta$
\item We can estimate $g(\cdot)$ by nonparametrically regressing $Y_i-X_i\hat{\beta}$ onto $W_i$
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Semiparametric Regression}
Semiparametric Regression: Estimation
\begin{itemize}
\item $\beta$ follows the properties of parametric estimators (Converges at rate $n^{-1/2}$ regardless of the dimensions of $X_i, W_i$)
\item Estimating $g(\cdot)$ follows the same properties as nonparametric estimators  (Slower convergence rate, which becomes even slower with more dimensions of $W_i$)
\item Caveat: Identification of $\beta$ requires an exclusion restriction
\begin{itemize}
\item None of the components in $X_i$ is perfectly predictable by $W_i$ components ($X_i\neq E[X_i|W_i]$)
\item This would effectively rule out including a constant in the $X_i$ part of the regression
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Semiparametric Regression}
Examples
\begin{itemize}
\item Horowitz, Lee (2002): The paper shows that semiparametrics allow more flexibility than parametric modeling and more precision than nonparametric models. 
\begin{itemize}
\item For fun (at least for a baseball nerd like me), this paper tests this idea on a data of salaries, runs, tenure of baseball players in 1987. 
\end{itemize}
\item Ucal et al (2010): This paper analyzes whether and to what extent the inflow of FDI is affected before and after the occurence of a financial crisis in developing countries using generalized partial linear models. 
\begin{itemize}
\item The results indicate that FDI inflows decrease in the years after a financial crisis and an upturn in FDI inflows the year before a financial crisis hit the country.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Finding Causality
\begin{itemize}
\item We are looking to see whether $X$ causes $y$
\item $cov(X,Y)\neq 0$ is not enough because..
\begin{itemize}
\item $X$ do cause $Y$, which is good for us. But..
\item $Y$ could also cause $X$. So there is a reverse causality bias here
\item $Z$ mutually affects $X$ and $Y$. This is an omitted variable bias and leads to nonzero correlation even if $X$ and $Y$ has no connection whatsoever. 
\end{itemize} 
\item The key issue is the assignment of the treatment, which could be..
\begin{itemize}
\item \textbf{Random Assignment}: This would be a case when we can guarantee that the assignment to the treatment arms (treatment and control) are determined by chance
\item \textbf{Selection on Observables}: The treatment assignment is effectively random once we condition on some observable covariates
\item \textbf{Selection on Unobservables}: The assignment depends fundamentally on unobservables, or in other words, we cannot break down the dependence structure of assignment using observed variables. 
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Theoretical Setup
\begin{itemize}
\item Consider a binary treatment variable - whether individual $i$ received a treatment or not
\item  Define a variable $D_i$ s.t.
\[
D_i = \begin{cases} 1 & \text{If treated} \\ 0 & \text{If not treated}\end{cases}
\]
\item $i$ indexes the unit of the treatment
\item For each unit $i$, there are two possible outcomes. 
\begin{itemize}
\item The outcome without treatment,  $Y_i(0)$
\item The outcome with treatment, $Y_i(1)$
\end{itemize}
\item This can be seen as a counterfactual framework - if we get $Y_i(1)$ for unit $i$, we cannot get $Y_i(0)$ and vice versa. 
\item We always have a missing data problem in this regard
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Theoretical Setup
\begin{itemize}
\item A mathematical way to treat this is 
\[
Y_i = D_iY_i(1) + (1-D_i)Y_i(0)
\]
\item The left hand side is an outcome for unit $i$, treated or untreated. This is observed for everyone
\item The right hand side is meant to capture that the observed outcome for individual $i$ is either one of $Y_i(1)$ or $Y_i(0)$. 
\item This framework is sometimes called \textbf{potential outcome framework} 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Outcome of interest
\begin{itemize}
\item We are interested in how an outcome for unit $i$ changes between treatment and control. In other words,
\[
TE_i = Y_i(1)-Y_i(0)
\]
\item Another parameter of interest could be the treatment effect averaged over those who share the common covariate value, which is
\[
TE(x) = E(TE_i|X_i=x)
\]
\item We could also be interested in the treatment effect averaged over the population. This is the average treatment effect, defined as
\[
ATE=E(TE_i)=E(Y_i(1)-Y_i(0))
\]
\item And others: ATT, ATUT, etc. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Problem caused by missing data 
\begin{itemize}
\item  We need to make an assumption about the things we cannot observe.
\item Take the ATE for example. We can write
\footnotesize{\begin{align*}
E[Y_i(1)-Y_i(0)] & = E[Y_i(1)]-E[Y_i(0)]\\
&=\{\Pr(D_i=1)E[Y_i(1)|D_i=1]+(1-\Pr(D_i=1))E[Y_i(1)|D_i=0]\}\\
&-\{\Pr(D_i=1)E[Y_i(0)|D_i=1]+(1-\Pr(D_i=1))E[Y_i(0)|D_i=0]\}
\end{align*}}\normalsize
\item We can get what $E[Y_i(1)|D_i=1]$ and $E[Y_i(0)|D_i=0]$ are
\footnotesize{\begin{align*}
E[Y_i|D_i=1]&=E[1\cdot Y_i(1)-(1-1)\cdot Y_i(0)|D_i=1]=E[Y_i(1)|D_i=1]\\
E[Y_i|D_i=0]&=E[0\cdot Y_i(1)-(1-0)\cdot Y_i(0)|D_i=0]=E[Y_i(0)|D_i=0] \tag{\text{TE}}\label{TE}
\end{align*}}\normalsize
\item We can get $E[Y_i(1)|D_i=1]$ and $E[Y_i(0)|D_i=0]$ from the data - take the expected value of observed $Y_i$ conditional on $D_i=1 \ \text{and}\ 0$. 
\item We cannot do the same for $E[Y_i(1)|D_i=0]$ and $E[Y_i(1)|D_i=0]$, forcing us to make assumptions
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Characterize TE in an econometrics-friendly way
\begin{itemize}
\item  Define the counterfactual outcomes as
\[
Y_i(D_i=d)=\mu(X_i,d)+\epsilon_i(d)
\]
where $d$ can take either $0, 1$. 
\item What we want to learn involves understanding the joint distribution of the variables $Y_i(0)$ and $Y_i(1)$. 
\item Take the treatment effect at $X_i=x$, written as
\[
TE(x)=\mu(x,1)-\mu(x.0)
\]
\item Interpretation: If we shift everyone with $X_i=x$ from the control group to treatment, the average outcome increases by $TE(x)$.
\item We can also calculate ATE as
\footnotesize{\begin{align*}
ATE&=E[Y_i(1)-Y_i(0)]=E[\mu(X_i,1)-\mu(X_i,0)+\epsilon_i(1)-\epsilon_i(0)]\\
&=E\left[E[\mu(X_i,1)-\mu(X_i,0)+\epsilon_i(1)-\epsilon_i(0)|X_i]\right]=E\left[E[\mu(X_i,1)-\mu(X_i,0)|X_i]\right]\\
&=E[\mu(X_i,1)-\mu(X_i,0)]=E[TE(X_i)]
\end{align*}}\normalsize
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Random Assignments
\begin{itemize}
\item  A \textbf{random assignment} assumes that the outcome is independent of the treatment status. More formally
\[
(Y_i(0), Y_i(1))\perp \!\!\! \perp D_i \ \tag{\text{RA}}\label{RA}
\]
\item This implies that (similarly for $E[Y_i(0)]$)
\[
E[Y_i(1)]=E[Y_i(1)|D_i=1]=E[Y_i(1)|D_i=0]
\]
\item Now what we are doing is to equate
\footnotesize{\begin{gather*}
E[Y_i|D_i=1]=E[Y_i(1)|D_i=1]=E[Y_i(1)|D_i=0]\\
 E[Y_i|D_i=0]=E[Y_i(0)|D_i=0]=E[Y_i(0)|D_i=1]
\end{gather*}}\normalsize
\item This allows us to rewrite the ATE as 
\footnotesize{\begin{align*}
E[Y_i(1)-Y_i(0)]&=E[Y_i(1)]-E[Y_i(0)]\\
&=E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=0] \ (\because \ref{RA})\\
&=E[Y_i|D_i=1]-E[Y_i|D_i=0] \ (\because \ref{TE})
\end{align*}}\normalsize
Therefore, we can estimate ATE by mapping $E[Y_i(1)]$ to $E[Y_i|D_i=1]$, $E[Y_i(0)]$ to $E[Y_i|D_i=0]$. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Conditional Independence Assumption
\begin{itemize}
\item Assume conditional on $X_i$, the outcomes and $D_i$ are independent.
\item Formally, we can write
\[
 (Y_i(0), Y_i(1))\perp \!\!\! \perp D_i|X_i \ \tag{\text{CIA}}\label{CIA}
\]
\item Alternatively: Define $D_i$ as
\[
D_i = 1(u_i<p(X_i))
\]
$u_i\equiv U[0,1]$ determines selection and $p(X_i)$ can be interpreted as a propensity score
\item Then we can also say
\[
(\epsilon_i(1), \epsilon_i(0))\perp \!\!\! \perp u_i|X_i \tag{\text{CIA2}}\label{CIA2}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
Conditional Independence Assumption
\begin{itemize}
\item Why?
\footnotesize{\begin{align*}
E[Y_i(1)|X_i=x]&=E[Y_i(1)|D_i=1,X_i=x]=E[Y_i(1)|D_i=0,X_i=x] \ (\because \ref{CIA})\\
&\implies E[\mu(x,1)+\epsilon_i(1)|D_i=1,x]=E[\mu(x,1)+\epsilon_i(1)|D_i=0,x]\\
&\implies E[\mu(x,1)|D_i=1,x]+E[\epsilon_i(1)|D_i=1,x]\\
&=E[\mu(x,1)|D_i=0,x]+E[\epsilon_i(1)|D_i=0,x]\\
&\implies E[\epsilon_i(1)|D_i=1,x]=E[\epsilon_i(1)|D_i=0,x]\\
&\implies E[\epsilon_i(1)|u_i<p(x),x]=E[\epsilon_i(1)|u_i>p(x),x]\\
&\implies (\epsilon_i(1), \epsilon_i(0))\perp \!\!\! \perp u_i|X_i \tag{\text{CIA2}}\label{CIA2}
\end{align*}}\normalsize
\item The caveat, however, is that $p(x)\in(0,1)$. 
\begin{itemize}
\item If $p(x)=1$, then for every possible $u_i$, $D_i=1$ - everyone gets treated and no meaningful statement can be made about the $D_i=0$ case. 
\item In some textbooks, this is also known as \textbf{overlap} assumption
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA
\begin{itemize}
\item We can write
\scriptsize{\begin{align*}
E[Y_i|1, x]-E[Y_i|0, x]&=E[\mu(x,1)+\epsilon_i(1)|1,x]-E[\mu(x,0)+\epsilon_i(0)|0, x]\\
&=E[\mu(x,1)|1, x]+E[\epsilon_i(1)|1, x]-E[\mu(x,0)|0, x]-E[\epsilon_i(0)|0, x]\\
&=\mu(x,1)+E[\epsilon_i(1)|x]-\mu(x,0)-E[\epsilon_i(0)|x] \ (\because \ref{CIA2})\\
&=\mu(x,1)-\mu(x,0)
\end{align*}}\normalsize
\item Note that 
\scriptsize{\begin{align*}
E[Y_i(1)-Y_i(0)|x] &= E[Y_i(1)|x]- E[Y_i(0)|x]\\
&=(\Pr(1|x)\cdot E[Y_i(1)|1,x]+\Pr(0|x)\cdot E[Y_i(1)|0,x])\\
&-(\Pr(1|x)\cdot E[Y_i(0)|1,x]+\Pr(0|x)\cdot E[Y_i(0)|0,x])\\
&=  E[Y_i(1)|1,x]- E[Y_i(0)|0,x] \ (\because \ref{CIA}) \\
&=  E[Y_i|1, x]-E[Y_i|0, x]
\end{align*}}\normalsize
\item Thus, we can back out the ATE for $X_i=x$ using the observables by mapping $ E[Y_i(1)|x]$ to $ E[Y_i|1, x]$ and $E[Y_i(0)|x]$ to $ E[Y_i|0, x]$ 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Regression Adjustments
\begin{itemize}
\item This is called a \textbf{regression adjustment} method. 
\item The treatment effect for $X_i=x$ using a regression adjustment can be obtained by utilizing the fact that 
\[
\mu(x,1)=E[Y_i|D_i=1, X_i=x], \ \mu(x,0)=E[Y_i|D_i=0, X_i=x]
\]
and regressing on the subsample of each treatment arm to get $\hat{\mu}(x,1)$ and $\hat{\mu}(x,0)$. Thus
\[
\frac{1}{N}\sum_{i=1}^N(\hat{\mu}(x,1)-\hat{\mu}(x,0))
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Inverse Probability Weight
\begin{itemize}
\item We can obtain the ATE using a different approach. 
\item This is an \textbf{inverse probability weighting}. The steps are as follows
\begin{enumerate}
\item Estimate the propensity score $p(X_i)$ by computing
\[
\hat{p}_n(X_i)=\Pr(D_i=1|X_i=x)
\]
\item Use the magic formula to estimate $E(TE(x)|x\in A)$: That is,
\[
\frac{\sum_{D_i=1,x_i\in A}a_iY_i}{\sum_{D_i=1,x_i\in A}a_i}- \frac{\sum_{D_i=0,x_i\in A}b_iY_i}{\sum_{D_i=0,x_i\in A}b_i}
\]
where $a_i=\frac{1}{\hat{p}_n(x_i)}, b_i=\frac{1}{1-\hat{p}_n(x_i)}$. The above can also be written as
\[
\frac{\sum_{i=1,x_i\in A}^N\frac{D_iY_i}{\hat{p}(X_i)}}{\sum_{i=1,x_i\in A}^N\frac{D_i}{\hat{p}(X_i)}}-\frac{\sum_{i=1,x_i\in A}^N\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}}{\sum_{i=1,x_i\in A}^N\frac{1-D_i}{1-\hat{p}(X_i)}}\]
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Inverse Probability Weight
\begin{itemize}
\item  Notice that
\begin{align*}
D_iY_i& = D_i(D_iY_i(1)+(1-D_i)Y_i(0))=D_iY_i(1)\\
(1-D_i)Y_i&=(1-D_i)Y_i(0)
\end{align*}
\item Thus, we have
\footnotesize{\begin{align*}
E\left[\frac{D_iY_i}{p(X_i)}\right]&=E\left[E\left[\frac{D_iY_i(1)}{p(X_i)}|X_i\right]\right]\\
&=E\left[\frac{E[D_i|X_i]E[Y_i(1)|X_i]}{p(X_i)}\right]\\
&=E\left[\frac{p(X_i)E[Y_i(1)|X_i]}{p(X_i)}\right]=E[E[Y_i(1)|X_i]]=E[Y_i(1)]
\end{align*}}\normalsize
\item Thus,  $E\left[\frac{D_iY_i}{p(X_i)}\right]=E[Y_i(1)], E\left[\frac{D_iY_i}{p(X_i)}|X_i\right]=E[Y_i(1)|X_i]$. 
\item We can make the similar arguments for $Y_i(0)$. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Inverse Probability Weight
\begin{itemize}
\item Thus, the ATE for $X_i\in A$ can be written as
\[
\frac{1}{N}\sum_{i=1}^N\left(\frac{D_i Y_i}{p(X_i)}-\frac{(1-D_i)Y_i}{1-p(X_i)}\right) \ \forall X_i\in A
\]
\item In most cases, the propensity score should be estimated.
\item  So we use the estimator involving the $\hat{p}$, which is 
\[
\frac{\sum_{i=1,x_i\in A}^N\frac{D_iY_i}{\hat{p}(X_i)}}{\sum_{i=1,x_i\in A}^N\frac{D_i}{\hat{p}(X_i)}}-\frac{\sum_{i=1,x_i\in A}^N\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}}{\sum_{i=1,x_i\in A}^N\frac{1-D_i}{1-\hat{p}(X_i)}}\]
\item  It is said that normalizing the weights to one in finite samples improves the mean squared error properties of the estimator (Imbens, Rubin (2019) - \textit{Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction})
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Matching
\begin{itemize}
\item Impute the values for something we cannot get from the data: Namely,  $Y_i(0)$ for those in $D_i=1$ and $Y_i(1)$ for those in $D_i=0$. 
\item In other words, you try to find a `close' match for a particular unit $i$ in a different treatment arm. 
\item When we say we find $k$-closest neighbors for unit $i$ in $D_i=0$, we find $k$ individuals in $D_i=1$ that has close traits ($X_i$) to individual $i$, or $k$ individuals with the smallest values of $||x_j - x_i||$.
\item Then, we construct a counterfactual $Y_i(1)$ by taking a (weighted) average over the $Y_i$'s of the $k$ individuals found in the other group.
\item The treatment effect would than be $Y_i(1)$ -$Y_i$, where $Y_i(1)$ is calculated as in previous sentence. \par
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Matching
\begin{itemize}
\item Let's try with $k=1$ - we find the one individual in the opposite treatment arm that has the similar value of $X_i$.
\item Then, the average treatment effect can be written as
\[
\frac{1}{N}\sum_{i=1}^N(\hat{Y}_i(1)-\hat{Y}_i(0))
\]
where
\footnotesize{\[
\hat{Y}_i(1)=\begin{cases} Y_i & (D_i=1) \\ \text{imputed value} & (D_i=0)\end{cases}, \ \hat{Y}_i(0)=\begin{cases} \text{imputed value} & (D_i=1) \\ Y_i & (D_i=0)\end{cases}
\]}\normalsize
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: Matching
\begin{itemize}
\item There are some caveat in this approach
\begin{itemize}
\item The covariate $X_i$ that is used to find the closest match in the other treatment arm should not affect the assignment of the treatment. %In other words, if the assignment of treatment vs. control depends on some subset of $X_i$, you should not use that subset as a standard for finding the closest match.
\item The overlap condition becomes critical. If it is not satisfied, i.e. for some $X_i$, $p(X_i)=1$ or $0$, then we are unable to find a closest match in the other treatment arm because for individuals with that covariate value, all of them are either in control or treatment group and not spread around. 
\item Who are the neighbors? The answer might depend on what distance measure we use. Moreover, how many of those in the treatment can be considered neighbors? There is a trade-off in the sense that using larger $k$ would force us to put someone who is not `close' as neighbors and using small $k$ may cause difficulty in imputing counterfactual values. 
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: DID
\begin{itemize}
\item This involves a specific framework where we can clearly define a `before and after' - denoted as $t_0$ and $t_1$. 
\item No one is treated at $t_0$ but there is a subset of people ($G_i=1$) that are treated at $t_1$. Those in $G_i=0$ are never treated in either time period. 
\item If we define
\[
Y_{i,t}=G_iY_{i}(1,t)+(1-G_i)Y_{i}(0,t) \ \ (t\in\{t_0, t_1\})
\]
and impose
\[
Y_i(G_i, t_0)=Y_{i}(t_0) \ \ \text{for both }G_i=1 \ \text{and }G_i=0 
\]
then we will be able to observe $(Y_{i,t_1}, Y_{i,t_0},G_i. X_i)$ for every unit $i$. 
\item What we do not observe is $Y_i(1,t_1)$ for those in $G_i=0$ and $Y_i(0,t_1)$ for those in $G_i=1$.  
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: DID
\begin{itemize}
\item The analogue to the conditional independence assumption in this context is a \textbf{parallel trend assumption}, defined as
\[
(Y_i(1,t_1)-Y_i(t_0), Y_i(0,t_1)-Y_i(t_0)) \perp\!\!\! \perp G_i|X_i
\]
\item To see why they are equal, consider a setting where $D_i=G_i$ and write
\footnotesize{\begin{align*}
Y_i = Y_{i,t_1}-Y_{i,t_0}&=D_i(\underbrace{Y_i(1,t_0)-Y_i(1,t_0)}_{Y_i(1)})+(1-D_i)(\underbrace{Y_i(0,t_1)-Y_i(0,t_0)}_{Y_i(0)})\\
&=D_iY_i(1) + (1-D_i)Y_i(0)
\end{align*}}\normalsize
\item Therefore, we can rewrite the parallel trend assumption as
\[
(Y_i(1), Y_i(0)) \perp\!\!\! \perp D_i|X_i
\]
\item Testing: Select a time period $\tilde{t}<t_0$ and find out if the difference $y_i(t_0)-y_i(\tilde{t})$ is independent with $G_i$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Treatment Effects}
TE under CIA: DID
\begin{itemize}
\item To  apply this in regression, we can write
\[
Y_{it}=\beta_0(X_i)+\beta_1(X_i)\cdot1(t=t_1)+\beta_2(X_i)\cdot G_i + \beta_3(X_i)\cdot G_i\cdot1(t=t_1)+\epsilon_{it}
\]
where $X_i$ is a set of covariates, which can include a constant. 
\item In this context, the treatment effect for $X_i=x$ would be
\footnotesize{\begin{align*}
TE(x)&=E[Y_i(1)-Y_i(0)|X_i=x]\\
&=E[(Y_i(1,t_1)-Y_i(1,t_0))-(Y_i(0,t_1)-Y_i(0,t_0))|X_i=x]\\
&=x\cdot E\{[(\beta_0+\beta_1+\beta_2+\beta_3)-(\beta_0+\beta_2)]-[(\beta_0+\beta_1)-(\beta_0)]|X_i=x\}\\
&=x\cdot E[(\beta_1+\beta_3)-\beta_1|X_i=x]\\
&=\beta_3 x
\end{align*}}\normalsize
So $\beta_3$ would be our parameter of interest.
\end{itemize}
\end{frame}
%%%%%%%%%%%
\end{document}
