
\chapter{Panel Data}
\section{Basic Framework}
To motivate the discussion going forward, consider the following setting. Let $y$ and $x=(x_1,x_2,...,x_k)$ be the observable factors. Denote $\alpha$ as an unobservable random variable that is incorporated into the data generating process additively. Then, we can write $E[y|x,\alpha]$ as 
\[
E[y|x,\alpha] = x'\beta+\alpha
\]
We are interested in estimating $\beta$. To see whether we can find a consistent estimator for $\beta$, we need to see how $\alpha$ is correlated with $x$. If $\alpha$ is independent from $x$, the it is not different from the idiosyncratic error and $\beta$ is consistently estimable. However, in most cases, $\alpha$ could be correlated with $x$. If this is the case, then we cannot find a consistent estimator for $\beta$ unless we find a perfect proxy for $\alpha$. However, if $\alpha$ is fixed across time for an individual and we have access to panel data, we can address this issue. Most discussion of panel data in class will focus on $\alpha$ representing an unobservable trait inherent for individuals.   \par
In a panel data setting, the data now has two dimensions - dimensions across different unit of observation $i$ and across time $t$. In maths, 
\[
(y_{it},x_{it}) \ \text{where } i=1,...,n, \ \text{and }  t=1,...,T
\]
For our discussion, we will fix $T$ and let the $(y_{it},x_{it})$ be IID across $i=1,...,n$. Unless otherwise state, the asymptotics will be carried out by extending $n$ to infinity. \par
To be more concrete, we can write the data generating process as
\[
y_{it}= x_{it}'\beta+\alpha_i+u_{it} \ \ \footnote{Note that we can be interested in the time fixed effect, usually denoted as $\gamma_t$. The analysis of such effects is largely similar to individual fixed effects.}
\]
where $x_{it}$ is an observable variable that varies across individuals and time periods. $\alpha_i$ is the \textbf{individual (fixed) effect} that is unobservable. As such, we can define $v_{it}=\alpha_i+u_{it}$ and rewrite the data generating process as
\[
y_{it}= x_{it}'\beta+v_{it}
\]
Depending on whether $\alpha_i$ is correlated with $x_{it}$ or not, we can categorize $\alpha_i$ into the following
\begin{itemize}
\item \textbf{Random Effects}: There is no correlation between the observables and $\alpha_i$
\item \textbf{Fixed Effects}: The correlation between $x_{it}$ and $\alpha_i$ is nonzero. 
\end{itemize}
In case you are looking into some old textbooks, the categorization is slightly different. If $\alpha_i$ is considered to be a parameter to be estimated, the old textbooks refers to this as fixed effects. If $\alpha_i$ is a random variable, it was called a random effect. Note that in the above discussion, $\alpha_i$ is random variable in both fixed and random effects. You should see that what matters now is whether $\alpha_i$ is correlated with $x_{it}$ or not. \par
\section{Strict Exogeneity}
Before moving further, the following assumption is required to show whether the panel estimates are consistent or not
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}[Strict Exogeneity] We say that the regressor $x_{it}$ is \textbf{strictly exogenous} with respect to $u_{it}$ if
\[
E[y_{it}|x_{i1},..,x_{iT},\alpha_i]=E[y_{it}|x_{it},\alpha_i]
\]
which boils down to
\[
E[u_{it}|x_{i1},..,x_{iT},\alpha_i]=E[u_{it}|x_{it},\alpha_i]=0
\]
\end{assumption}
\end{mdframed} \par
In words, strict exogeneity implies that once $x_{it}, \alpha_i$ is controlled for, $x_{is}, s\neq t$ has no partial effect on $y_{it}$. The above condition also implies that 
\[
E[x_{is}u_{it}]=0 \ (s,t)=1,..,T
\]
while this itself is a weaker condition that $E[u_{it}|x_{i1},..,x_{iT},\alpha_i]=0$, it is sufficient for most of the consistency analysis. However, it is stronger than the exogeneity assumptions of the previous chapters. Previously it was enough for the regressors to be exogenous with the contemporaneous $e_{it}$. Now we assume $x_{it}$ to be exogenous with all error terms. \footnote{Yes, you may think that this assumption is extreme. Later in the panel data discussion, we will talk about weaker version of this assumption - predetermined regressors. }\par
Also note that this condition is silent about the correlation between $x_{it}$ and $\alpha_i$. 
\section{Pooled OLS (POLS)}
Pooled OLS estimation practically ignores the panel structure of the data and involves applying a typical OLS estimation on the data generating process with time and individual dimension. Given that our model is
\small{\[
y_{it}= x_{it}'\beta+\underbrace{\alpha_i+u_{it}}_{=v_{it}}
\]}\normalsize
The estimator can be written as
\small{\[
\hat{\beta}_{POLS}=\left(\sum_{i=1}^n\sum_{t=1}^T x_{it}x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T x_{it}y_{it}
\]}\normalsize
Under the situation where $cov(x_{it}.\alpha_i)\neq 0$, we can see that $\hat{\beta}_{POLS}$ is inconsistent. Note that we can re-write the POLS estimator in terms of the true $\beta$ 
\footnotesize{\[
\hat{\beta}_{POLS}-\beta = \left(\sum_{i=1}^n\sum_{t=1}^T x_{it}x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T x_{it}v_{it}
\]}\normalsize
The trick is to show that $E[x_{it}v_{it}]\neq0$ for $t=1,..,T$. Since I can write $E[x_{it}(\alpha_i+u_{it})]$ instead, it can be seen that unless $cov(x_{it},\alpha_i)=0$, the $E[x_{it}\alpha_i]$ term will remain. Thus, POLS estimator is not consistent. \par
\section{First Difference (FD)}
For this to work, we need $T\geq 2$. To obtain the first difference estimator, we need to subtract the original data generating process by one lag of $y_{it}$. Or
\[
\Delta y_{it}= \Delta x_{it}'\beta+\Delta u_{it}\ \ (i=1,...,n \ \text{and }t=2,..,T)
\]
where $\Delta y_{it} = y_{i,t}-y_{i,t-1}$ and similarly for other variables. Notice that since $\alpha_i$ is same across $t=1,..,T$ (but different for each $i$), it vanishes. By taking an OLS, we can obtain
\[
\hat{\beta}_{FD}=\left(\sum_{i=1}^n\sum_{t=2}^T \Delta x_{it}\Delta x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=2}^T \Delta x_{it}\Delta y_{it}
\]
\par
We can show that this is a consistent estimator. Write
\[
\hat{\beta}_{FD}-\beta =\left(\sum_{i=1}^n\sum_{t=2}^T \Delta x_{it}\Delta x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=2}^T \Delta x_{it}\Delta u_{it}
\]
We need to show that $E[\Delta x_{it} \Delta u_{it}]=0$. This can be written
\[
\begin{aligned}
E[\Delta x_{it} \Delta u_{it}]&=E[ (x_{it}-x_{i,t-1})( u_{it}-u_{i,t-1})]\\
&=E[ x_{it}u_{it}]-E[x_{it}u_{i,t-1}]-E[x_{i,t-1}u_{it}]+E[x_{i,t-1}u_{i,t-1}]\\
&= 0-0-0+0=0
\end{aligned}
\] 
Therefore, $\hat{\beta}_{FD}$ is consistent. Another requirement for this to be defined is that $\left(\sum_{t=2}^T \Delta x_{it}\Delta x_{it}'\right)$ should be a full column matrix so that the inverse matrix is defined. This would effectively rule out time-constant regressors. 
\section{Within Estimators (WE)}
This shares a similar idea with First Difference in the sense that it attempts to weed out $\alpha_i$. This approach uses a different method. I will define new notations that involve variables that are averaged across time. Write 
\[
\bar{y}_i =\frac{1}{T}\sum_{t=1}^T y_{it}
\]
and similarly for other variables. By average over time across all variables, we can get a cross-sectional equation
\[
\bar{y}_i = \bar{x}_i'\beta + \alpha_i+\bar{u}_i
\]
The key is that even if $y_i$ is average across time, we still get $\alpha_i$ itself. This is because $\frac{1}{T}\sum_{t=1}^T\alpha_i = (T\alpha_i)/T=\alpha_i$. So subtract the cross-sectional equation from the original data generating process to get
\[
\tilde{y}_{it}= \tilde{x}_{it}'\beta+\tilde{u}_{it}, \ \ (i=1,..,n, \ \text{and } t=1,..,T)
\]
where $\tilde{y}_{it}=y_{it}-\bar{y}_i$. The within estimator is obtained by taking an OLS to above equation
\[
\hat{\beta}_{WE}=\left(\sum_{i=1}^n\sum_{t=1}^T \tilde{x}_{it}\tilde{x}_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T \tilde{x}_{it}\tilde{y}_{it}
\]
\par
To show consistency, rewrite the above as
\[
\hat{\beta}_{WE}-\beta=\left(\sum_{i=1}^n\sum_{t=1}^T \tilde{x}_{it}\tilde{x}_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T \tilde{x}_{it}\tilde{u}_{it}
\]
$E[\tilde{x}_{it}\tilde{u}_{it}]$ can be written as
\[
\begin{aligned}
E[\tilde{x}_{it}\tilde{u}_{it}]&=E[x_{it}u_{it}]-E[x_{it}\bar{u}_i]-E[\bar{x}_iu_{it}]+E[\bar{x}_i\bar{u}_i]\\
\end{aligned}
\] Since $\bar{x}_i, \bar{u}_i$ incorporates regressors and errors from all time periods, applying strict exogeneity (and strict exogeneity only) reduces the above equation to 0. Therefore, $\hat{\beta}_{WE}$ is consistent. Note that for this estimator, we REALLY need a strict exogeneity assumption. Anything weaker than this could make this estimator inconsistent. In addition, we need that $E\left[\tilde{x}_{it}\tilde{x}_{it}'\right]$ be a full column rank for its inverse to be defined. Time-constant regressors are ruled out. \par
\section{Least Square Dummy Variables (LSDV)}
This comes from viewing $\alpha_i$ as a parameter to be estimated. Let $Dk_i$ be the dummy variable that equals 1 if $i=k$ and 0 otherwise. The idea is to put a total of $N-1$ of such dummy variables into the regression\footnote{This is assuming that $x_{it}$ contains a vector of 1's for a `overall' constant.}. Therefore, we work with
\small{\[
y_{it}=x_{it}'\beta + D1_i\alpha_1+...+D(n-1)_i\alpha_{n-1}+u_{it} 
\]}\normalsize
What is going on here is that each individual has his/her own constant term. For the $n$'th individual, the constant term is represented by the $\beta_0$, the coefficient on the column vector of $x_{it}$. For $k(\neq n)$'th individual, the intercept term is $\beta_0 + \alpha_k$. 
\section{Some Interesting Topics on Fixed Effects Panel Estimators}
\begin{itemize}
\item \textbf{WE vs FD}: When $T=2$, it can be shown that they are numerically equal. I start with the WE, written as
\[
\left(\sum_{i=1}^n\sum_{t=1}^2\tilde{x}_{it}\tilde{x}_{it}' \right)^{-1}\left(\sum_{i=1}^n\sum_{t=1}^2\tilde{x}_{it}\tilde{y}_{it} \right)
\]\par
Each term can be replaced by
\[
\begin{aligned}
\sum_{t=1}^2\tilde{x}_{it}\tilde{x}_{it}'&=\sum_{t=1}^2\left(x_{it}-\frac{x_{i1}+x_{i2}}{2}\right)\left(x_{it}-\frac{x_{i1}+x_{i2}}{2}\right)'\\
&=\sum_{t=1}^2x_{it}x_{it}'-\frac{x_{i1}x_{i1}'+x_{i1}x_{i2}'+x_{i2}x_{i1}'+x_{i2}x_{i2}'}{2}\\
&=\frac{x_{i1}x_{i1}'-x_{i1}x_{i2}'-x_{i2}x_{i1}'+x_{i2}x_{i2}'}{2}\\
&=\frac{(x_{i1}-x_{i2})(x_{i1}-x_{i2})'}{2}=\frac{\Delta x_{i2} \Delta x_{i2}'}{2}\\
\end{aligned}
\]
and 
\[
\begin{aligned}
\sum_{t=1}^2\tilde{x}_{it}\tilde{y}_{it}&=\sum_{t=1}^2\left(x_{it}-\frac{x_{i1}+x_{i2}}{2}\right)\left(y_{it}-\frac{y_{i1}+y_{i2}}{2}\right)\\
&=\sum_{t=1}^2 x_iy_i - \frac{x_{i1}y_{i1}+x_{i1}y_{i2}+x_{i2}y_{i1}+x_{i2}y_{i2}}{2}\\
&=\frac{x_{i1}y_{i1}-x_{i1}y_{i2}-x_{i2}y_{i1}+x_{i2}y_{i2}}{2}\\
&=\frac{(x_{i1}-x_{i2})(y_{i1}-y_{i2})}{2}=\frac{\Delta x_{i2} \Delta y_{i2}}{2}\\
\end{aligned}
\]
By combining the two, I get $\left(\sum_{i=1}^n \Delta x_{i2}\Delta x_{i2}'\right)^{-1}\sum_{i=1}^n \Delta x_{i2}\Delta y_{i2}$, which is equivalent to FD when $T=2$. 

When $T\geq3$, they are no longer equal. So it begs the question as to which estimator to use. This depends on the structure of the error terms. If $u_{it}$ is free from serial correlation (or IID), then taking a first difference would introduce serial correlation. This is because
\[
\begin{aligned}
cov(\Delta u_{it},\Delta u_{i,t-1})&=E[u_{it}u_{it-1}]-E[u_{it}u_{it-2}]-E[u_{it-1}u_{it-1}]+E[u_{it-1}u_{it-2}]\\
&=0-0-var(u_{it-1})+0\neq 0 \\
\end{aligned}
\]
As such, first difference in this situation suffers from inconsistency problems arising due to serial correlation. \par
There may be a case when $\Delta u_{it}$ is serially uncorrelated. For instance, $u_{it}$ could be a random walk process in the sense that
\[
u_{it}=u_{it-1}+\eta_{it} \ \ (E[\eta_{it}]=0, E[\eta_{it}\eta_{is}]=0 (s\neq t), var(u_{it})=\sigma^2)
\]
If we use a first difference estimator here, we get to obtain the most efficient estimator. 
\item \textbf{WE=LSDV} To show that they are numerically equal, it helps to know how these estimators can be written in terms of matrices and understand what Kronecker product is. Note the following
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{definition}[Kronecker Product] 
Let $A\in\mathbb{R}^{m\times n}, B\in\mathbb{R}^{p\times q}$. Then their Kronecker product $A\otimes B$ is defined as
\[
A\otimes B = \begin{pmatrix}a_{11}B & ... &a_{1n}B\\ ...&...&...\\a_{m1}B &...&a_{mn}B \end{pmatrix}\in\mathbb{R}^{mp\times nq}
\]
where $B=\begin{pmatrix}b_{11} & ... &b_{1q}\\ ...&...&...\\b_{p1} &...&b_{pq} \end{pmatrix}$. Some of its properties are
\begin{itemize}
\item $A\otimes(B+C)=A\otimes B + A\otimes C$ ($B,C$ are of same size)
\item $(A\otimes B)(C\otimes D)=AC\otimes BD$ (Assuming multiplication is implementable)
\item $(A\otimes B)^{-1}= (A^{-1}\otimes B^{-1})$ (Assuming inverses are defined)
\item $(A\otimes B)'= (A'\otimes B')$
\end{itemize}
\end{definition}
\end{mdframed}\par
I will first write WE in terms of matrices. for each individual $i$, we can stack up total of $T$ observations and write
\[
\mathbf{y}_i =\mathbf{X}_i\beta + \alpha_i 1_T +\mathbf{u}_i \in \mathbb{R}^T 
\]
where $1_T$ is the $T\times 1$ vector of 1's as elements. Now I define $Q_T\equiv I_T-1_T(1_T'1_T)^{-1}1_T'$ Note that $(1_T'1_T)^{-1}=T^{-1}$ and $1_T1_T'$ is the $T$-dimensional square matrix of 1's as elements. This is symmetric and idempotent. Also note that 
\[
Q_T1_T =0, Q_T\mathbf{y}_i=\mathbf{y}_i-\frac{1}{T}\begin{pmatrix}\sum_t y_{it}\\ ...\\\sum_t y_{it}\end{pmatrix}=\begin{pmatrix}y_{i1}-\frac{1}{T}\sum_t y_{it}\\ ... \\y_{iT}-\frac{1}{T}\sum_t y_{it}\end{pmatrix}=\mathbf{\tilde{y}}_i, Q_T\mathbf{X}_i = \mathbf{\tilde{X}}_i,Q_T\mathbf{u}_i = \mathbf{\tilde{u}}_i
\]
This shows that pre-multiplying $Q_T$ to $\mathbf{y}_i =\mathbf{X}_i\beta + \alpha_i 1_T +\mathbf{u}_i \in \mathbb{R}^T $ gives us the demeaned version of the original data generating process. So we need to take OLS on
\begin{gather*}
Q_T\mathbf{y}_i=Q_T\mathbf{X}_i\beta+ Q_T\mathbf{u}_i\\
\implies \hat{\beta}_{WE}=\left(\sum_{i=1}^n\mathbf{X}_i'Q_T\mathbf{X}_i\right)^{-1}\sum_{i=1}^n\mathbf{X}_i'Q_T\mathbf{y}_i
\end{gather*}
We can characterize the LSDV estimator from the same original data generating process. What we need to do is to stack $\mathbf{y}_i$ and $\mathbf{u}_i$  for each $i$ and obtain a $nT$ dimensional vector $\mathbf{y}$ and $\mathbf{u}$. Similarly, stack each $\mathbf{X}_i$ to get an $nT\times k$ matrix $\mathbf{X}$. Since each $\alpha_i$ exists for each individual $i$, the idea here is to use a Kronecker product and express the individual fixed effect as
\[
\begin{pmatrix}1_T &...& 0 \\ ... &...&...\\ 0&...& 1_T \end{pmatrix}\begin{pmatrix}\alpha_1\\ ...\\ \alpha_n \end{pmatrix}=(I_n\otimes 1_T)\alpha\in\mathbb{R}^{nT\times n}\times \mathbb{R}^{n\times 1}=\mathbb{R}^{nT\times 1}
\]
Combine what we know to get 
\[
\mathbf{y} = \mathbf{X}\beta+(\underbrace{I_n\otimes 1_T}_{=D})\alpha+\mathbf{u}
\]
We then use the Frisch-Waugh-Lovell theorem (Chapter 3, Hansen (2019)) to get
\[
\hat{\beta}_{LSDV}=(\mathbf{X}'M_D\mathbf{X})^{-1}(\mathbf{X}'M_D\mathbf{y})
\]
where $M_D = I_{nT}-D(D'D)^{-1}D'$. Rewrite $D(D'D)^{-1}D'$ using the properties mentioned above as
\[
\begin{aligned}
D(D'D)^{-1}D'&=(I_n\otimes 1_T)[(I_n\otimes 1_T)'(I_n\otimes 1_T)]^{-1}(I_n\otimes 1_T)'\\
&=(I_n\otimes 1_T)[(I_n\otimes 1_T')(I_n\otimes 1_T)]^{-1}(I_n\otimes 1_T')\\
&=(I_n\otimes 1_T)[(I_n\otimes 1_T'1_T)]^{-1}(I_n\otimes 1_T')\\
&=(I_n\otimes 1_T)[I_n\otimes (1_T'1_T)^{-1}](I_n\otimes 1_T')\\
&=I_n\otimes 1_T(1_T'1_T)^{-1}1_T'\\
\end{aligned}
\]
Thus, $M_D = I_{nT} - I_n\otimes 1_T(1_T'1_T)^{-1}1_T'=I_n\otimes I_T -I_n\otimes  1_T(1_T'1_T)^{-1}1_T'$. By using the first property mentioned with the Kronecker product, this is equal to $I_n\otimes(I_T - 1_T(1_T'1_T)^{-1}1_T')=I_n\otimes Q_T$. So $\hat{\beta}_{LSDV}$ is equal to
\[
\hat{\beta}_{LSDV}=(\mathbf{X}'(I_n\otimes Q_T)\mathbf{X})^{-1}(\mathbf{X}'(I_n\otimes Q_T)\mathbf{y})
\]
Since $\mathbf{X} = \begin{pmatrix}\mathbf{X}_1\\ ... \\ \mathbf{X}_n \end{pmatrix}$, $I_n\otimes Q_T =  \begin{pmatrix}Q_T & 0 & 0 \\ ...&...&...\\0 & 0 & Q_T \end{pmatrix}$ and $\mathbf{y} = \begin{pmatrix}\mathbf{y}_1\\ ... \\ \mathbf{y}_n \end{pmatrix}$, we get
\[
\hat{\beta}_{LSDV}=\left(\sum_{i=1}^n \mathbf{X}_i'Q_T \mathbf{X}_i\right)^{-1}\sum_{i=1}^n \mathbf{X}_i'Q_T \mathbf{y}_i = \hat{\beta}_{WE}
\]
and we are done.
\end{itemize}
%%%%%%%%%%%%
\section{Random Effects}
Consider the following data generating process
\[
y_{it}=x_{it}'\beta_1 + \alpha_i +u_{it}, \ \ (i=1,..,n \ \text{and } \ t=1,..,T)
\]
where we assume that $E(\alpha_i)=0, E(\alpha_i u_{it})=0$, $u_{it}$ is IID across $i$ and $t$ and independent of $x_{it}$. On top of this, a key assumption that separates random effects from fixed effects is $\alpha_i$ and $x_{it}$ is now \textbf{uncorrelated}. The major consequence of this assumption is that even the POLS estimator becomes consistent. (We also still assume that strict exogeneity holds.) \par
To show this, I apply an OLS procedure on the data generating process above, while writing $v_{it}=\alpha_i + u_{it}$. This gets us
\[
\hat{\beta}_{POLS}=\left( \sum_{i=1}^n\sum_{t=1}^T x_{it} x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T x_{it} y_{it}
\]
which can alternatively be written as
\[
\hat{\beta}_{POLS}-\beta_1 =\left( \sum_{i=1}^n\sum_{t=1}^T x_{it} x_{it}'\right)^{-1}\sum_{i=1}^n\sum_{t=1}^T x_{it} v_{it}
\]
We need to work with the asymptotics of $\frac{1}{nT}\sum_{i=1}^n\sum_{t=1}^T x_{it} v_{it}$. Since
\begin{align*}
\frac{1}{nT}\sum_{i=1}^n\sum_{t=1}^T x_{it} v_{it}&=\frac{1}{nT}\sum_{i=1}^n\sum_{t=1}^T x_{it} (\alpha_i+ u_{it})\\
&=\frac{1}{nT}\sum_{i=1}^n\sum_{t=1}^T x_{it}\alpha_i+ \frac{1}{nT}\sum_{i=1}^n\sum_{t=1}^T x_{it}u_{it}\\
&\xrightarrow{p} E(x_{it}\alpha_i)+E(x_{it}u_{it})=0+0=0
\end{align*}
The last summation terms converge in probability to 0 now. The reason POLS in fixed effects setting was inconsistent was that this term converges in probability something nonzero. Therefore, POLS is now consistent as well. \par
However, this is not really the best we can do. The reason is that the composite error from different time periods are serially correlated. This can be seen by
\[
\begin{aligned}
cov(v_{it},v_{is})&=cov(\alpha_i + u_{it},\alpha_i + u_{is})\\
&=E[(\alpha_i + u_{it})(\alpha_i + u_{is})]-E[\alpha_i + u_{it}]E[\alpha_i + u_{is}]\\
&=E[\alpha_i^2 + \alpha_iu_{it}+\alpha_iu_{is}+u_{it}u_{is}]-(0+0)(0+0)\\
&=E[\alpha_i^2] + E[\alpha_iu_{it}]+E[\alpha_iu_{is}]+E[u_{it}u_{is}]\\
&=var(\alpha_i)\\
\end{aligned}
\]
If it is the case that $var(\alpha_i)=0$, then $\alpha_i$ is a degenerate variable centered at 0 - effectively there is no individual fixed effect. Except for this case, the composite error terms would be serially correlated. \par
The estimation method that is the most efficient for this case would be the GLS. The key to constructing a GLS is to find a variance-covariance matrix for $\mathbf{v}_i$. What we can do is to stack the $T$ observations for each individual and work with
\[
\mathbf{y}_ i = \mathbf{X}_i\beta_1 + \mathbf{v}_i
\]
where $\mathbf{y}_i = \begin{pmatrix}y_{i1} \\ ... \\ y_{iT} \end{pmatrix}$, $\mathbf{X}_i=\begin{pmatrix}x_{i1}' \\ ... \\ x_{iT}' \end{pmatrix}$,  $\mathbf{v}_i = \begin{pmatrix}v_{i1} \\ ... \\ v_{iT} \end{pmatrix}=\begin{pmatrix}\alpha_i+u_{i1} \\ ... \\ \alpha_i+u_{iT} \end{pmatrix}$. To make algebra more straightforward, I will define $1_T$ as a $T$-dimensional column vector of 1's.  Then,
\[
\mathbf{v}_i = 1_T\alpha_i + \mathbf{u}_i
\]
Then the variance can be calculated by the $E[(X-E(X))(X-E(X))']$ formula. Thus,
\begin{align*}
var(v_i)&=E[(\mathbf{v}_i-E(\mathbf{v}_i))(\mathbf{v}_i-E(\mathbf{v}_i))']\\
&=E[\mathbf{v}_i\mathbf{v}_i'] \ (\because E(\mathbf{v}_i)=0)\\
&=E\left[1_T\alpha_i^2 1_T' + \mathbf{u}_i\alpha_i 1_T'+1_T\alpha_i \mathbf{u}_i + \mathbf{u}_i\mathbf{u}_i' \right]\\
&=1_T1_T'var(\alpha_i)+E[ \mathbf{u}_i\mathbf{u}_i' ]
\end{align*} 
Suppose that $u_{it}$ is homoskedastic, then $E[ \mathbf{u}_i\mathbf{u}_i' ]$ becomes $\sigma_u^2 I_T$, the result derived in class. \par
The GLS estimator can be derived by the following: Note that $V$ is a symmetric matrix and positive semidefinite. Then $V=Q\Omega Q'$ by spectral decomposition theorem\footnote{It is a theorem that states that if a matrix is symmetric and positive semidefinite, there exists real eigenvalues and real orthogonal eigenvectors. You can look at Gockenbach(2010) for more details. } and $V^{-1/2}$ can be defined (It would be $Q\Omega^{-1/2}Q'$). The trick now is to pre-multiply $V^{-1/2}$ to get
\[
V^{-1/2}\mathbf{y}_ i = V^{-1/2}\mathbf{X}_i\beta_1 + V^{-1/2}\mathbf{v}_i
\]
and the resulting GLS estimator would be
\[
\left(\sum_{i=1}^n (V^{-1/2}\mathbf{X}_i)'(V^{-1/2}\mathbf{X}_i)\right)^{-1}\sum_{i=1}^n (V^{-1/2}\mathbf{X}_i)'(V^{-1/2}\mathbf{y}_i)=\left(\sum_{i=1}^n \mathbf{X}_i'V^{-1}\mathbf{X}_i\right)^{-1}\sum_{i=1}^n \mathbf{X}_i'V^{-1}\mathbf{y}_i
\]
\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{comment}[So when do we use fixed effects vs. random effects?]
One way of figuring out when to use random effect is through a Hausmann Test. Random effects can be considered as a special case of fixed effects. In this test, the null hypothesis would be random effects and the alternative would be fixed effects. Note that the ingredients we have here are similar to the Hausmann Test in the endogeneity test. Random effects estimators would only be consistent if $H_0$ is true whereas fixed effects are consistent in both $H_0$ and $H_1$. If you are planning to use the random
effects estimator (and believe that the random effects assumptions are appropriate in your context), the Hausmann test can be used as a specification check for this assumption and provide evidence to support your approach.
\end{comment}
\end{mdframed}


\section{Dynamic Panel Data: Endogeneity Problems}
Now consider a setting where
\[
y_{it}=\rho y_{it-1}+\alpha_i +u_{it}
\]
where $t=0,1,...,T$ and $i=1,..,n$. Additionally, assume that $E[\alpha_i]=0, E[u_{it}]=0, E[\alpha_i u_{it}]=0 \ \forall i,t$ and $E[u_{it}u_{is}]=0$ for $t\neq s$. Also, the initial observation $y_{i0}$ satisfies $E[y_{i0}u_{it}]=0\ (t\geq1)$ and that $E|u_{it}|^{2+\delta}\leq M <\infty (\delta>0)$. Also note that observations are independent across $i$. \par
The difference between this and the previous panel estimation is that the lagged dependent variable enters as the regressor. In this case, all of the previous methods - random effects and fixed effects - are inconsistent. I will show this for POLS. Let $v_{it}=\alpha_i + u_{it}$. The OLS estimates would be
\small{\[
\hat{\rho}=\left(\sum_{i=1}^n \sum_{t=0}^Ty_{it-1}^2 \right)^{-1}\sum_{i=1}^n \sum_{t=0}^Ty_{it-1}y_{it}
\]}\normalsize
or
\small{\[
\hat{\rho}-\rho = \left(\frac{1}{nT}\sum_{i=1}^n \sum_{t=0}^Ty_{it-1}^2 \right)^{-1}\frac{1}{nT}\sum_{i=1}^n \sum_{t=0}^Ty_{it-1}(\alpha_i + u_{it})
\]}\normalsize
The problem is that since $y_{it-1}= \rho y_{it-2}+\alpha_i + u_{it-1}$, the $\frac{1}{nT}\sum_{i=1}^n \sum_{t=0}^Ty_{it-1}\alpha_i$ term does not converge in probability to 0.\par
Even if we get rid of $\alpha_i$ by using the first difference at $T=2$, $\rho$ estimates are still inconsistent. To show this, start with
\[
\Delta y_{i2}=\rho\Delta y_{i1}+\Delta u_{i2}
\]
We can now show that the regressor and the error term are correlated, since
\begin{align*}
cov(\Delta y_{i1}, \Delta u_{i2})&=E[(y_{i1}-y_{i0})(u_{i2}-u_{i1})]\\
 &=E[y_{i1}u_{i2}]-E[y_{i1}u_{i1}]-E[y_{i0}u_{i2}]+E[y_{i0}u_{i1}]
\end{align*}
Because $E[y_{i1}u_{i1}]$ contains terms from the 1st period, this contains term that is nonzero. \par
Even for the within estimator, which is written as
\[
y_{it}-\frac{1}{T}\sum_{i=1}^Ty_{it}=\rho\left(y_{it-1}-\frac{1}{T}\sum_{i=1}^Ty_{it-1}\right)+u_{it}-\frac{1}{T}\sum_{i=1}^Tu_{it}
\]
The regressor contains $y_{i0},..,y_{iT-1}$ and residuals contain $u_{i1},..,u_{iT}$. There are overlapping time periods, implying that the regressor becomes endogenous.
\section{Anderson-Hsiao Estimator}
In estimating 
\[
y_{it}=\rho y_{it-1}+\alpha_i +u_{it}
\]
Anderson and Hsiao (1982) proposed the following IV approach. First difference the above equation and obtain
\[
\Delta y_{it}=\rho\Delta  y_{it-1} +\Delta u_{it}
\]
where possible values of $i$ remain the same but $t=2,...,T$. The suggested IV is that $\Delta y_{it-1}$ be instrumented with $y_{it-2}$. We can check that this is a valid IV. 
\begin{itemize}
\item \textbf{Relevancy}: Note that $\Delta y_{it-1}= y_{it-1}-y_{it-2}$. So this term contains $y_{it-2}$ and thus relevancy is satisfied.
\item \textbf{Exogeneity}: Note that
\begin{align*}
cov(y_{it-2},\Delta u_{it})&=E[y_{it-2}, u_{it}-u_{it-1}]\\
&=E[y_{it-2}u_{it}]-E[y_{it-2}u_{it-1}]\\
&=0-0=0
\end{align*}
The last line is justified as follows. Since we are assuming $E[y_{i0}u_{it}]=0$ for $t\geq1$, we can expand to
\begin{align*}
E[y_{i1}u_{it}]\ (t\geq 2)&=E[(\rho y_{i0}+\alpha_i +u_{i1})u_{it}]\\
&=\rho E[y_{i0}u_{it}]+E[\alpha_iu_{it}]+E[u_{i1}u_{it}]\\
&=\rho\times0+0+0=0
\end{align*}
Likewise, 
\begin{align*}
E[y_{i2}u_{it}]\ (t\geq 3)&=E[(\rho y_{i1}+\alpha_i +u_{i2})u_{it}]\\
&=\rho E[y_{i1}u_{it}]+E[\alpha_iu_{it}]+E[u_{i2}u_{it}]=0
\end{align*}
Therefore, we can generalize to $E[y_{is}u_{it}]=0$ for $t\geq s+1$. 
\end{itemize}
We can generalize using matrix notation. Define
\[
\Delta \mathbf{y}_i = \rho\Delta\mathbf{y}_{i,-1}+\Delta \mathbf{u}_i
\]
where $\Delta \mathbf{y}_i=\begin{pmatrix} \Delta y_{i2}\\ ...\\ \Delta y_{iT} \end{pmatrix}$, $\Delta \mathbf{y}_{i,-1}=\begin{pmatrix} \Delta y_{i1}\\ ...\\ \Delta y_{iT-1} \end{pmatrix}$ and $\Delta \mathbf{u}_i=\begin{pmatrix} \Delta u_{i2}\\ ...\\ \Delta u_{iT} \end{pmatrix}$. The matrix of instruments $Z_i$ would be
\[
Z_i = \begin{pmatrix}y_{i0}& 0 & 0 & ... \\ 0 & y_{i1}& 0 & ... \\ ... & ... &...&...\\ 0 & 0 & ... & y_{iT-2} \end{pmatrix}\in\mathbb{R}^{(T-1)\times (T-1)}
\]
This is a just identified case in the sense that the number of IV is equal to the number of endogenous variables. So we solve the sample analogue of
\begin{align*}
E[Z_i'\Delta \mathbf{u}_i]=0&\iff E[Z_i'(\Delta \mathbf{y}_i - \rho\Delta\mathbf{y}_{i,-1})]=0\\
&\implies \frac{1}{n}\sum_{i=1}^n(Z_i'\Delta \mathbf{y}_i - \rho Z_i'\Delta \mathbf{y}_{i,-1})=0\\
&\iff \rho \frac{1}{n}\sum_{i=1}^n  Z_i'\Delta \mathbf{y}_{i,-1}=  \frac{1}{n}\sum_{i=1}^n \rho Z_i'\Delta \mathbf{y}_{i}\\
&\iff \hat{\rho}=\left(\sum_{i=1}^n  Z_i'\Delta \mathbf{y}_{i,-1}\right)^{-1}\sum_{i=1}^n  Z_i'\Delta \mathbf{y}_{i}
\end{align*} 
\section{Arellano-Bond Estimator: Without Regressors}
This approach is similar to that of Anderson-Hsiao in the sense that we start with the first differentiation. The difference is in the instruments used. Arellano and Bond (1991) suggests that to instrument for $\Delta y_{it-1}$, we use $y_{i0},...,y_{it-2}$ as instruments. To see why they are valid
\begin{itemize}
\item \textbf{Relevancy:} It should be clear why $y_{it-2}$ is relevant. As for others, since $y_{it-1}=\rho y_{it-2}+u_{it-1}$ and $y_{it-2}=\rho y_{it-3}+u_{it-2}$ We can write recursively that
\[
y_{it-1} = \rho^2 y_{it-3}+\rho u_{it-2} + u_{it-1}
\]
... and so on. Therefore, we can verify relevancy.
\item \textbf{Exogeneity:} Note that $cov(y_{is},\Delta u_{it})$ for any $s<t$ is 0, as we have shown above. So exogeneity holds as well. 
\end{itemize}
The generalized approach using matrix will be similar except for the instrument matrix $Z_i$.
\[
Z_i = \begin{pmatrix}y_{i0}& 0 & 0 & ... \\ 0 & (y_{i0},y_{i1})& 0 & ... \\ ... & ... &...&...\\ 0 & 0 & ... & (y_{i0},....,y_{iT-2}) \end{pmatrix}\in\mathbb{R}^{(T-1)\times \frac{T(T-1)}{2}}
\]
Unlike Anderson-Hsiao estimator, this is an overidentified case. So we would need to use a GMM criterion with a weight matrix $W_n$. This would result in
\begin{align*}
\hat{\rho}&=\arg\min_\rho \left\{n\times \frac{1}{n}\sum_{i=1}^n(Z_i'\Delta \mathbf{y}_i - \rho Z_i'\Delta \mathbf{y}_{i,-1})'W_n  \frac{1}{n}\sum_{i=1}^n(Z_i'\Delta \mathbf{y}_i - \rho Z_i'\Delta \mathbf{y}_{i,-1})\right\}\\
&\implies \left[\left(\sum_{i=1}^n\Delta \mathbf{y}_{i,-1}' Z_i\right)W_n\left(\sum_{i=1}^n Z_i' \Delta \mathbf{y}_{i,-1}\right) \right]^{-1}\left(\sum_{i=1}^n\Delta \mathbf{y}_{i,-1}' Z_i\right)W_n\left(\sum_{i=1}^n Z_i' \Delta \mathbf{y}_i \right)
\end{align*}

The remaining question is to now select a matrix $W_n$ that would lead to the lowest variance possible. If $g(Z_i,\rho)$ is the moment condition, the following would qualify as the most efficient weighting matrix.
\[
W_n = E[g(Z_i,\rho)g(Z_i,\rho)']^{-1}
\]
In our context, $g(Z_i,\rho)$ would be equivalent to $Z_i'\Delta u_i$. So we would need to find a sample analogue of
\[
E[Z_i'\Delta \mathbf{u}_i \Delta \mathbf{u}_i'Z_i] \implies \frac{1}{n}\sum_{i=1}^nZ_i\Delta \mathbf{u}_i \Delta \mathbf{u}_i' Z_i
\]
For further simplification, we can assume two types of settings
\begin{itemize}
\item $u_{it}$ is \textbf{IID and homoskedastic}: In such case, where $E[u_{it}^2]=\sigma_u^2$,  we can write
\[
E[\Delta \mathbf{u}_i \Delta \mathbf{u}_i]=E\begin{bmatrix}\Delta u_{i2}^2 & \Delta u_{i2}\Delta u_{i3} & ... & \Delta u_{i2}\Delta u_{iT} \\\Delta u_{i2}\Delta u_{i3} & \Delta u_{i3}^2 & ... & \Delta u_{ie}\Delta u_{iT}\\ ... & ...  & ... & ... \\  \Delta u_{i2}\Delta u_{iT} & \Delta u_{i3}\Delta u_{iT} & ... & \Delta u_{iT}^2  \end{bmatrix}=\begin{bmatrix}2\sigma_u^2& -\sigma_u^2 &... &0\\ -\sigma_u^2 & 2\sigma_u^2 & ... &0 \\ ...&...&...&...\\ 0 & 0 & ... & 2\sigma_u^2\end{bmatrix}
\]
Define matrix $H\in\mathbb{R}^{(T-1)\times(T-1)}$ to have $2$'s in the diagonal elements, $-1$'s in the immediate off-diagonals, and $0$ everywhere else. Then $E[\Delta \mathbf{u}_i \Delta \mathbf{u}_i']=\sigma_u^2 H$. This implies that the weighting matrix that we are looking for is
\[
E[Z_i'\Delta \mathbf{u}_i \Delta \mathbf{u}_i'Z_i]^{-1}=\left(\frac{1}{n}\sum_{i=1}^nZ_iH Z_i\right)^{-1}
\]
where $\sigma_u^2$ is taken out since scaling $W_n$ by a scalar does not affect the value of the estimator. However, note that it may affect the test statistics for the overidentification test we will conduct later. 
\item $u_{it}$ is \textbf{heteroskedastic}: For this, we take an approach similar to the two-step GMM estimator we did some weeks ago. The optimal weighting matrix in this case would be
\[
W_n=\left(\frac{1}{n}\sum_{i=1}^nZ_i\Delta\tilde{\mathbf{u}}_i \Delta\tilde{\mathbf{u}}_i' Z_i\right)^{-1} 
\]
where $\Delta\tilde{\mathbf{u}}_i=\Delta \mathbf{y}_ i -\tilde{\rho}\Delta \mathbf{y}_{i,-1}$, a residual from the preliminary estimator $\tilde{\rho}$. The preliminary estimator could be either from $W_n = I_{T(T-1)/2}$ or from the one-step GMM estimator that we derived earlier. 
\end{itemize}

Because we are using more moment conditions that the number of endogenous variables, this is when we could test for an overidentification restriction. Suppose that $W_n$ is the efficient weighting matrix. Then, similar to the GMM overidentification test, we are testing
\[
H_0: E[g(Z_i,\rho)]=0, \ \ \ H_1: E[g(Z_i,\rho)]\neq0
\]
We can construct the following test statistic (Sargan-Hansen overidentification statistic)
\[
J= n \bar{g}_n(\hat{\rho})'W_n\bar{g}_n(\hat{\rho})
\]
Under $H_0$, $J$ has a limiting distribution $\chi^2_{\left(\frac{T(T-1)}{2}-1\right)}$. We lose one degree of freedom since we have used one estimator for $\rho$. 
\section{Arellano-Bond Estimator: With Regressors}
Now we generalize further by including regressors $x_{it}\in\mathbb{R}^k$. We can write the data generating process as
\[
y_{it}=\rho y_{it-1}+\beta'x_{it}+\alpha_i + u_{it}
\]
There are two types of assumption we can put on the regressors
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}[Predetermined]
We say that the regressors are \textbf{predetermined} if
\[
E[u_{it}|x_{i0},...,x_{it}]=0 \ \text{for each } t=0,...,T
\]
\end{assumption}
\begin{assumption}[Strictly Exogenous]
We say that the regressors are \textbf{strictly exogenous} if
\[
E[u_{it}|x_{i0},...,x_{iT}]=0 \ \text{for all } t=0,...,T
\]
\end{assumption}
\end{mdframed}
Since there is an unobservable term $\alpha_i$ which we allow to be correlated with $x_{it}$ and $y_{it-1}$, one thing we can do is to take the first difference
\[
\Delta y_{it}=\rho \Delta y_{it-1}+\beta'\Delta x_{it}+\Delta  u_{it}
\]
However, note that the equation is endogenous because of the two reasons
\begin{itemize}
\item $\Delta y_{it-1}$ is endogenous as 
\begin{align*}
E[\Delta y_{it-1}\Delta u_{it}] &= E[(y_{it-1}y_{it-2})(u_{it}-u_{it-1})]\\
&=E[y_{it-1}u_{it}]-E[y_{it-1}u_{it-1}]-E[y_{it-2}u_{it}]+E[y_{it-2}u_{it-1}]
\end{align*}
Since we still assume $E[y_{i0}u_{it}]=0$ for $t\geq1$, (and thus $E[y_{it}u_{is}]=0$ for $s\geq t+1$), we can still get rid of other terms except $E[y_{it-1}u_{it-1}]$. The remaining term has a nonzero correlation, so $\Delta y_{it-1}$ becomes endogenous
\item  $\Delta x_{it}$ is endogenous under predetermined case since
\begin{align*}
E[\Delta x_{it}\Delta u_{it}]&=E[(x_{it}-x_{it-1})(u_{it}-u_{it-1})]\\
&=E[x_{it}u_{it}]-E[x_{it}u_{it-1}]-E[x_{it-1}u_{it}]+E[x_{it-1}u_{it-1}]
\end{align*}
While other terms are 0 by predetermined assumption, this assumption is silent about $E[x_{it}u_{it-1}]$. This is where the nonzero correlation is from. 
\end{itemize}\par
Given the two possible assumptions on the regressors, there are two sets of IV/GMM estimation that we can use.\par
\begin{itemize}
\item \textbf{Predetermined Regressors}: Rewrite the differenced equation and stack in the vector form for each individual by writing
\[
\Delta \mathbf{y}_i=\Delta \mathbf{w}_i\delta + \Delta \mathbf{u}_i
\]
where $\Delta \mathbf{w}_i=[\Delta \mathbf{y}_{i,-1},\ \Delta \mathbf{x}_i ]\in\mathbb{R}^{(T-1)\times (k+1)}$, $\Delta \mathbf{x}_i=\begin{pmatrix} x_{i2} \\ ... \\x_{iT}\end{pmatrix}$, $\delta=\begin{pmatrix}\rho \\ \beta'\end{pmatrix}$ and the other two variables are the same as before. For notational convenience, I define
\[
\mathbf{y}_{it}=\begin{pmatrix}y_{i0}\\ ... \\ y_{it} \end{pmatrix},\mathbf{x}_{it}=\begin{pmatrix}x'_{i0}\\ ... \\ x'_{it} \end{pmatrix}
\]
The possible instruments for the endogenous variable $\Delta x_{it}$ is any regressor up to period $t-1$, or $\mathbf{x}'_{it-1}$. The exogeneity is satisfied since the error term is $u_{it}-u_{it-1}$, regressors $x_{i1},...,x_{it-1}$ is not correlated to this by predeterminedness assumption. Since $\mathbf{x}'_{it-1}$ includes $x_{it-1}$, the IV's are relevant as well.  Thus, we can write
\[
Z_i = \begin{pmatrix}y_{i0}, \mathbf{x}'_{i1}& 0 &0 &...& 0\\ 0 & \mathbf{y}'_{i1},\mathbf{x}'_{i2} & 0 & ... & 0 \\ ...&...&...&...&...\\ 0 & 0& 0& ... & \mathbf{y}'_{iT-2},\mathbf{x}'_{iT-1}\end{pmatrix}
\]
 Then we take a GMM approach by using the moment condition $E[Z_i'\Delta\mathbf{u}_i]=0$, which results in the following estimator
\[
\hat{\delta}=\left[\left(\sum_{i=1}^n\Delta \mathbf{w}_{i}' Z_i\right)W_n\left(\sum_{i=1}^n Z_i' \Delta \mathbf{w}_{i}\right) \right]^{-1}\left(\sum_{i=1}^n\Delta \mathbf{w}_{i}' Z_i\right)W_n\left(\sum_{i=1}^n Z_i' \Delta \mathbf{y}_i \right)
\]
\item \textbf{Strictly exogenous}: The difference between predetermined and strictly exogenous case is that the entire elements in $\mathbf{x}'_{iT}$ is a valid IV. Strict exogeneity prevents any correlation between error terms of all time periods and there are elements in $\mathbf{x}'_{iT}$ that are included in $\Delta y_{it}$ for each $t$. So we can write the $Z_i$ matrix as
\[
Z_i = \begin{pmatrix}y_{i0}, \mathbf{x}'_{iT}& 0 &0 &...& 0\\ 0 & \mathbf{y}'_{i1},\mathbf{x}'_{iT}& 0 & ... & 0 \\ ...&...&...&...&...\\ 0 & 0& 0& ... & \mathbf{y}'_{iT-2},\mathbf{x}'_{iT}\end{pmatrix}
\]
We still work with the same moment condition $E[Z_i'\Delta \mathbf{u}_i]=0$. The resulting estimator for $\delta$ shares the same expression as in the predetermined case, but with different matrix for $Z_i$. 
\end{itemize}

%%%%%
\section{Regressing with an `Overall' Intercept}
Consider the following model
\[
y_{it}=\mu+\rho y_{it-1}+x_{it}'\beta+\alpha_i + u_{it}
\]
where $\mu$ term represents an `overall' constant - this term applies commonly to all $i$'s and $t$'s in the regression. When we estimated $\rho$ and $\beta$ before, we relied on first-differencing. The cost of doing this is the loss of time-invariant terms. This would now include $\mu$. If we are desperate to know what $\mu$ is, how do we estimate for them? \par
There are two ways to do this. One is a two step approach in the sense that we estimate the other parameters first and the back out $\mu$. The other is doing it simultaneously using additional moment conditions. \par
Before proceeding, it should be noted that $E(\alpha_i)=0$ is implicitly assumed. If not, we can change the above equation in the following fashion:
\[
y_{it}=\underbrace{\mu+E(\alpha_i)}_{\mu_0}+\rho y_{it-1}+x_{it}'\beta+\underbrace{\alpha_i-E(\alpha_i)}_{=\alpha_{0i}} + u_{it}
\]
In this setup, $E(\alpha_{0i})=0$. and we estimate for $\mu_0$. After that, we can find $\mu$.  \par
\begin{itemize}
\item \textbf{Two-step Approach}: We first obtain the estimate $\hat{\rho}$ and $\hat{\beta}$ using an Arellano-Bond estimator. Then, we can rearrange the equation into
\[
y_{it}-\hat{\rho} y_{it-1}-x_{it}'\hat{\beta}=\mu+\alpha_i + u_{it}
\]
Given that the mean of $\alpha_i$ and $u_{it}$ is zero, we can use the sample mean of the left-hand-side to obtain the estimate for $\mu$. Since $\mu$ applies to all $i$ and $t$, we need to sum over all $i$'s and $t$'s. Therefore, 
\[
\hat{\mu}=\frac{1}{nT}\sum_{i=1}^n \sum_{t=1}^T\left(y_{it}-\hat{\rho} y_{it-1}-x_{it}'\hat{\beta}\right)
\]
\item \textbf{Simultaneous Approach}: Note that $\alpha_i + u_{it}$ has a zero mean. For a fixed $i$, we can define $\mathbf{y}_i, \mathbf{y}_{i,-1}$ and $\mathbf{x}_i$ as stacked-up vector for individual $i$ with $T$ observations. We obtain
\small{\begin{align*}
\mathbf{y}_i =& 1_T\mu+\rho\mathbf{y}_{i,-1}+\mathbf{x}_i\beta+\underbrace{1_T\alpha_i + \mathbf{u}_i}_{=\mathbf{v}_i}\\
&=1_T\mu+\mathbf{w}_i\delta+\mathbf{v}_i
\end{align*}}\normalsize
using the fact that $\alpha_i + u_{it}$ has a zero mean yields this moment condition
\[
E[\mathbf{v}_i]=E[1_T\alpha_i + \mathbf{u}_i]=E[\mathbf{y}_i-\mathbf{w}_i\delta-1_T\mu]=0
\]
Along with the $E[Z_i'\Delta \mathbf{u}_i]=0$ condition, we can jointly write
\[
\begin{bmatrix}E(\mathbf{v}_i) \\ E(Z_i'\Delta \mathbf{u}_i) \end{bmatrix}=E\left[\begin{bmatrix}I_T & 0 \\ 0 & Z_i\end{bmatrix}'\begin{bmatrix}\mathbf{v}_i \\ \Delta\mathbf{u}_i \end{bmatrix}\right]
\]
\end{itemize}
Define \small{$\bar{Z}_i = \begin{bmatrix}I_T & 0 \\ 0 & Z_i\end{bmatrix}$, $\bar{W}_i=\begin{bmatrix}I_T & \mathbf{w}_i \\ 0 & \Delta\mathbf{w}_i\end{bmatrix}$, $\bar{Y}_i = \begin{bmatrix} \mathbf{y}_i \\ \Delta \mathbf{y}_i\end{bmatrix}$}\normalsize. Then $\begin{bmatrix}\mathbf{v}_i \\ \Delta\mathbf{u}_i \end{bmatrix}=\bar{Y}_i - \bar{W}\begin{bmatrix}\mu \\ \delta\end{bmatrix}$. The moment condition and the resulting estimator for $\begin{bmatrix}\mu \\ \delta\end{bmatrix}$ is ($\bar{V}_n$ is the weighting matrix)
\footnotesize{\begin{gather*}
E\left[\bar{Z}_i'(\bar{Y}_i-\bar{W}_i)\begin{bmatrix}\mu \\ \delta\end{bmatrix}\right]=0\\
\begin{bmatrix}\hat{\mu} \\ \hat{\delta}\end{bmatrix}=\left[\sum_i(\bar{W}_i'\bar{Z}_i)\bar{V}_n\sum_i \bar{Z}_i'\bar{W}_i\right]^{-1}\sum_i(\bar{W}_i'\bar{Z}_i)\bar{V}_n\sum_i \bar{Z}_i'\bar{Y}_i
\end{gather*}}\normalsize\par
Note that in this manner, we can technically estimate for $\alpha_i$. However, since $T$ is small in usual datasets, we may not be able to guarantee the consistency of such estimator. 

\section{Subset of Regressors Uncorrelated with Fixed Effects}
Return to the model where
\[
y_{it}=\mu+\rho y_{it-1}+x_{it}'\beta+\alpha_i + u_{it}
\]
The difference is now that a subset of $x_{it}$ is exogenous with respect to  $\alpha_i$. Also assume that regressors are strictly exogenous. Denote such variable as $x_{it}^{(1)}$. Then the following holds
\[
E[x_{is}^{(1)}\alpha_i]=0\implies E[x_{is}^{(1)}v_{it}]=0\implies E[x_{is}^{(1)}(y_{it}-w_{it}'\delta-\mu)]=0 \ \forall s,t
\]
For notational convenience, define $h_i = vec(x_i^{(1)})=\begin{bmatrix} x_{i1}^{(1)} \\ ... \\ x_{iT}^{(1)}\end{bmatrix}$. Then the above moment condition is equivalent to
\[
E[(I_T\otimes h_i)\mathbf{v}_i]=0
\]
We can also make use of the fact that $E[\mathbf{v}_i]=0$, which is $E[(I_T\otimes 1)\mathbf{v}_i]$. This would give us the required moment condition for equations in levels (to back out $\mu$), expressed as
\[
E\left[\left(I_T\otimes \begin{bmatrix}1 \\h_i\end{bmatrix}\right)\mathbf{v}_i\right]=0
\]
Another set of required moment condition comes from $Z_i$ matrix defined for strictly exogenous case with $\mathbf{x}_{iT}'$ replaced with $vec(x_i^{(2)})'$. This gives us $E[Z_i'\Delta \mathbf{u}_i]=0$. The GMM estimator has the same expression as in the previous case but with $I_T$ in $\bar{Z}_i$ replaced with $\left(I_T\otimes \begin{bmatrix}1 \\h_i\end{bmatrix}\right)$.
\section{Nonlinear Moments}
Consider the model
\[
y_{it}=\rho y_{it-1}+\underbrace{\alpha_i +u_{it}}_{=v_{it}} 
\]
The assumptions on our variables were
\begin{itemize}
\item $E(\alpha_i)=0, E(u_{it})=0, E(\alpha_i u_{it})=0 \ \forall i,t$
\item $E(u_{it}u_{is})=0\  t\neq s$
\item $E(y_{i0}u_{it})=0 \ \forall i \ \text{and }t=1,..,T$
\end{itemize}
These three assumptions imply $E[\Delta v_{it-1}v_{it}]=0 \ t=3,..,T$. The rationale is as follows. Note that $\Delta v_{it-1}= u_{it-1}-u_{it-2}=\Delta u_{it-1}$. Also $v_{it}=\alpha_i + u_{it}$. Since fixed effects is not correlated with the error terms and there is no serial correlation, $E[\Delta v_{it-1}v_{it}]=0$ holds. In other words, 
\[
E[(\Delta y_{it-1}-\rho\Delta y_{it-2})(y_{it}-\rho y_{it-1})]=0
\]\par
The sample analogue of this would be
\[
\frac{1}{n}\sum_{i=1}^n (\Delta y_{it-1}y_{it}-\rho\Delta y_{it-1}y_{it-1}-\rho \Delta y_{it-2}y_{it}+\rho^2\Delta y_{it-2}y_{it-1})
\]
Notice the $\rho^2$ term here. Because of this, the moment condition becomes nonlinear (quadratic, to be exact). If GMM estimation is used, the objective function involves $\rho^4$ and FOC would involve $\rho^3$. So it is tricky to work with.
\par
In addition, assume that there is a time series homoskedasticity, or $E[u_{it}^2]=\sigma_u^2$. Then the following $T-1$ additional moments
\begin{gather*}
E[v_{it}^2]-E[v_{it-1}^2]=0, \ t=2,..,T\\
\iff E[(y_{it}-\rho y_{it-1})^2]-E[(y_{it-1}-\rho y_{it-2})^2]=0
\end{gather*}
Similar to the previous case, the objective function involves $\rho^4$ and FOC has $\rho^3$. This is again, tricky to work with. There is one way to `linearize' the moment conditions. Which is...
\section{Mean Stationarity}
If a distribution of a certain variable $y_t$ does not change over time, we call this a stationary distribution. Mean stationarity refers to the situation where the mean of a variable is time-invariant. One case where this can hold is as follows. Suppose
\[
y_{i0}=\frac{\alpha_i}{1-\rho}+e_{i0}
\]
Then $E[y_{i0}|\alpha_i]=\frac{\alpha_i}{1-\rho}$. Under the data generating process
\[
y_{it}=\rho y_{it-1}+\alpha_i +u_{it}
\]
we can show that means stationarity holds \footnote{This is what question 3 in 2019 Spring Midterm is about}. Specifically
\begin{align*}
E[y_{i1}|\alpha_i]&= E[\rho y_{i0}+\alpha_i +u_{i1}|\alpha_i]\\
&= \rho E[y_{i0}|\alpha_i]+\alpha_i + E[u_{i1}|\alpha_i]\\
&= \frac{\rho\alpha_i}{1-\rho}+\alpha_i = \frac{\alpha_i}{1-\rho}
\end{align*}
We can reiterate and get the same result for $E[y_{it}|\alpha_i]$ for $t=1,...,T$. Therefore, mean stationarity holds. \par
So how do the nonlinear moments become linearized? Let's start with the first nonlinear moment $E[(\Delta y_{it-1}-\rho\Delta y_{it-2})(y_{it}-\rho y_{it-1})]=0$. By mean stationarity, we get
\[
E[y_{it}-y_{it-1}|\alpha_i]=E[\Delta y_{it}|\alpha_i]=0 \implies E[\alpha_i \Delta y_{it}]=0 \ \forall t
\]
This result, along with $E[y_{i1}u_{is}]=0$ for $s\geq 2$ implies that $E[\Delta y_{i1}u_{is}]=0$ for $s\geq 2$. Thus, $E[\Delta y_{i1}v_{is}]=0 \ (s\geq 2)$. So we have $E[\Delta y_{i1}v_{i2}]=0$. For $s\geq 3$, we can use the nonlinear moment condition $E[\Delta v_{is-1}v_{is}]=0$ to back out
\begin{align*}
E[\Delta v_{is-1}v_{is}]=0&\implies E[(\Delta y_{is-1}-\rho \Delta y_{is-2})v_{is}]=0\\
(s=3)&\implies E[(\Delta y_{i2}-\rho\Delta y_{i1})v_{i3}]=0\\
&=E[\Delta y_{i2}v_{i3}]=0 \ (\because E[\Delta y_{i1}v_{is}]=0 \ (s\geq 2)])
\end{align*}
Repeat the similar process to ultimately get $E[\Delta y_{it-1}v_{it}]=0$\par
So the lagged differences of the instruments qualify as instruments for equation in levels. This is a Blundell-Bond estimator, or what is known as a system GMM estimation. Combine the above moment condition with the usual $E[Z_i'\Delta \mathbf{u}_i]=0$ to get a joint moment condition
\[
E\left[Z_i^{+'}\begin{pmatrix}\Delta \mathbf{u}_i \\ \mathbf{v}_i\end{pmatrix}\right]=0
\]
with $Z_i^+ = \footnotesize{\begin{bmatrix}Z_i & 0 & 0 & ... &0\\ 0 & \Delta y_{i1} & 0 & ... & 0 \\ ...&...&...&...&...\\ 0& 0& 0& ...& \Delta y_{iT-1}\end{bmatrix}}\normalsize$. Here, $\mathbf{v}_i$ has $t=2,...,T$. \par
As for the homoskedastic case, we can use the mean stationarity condition to obtain a linear moment condition
\begin{align*}
E[v_{it}^2]-E[v_{it-1}^2]&=E[(y_{it}-\rho y_{it-1})v_{it}-(y_{it-1}-\rho y_{it-2})v_{it-1}]\\
&=E[y_{it}v_{it}-y_{it-1}v_{it-1}]=0 \ (t=2,...,T)
\end{align*}
The remaining terms can be written as
\begin{align*}
E[-\rho y_{it-1} (\alpha_i + u_{it})+\rho y_{it-2}(\alpha_i+u_{it-1})]&=E[-\rho \Delta y_{it-1}\alpha_i +\rho y_{it-2}u_{it-1}-\rho y_{it-1}u_{it}]\\&=-0+0-0=0
\end{align*}
The mean stationarity justifies the first zero. Therefore, we can use more instruments, namely
\[
Z_{iH}^+ = \begin{bmatrix}Z_i & 0 & 0 & ... &0& 0&0&0&..&0\\ 0&0&0&..&0& y_{i1}& 0& 0& .. & 0 \\ 0 & \Delta y_{i1} & 0 & ... & 0& -y_{i2}& y_{i2}& 0& .. & 0 \\ ...&...&...&...&...&..&...&...&...&y_{T-1}\\ 0& 0& 0& ...& \Delta y_{iT-1}& 0 & 0 & 0 & .. & -y_T\end{bmatrix}
\]
Therefore, using the combined moment condition
\[
E\left[Z_{iH}^{+'} \begin{pmatrix} \Delta \mathbf{u}_i \\ \mathbf{v}_i\end{pmatrix}\right]=0
\]
we can obtain a GMM estimator of $\rho$. Here, $\mathbf{v}_i$ starts from $t=1$. 
\section{Factor Analysis and Interactive Fixed Effects}
Consider the following model
\[
y_{it}=\mu_i+x_{it}'\beta + \lambda_i'f_t + u_{it}
\]
where $\lambda_i$ is a vector of factor loadings and $f_t$ is a vector of factors. Each can be written
\[
\lambda_i = \begin{bmatrix}\lambda_{i1}\\ ... \\ \lambda_{ir} \end{bmatrix}, f_t = \begin{bmatrix}f_{1t}\\ ... \\ f_{rt} \end{bmatrix}
\]
where $r$ is usually a small number. We will assume that only $y_{it}$ and $x_{it}$ is observable. \par
In fact, we can see that this format is a generalized version of the additive fixed effect we have seen so far. For one thing, by writing
\[
\lambda_i = \begin{bmatrix}\alpha_{i}\\1 \end{bmatrix}, f_t = \begin{bmatrix}1\\\delta_{t} \end{bmatrix}
\]
we can back out the two-way fixed effects model of the following form
\[
y_{it}=x_{it}'\beta + \alpha_i+\delta_t + u_{it}
\]
Generally, we can capture unobserved individual traits that can vary with time. Or we can also capture entity-level responses to a common shock at certain time. \par
Estimating the above model with regressors can be done as follows. If it is the case that we know $\beta$, then we can write
\[
y_{it}-x_{it}'\beta = \mu_i+\lambda_i'f_t + u_{it}
\]
and estimate pure factor models. If we know what $\mu_i + \lambda_i'f_t$ is, we write
\[
y_{it}-\mu_i-\lambda_i'f_t = x_{it}'\beta+u_{it}
\]
which becomes a standard model. If we need to determine the parameters of interest all at once, we can do a LASSO-type regularized regression in the following sense. 
\[
\min \sum_i \sum_t (y_{it}-x_{it}'\beta-l_{it})^2 + \tau\Vert L \Vert_*
\]
where $l_{it}$ is the $\lambda_i'f_t$ and $L$ is the matrix of $l_{it}$'s. Note that we are applying a nuclear norm here. In this context, what we are doing is to minimize the sum squared residuals but with the penalty that applies when the rank of $L$ is large. Basically, we are treating $\beta, \lambda_i, f_t$ as a parameter to be estimated, whereas for the static dynamic model, our interest was on $\beta$. \par
A more compact way to write this is to stack observations in the time series format. Define
\[
\mathbf{y}_t = \begin{bmatrix}y_{1t}\\...\\y_{nt}\end{bmatrix}, \mu= \begin{bmatrix}\mu_{1}\\...\\ \mu_{n}\end{bmatrix}, \Lambda =  \begin{bmatrix}\lambda_{1}'\\...\\ \lambda_{n}'\end{bmatrix}, \mathbf{u}_t = \begin{bmatrix}u_{1t}\\...\\u_{nt}\end{bmatrix}
\]
Then the pure factor model can be written as
\[
\mathbf{y}_t = \mu+\Lambda f_t+\mathbf{u}_t 
\]
We impose these assumptions:
\begin{itemize}
\item $E(f_t)=0, var(f_t)=\Sigma_f\in\mathbb{R}^{r\times r}$
\item $E(\mathbf{u}_t)=0, var(\mathbf{u}_t)=\Psi$, where $\Psi$ is a diagonal matrix
\end{itemize}
This implies that $E[\mathbf{y}_t]=0$ and $var[\mathbf{y}_t]=\Lambda \Sigma_f \Lambda'+\Psi$. Also, $\Psi$ being diagonal implies that the error across entities are uncorrelated. The goal is to identify each parameters in the $var[\mathbf{y}_t]$ term. For this we need to put restrictions, $r^2$ of them. The reason is as follows. \par
Suppose that $\Lambda\Sigma_f\Lambda'$ is identifiable. So how do we determine individual parameters? This is tricky since for any matrix $A\in\mathbb{R}^{r\times r}$ s.t. $\det(A)\neq0$, 
\begin{align*}
\Lambda \Sigma_f \Lambda'&=\Lambda AA^{-1}\Sigma_f( AA^{-1})' \Lambda'\\
&=\underbrace{\Lambda A}_{\Lambda^*}\underbrace{A^{-1}\Sigma_fA^{-1'}}_{\Sigma_f^*}\underbrace{A' \Lambda'}_{\Lambda^{*'}}\\
&=\Lambda^* \Sigma_f^* \Lambda^{*'}\\
\end{align*}
We can get many observationally equivalent models. This is known as a rotational indeterminancy. Restrictions are needed to prevent this problem
\par
Typically, three sets of assumptions are imposed
\begin{itemize}
\item \textbf{Classical: } We impose $\Sigma_f = I_r$. That will take care of $\frac{r(r+1)}{2}$ restrictions ($\Sigma_f$ is symmetric). To take care of the rest, we impose $\Lambda'\Psi^{-1}\Lambda$ is diagonal - i.e off diagonals are symmetric (diagonal elements are still free). This takes care of $\frac{(r-1)r}{2}$ restrictions. Thus, 
\[
var(\mathbf{y}_t)=\Lambda\Lambda'+\Psi
\]
\item \textbf{Triangular: } Split $\Lambda$ into $\Lambda =\begin{bmatrix}\Lambda_1 \in\mathbb{R}^{r\times r}\\ \Lambda_2 \in\mathbb{R}^{(n-r)\times r}\end{bmatrix}$. Then we impose $\Sigma_f = I_r$ and $\Lambda_1$ be a lower triangular matrix. So the upper diagonal matrix of $\Lambda_1$ is restricted to 0. The sum of the number of both sets of restriction is $r^2$. This implies that
\begin{gather*}
y_{1t}= \lambda_{11}f_{1t}+u_{1t}\\
y_{2t}= \lambda_{21}f_{1t}+\lambda_{22}f_{2t}+u_{2t}\\
..\\
y_{rt}= \lambda_{r1}f_{1t}+....+\lambda_{rr}f_{rt}+u_{rt}
\end{gather*}
and $var(\mathbf{y}_t)=\Lambda\Lambda'+\Psi$. 
\item \textbf{Measurement error: } This involves an unrestricted $\Sigma_f$ and $\Lambda_1 = I_r$. Since $\Lambda_1$ is
\[
\Lambda_1= \begin{bmatrix}\lambda_{11} & \lambda_{12}& ... & \lambda_{1r}\\ \lambda_{21} & \lambda_{22}& ... & \lambda_{2r} \\ ... & ...& ... & ...\\ \lambda_{r1} & \lambda_{r2}& ... & \lambda_{rr}\end{bmatrix}
\]
imposing $\Lambda_1=I_r$ uses up all $r^2$ restrictions. Then we have
\begin{gather*}
y_{1t}= f_{1t}+u_{1t}\\
y_{2t}=f_{2t}+u_{2t}\\
..\\
y_{rt}=f_{rt}+u_{rt}
\end{gather*}
and hence the name measurement error restriction. Then 
\[
var(\mathbf{y}_t)=\begin{bmatrix}\Sigma_f & \Sigma_f\Lambda_2' \\ \Lambda_2\Sigma_f & \Lambda_2 \Sigma_f \Lambda_2' \end{bmatrix} + \Psi
\]
\end{itemize}
These assumptions solve rotational indeterminancy. To fully identify all parameters, additional identification assumptions are required. For this, I recommend referring to Professor Bai's lecture notes on Factor Analysis, available upon request. 
