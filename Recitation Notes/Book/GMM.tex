
%%%%%%%%%%%%%%%%%%
\chapter{Generalized Method of Moments}
GMM methods utilize the method of moments estimators to identify the values of the parameters of interest. It can be generalized in the sense that the number of moment conditions can be greater than the number of unknown parameters. In fact, we can show that OLS, IV, MLE estimators are part of GMM. 
\section{Framework}
The moment equation takes the following form: Let $w_i$ be IID across $i=1,..,n$,  $g_i(w_i, \theta)$ be a $l\times1$ function of the $i$th observation, and $\theta\in\mathbb{R}^{k\times1}$ be the parameter of interest.  Note that $l\geq k$. Then, the \textbf{moment equation model} is characterized by
\[
E[g(w_i,\theta)]=0
\]
 We say $\theta$ is identified if there is a unique mapping from the data distribution to $\theta$, or that there is a unique $\theta$ satisfying $E[g(w_i,\theta)]=0$. When $l=k$, then we are in a just-identified case. If $l>k$, then we are in the over-identified case in the sense that there is excess information. We will not bother with the case of $l<k$, as they are under-identified and the moment equation model is unsolvable. \par
\begin{itemize}
\item \textbf{Just-identified}: In this case, we can work with the sample analogue of $g(w_i.\theta)$ straight away. Define $\bar{g}_n(\theta)$ as 
\[
\bar{g}_n(\theta)=\frac{1}{n}\sum_{i=1}^ng_i(\theta)
\]
The \textbf{method of moments estimators} $\hat{\theta}_{MM}$ is defined as the parameter value which sets $\bar{g}_n(\theta)=0$. In other expression:
\[
\bar{g}_n(\hat{\theta}_{MM})=\frac{1}{n}\sum_{i=1}^ng_i(\hat{\theta}_{MM})=0
\] 
\item \textbf{General Case}: If we can generalize to include the case where $l>k$, we are likely to run into a situation where $\hat{\theta}_{MM}$ cannot be found. This is because there may be no choice of $\theta$ that sets the moment equations to 0, implying the nonexistence of the method of moments estimator. In such case, we work with a different approach. 
\par
Define $J(\theta)$ as
\[
J(\theta)=n \bar{g}_n(\theta)'W\bar{g}_n(\theta)
\]
where $W\in\mathbb{R}^{l\times l}$ is a positive definite weight matrix that is given. $n$ does not really affect our estimation, but it makes the analysis of the asymptotic features much easier. The \textbf{generalized method of moments estimator} is defined as the minimizer of the GMM criterion above, or
\[
\hat{\theta}_{GMM}=\arg\min_\theta J_n(\theta)
\]
\end{itemize}\par
Note that when $l=k$, then method of moments estimator solve $\bar{g}_n(\hat{\theta}_{MM})=0$. Given that $J(\theta)$ is a positive definite matrix, the method of moments estimator in this case also minimizes $J(\theta)$. Thus, method of moments estimator is a special case of GMM estimator. \par
If $l>k$, then the problem we should solve is
\[
\begin{aligned}
\min_\theta J_n(\theta)&=n\bar{g}(\theta)'W\bar{g}(\theta)\\
\implies\frac{\partial J_n(\theta)}{\partial \theta}& =2n\frac{\partial \bar{g}(\theta)}{\partial\theta}'W\bar{g}(\theta)=0\\
\end{aligned}
\]\par 
To obtain this, I have used the following properties of matrix differentiation.
\begin{mdframed}[backgroundcolor=green!5] 
\begin{property}[Matrix Differentiation Tricks]
Let Let $x\in\mathbb{R}^{k\times 1}$ and $A\in\mathbb{R}^{k\times l}, y\in\mathbb{R}^{l\times 1}$. Then we have
\[
\frac{\partial x'Ay}{\partial x}=Ay, \frac{\partial x'Ay}{\partial y}=A'x
\]Let $x\in\mathbb{R}^{k\times 1}$ and $A\in\mathbb{R}^{k\times k}$. Then the following holds
\[
\frac{\partial x'Ax}{\partial x}=(A+A')x
\]
\end{property}
\end{mdframed}


\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{example}[Various Examples of GMM] These are some types of a GMM estimator. 
\begin{itemize}
\item OLS: In a data generating process $y_i = x_i'\beta+e_i,\ x_i\in\mathbb{R}^k$ and the moment condition $E(x_ie_i)=0$, we have $k$ parameters $\beta_k$  and $k$ equations for each of the $k$ variables. We can rewrite the moment condition as
\[
E(x_i(y_i-x_i'\beta))=0
\]
And the method of moments estimators imply that we should solve 
\[
\frac{1}{n}\sum_{i=1}^nx_i(y_i-x_i'\beta)=0\iff \hat{\beta}_{OLS}=\left(\frac{1}{n}\sum_{i=1}^n x_ix_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n x_iy_i
\]
\item MLE: If $w_i$ is IID across $i=1,...,n$, then we can write the joint likelihood function as
\[
\prod_{i=1}^nf(w_i|\theta)
\]
and thus, the log-likelihood function
\[
\sum_{i=1}^n \log f(w_i|\theta)
\]
When we take partial differentiation w.r.t $\theta$, 
\[
\sum_{i=1}^n \frac{\partial \log f(w_i|\theta)}{\partial \theta}=0 \implies \frac{1}{n}\sum_{i=1}^n \frac{\partial \log f(w_i|\theta)}{\partial \theta}=0 
\]
which is equivalent to $E\left(\frac{\partial \log f(w_i|\theta)}{\partial \theta}\right)=0$. Practically, $E\left(\frac{\partial \log f(w_i|\theta)}{\partial \theta}\right)=0$ becomes the moment condition applicable to MLE.
\item IV: Suppose we have a data generating process $y_i=x_i'\beta+e_i$ with $x_i$ being $k$ dimensions.  Suppose we have an $l>k$ dimensional IV with $E(z_ie_i)=0$  $\bar{g}(\beta)$ in our context would be
\[
\frac{1}{n}\sum_{i=1}^n(z_iy_i-z_ix_i'\beta) = \frac{Z'y}{n}-\frac{Z'X\beta}{n}
\]
Then, we can write our $J_n(\beta)$ as
\[
n\left(\frac{Z'y}{n}-\frac{Z'X\beta}{n} \right)'W\left(\frac{Z'y}{n}-\frac{Z'X\beta}{n} \right)
\]
By solving the minimization problem we can obtain
\[
\begin{aligned}
\frac{\partial J_n(\beta)}{\partial \beta}&=-\frac{2}{n}(X'ZWZ'y)+\frac{2}{n}(X'ZWZ'X)\beta=0\\
&\implies\hat{\beta}=(X'ZWZ'X)^{-1}(X'ZWZ'y)\\
\end{aligned}
\]
In fact, we can show that by setting $W=\left(\frac{Z'Z}{n}\right)^{-1}$, the above estimator is in fact, a 2SLS estimator. 
\end{itemize}
\end{example}
\end{mdframed}
\section{Limiting Distribution of GMM}
From above, we know that $\hat{\beta}_{GMM}=(X'ZWZ'X)^{-1}(X'ZWZ'y)$ for the case of an overidentified IV model. We can rewrite this by replacing $y$ with $X\beta+e$, As a result, the limiting distribution of $\hat{\beta}_{GMM}$ is characterized by
\[
\sqrt{n}(\hat{\beta}_{GMM}-\beta)=\left(\frac{X'Z}{n}W\frac{Z'X}{n}\right)^{-1}\left(\frac{X'Z}{n}W\frac{Z'e}{\sqrt{n}}\right)
\]
For convenience, I assume the following
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}
Assume that
\begin{enumerate}
\item $E(z_ix_i')=Q$, and that $\frac{Z'X}{n}\xrightarrow{p}Q$
\item $\frac{Z'e}{\sqrt{n}}\xrightarrow{d}N(0,\Omega)$, where $\Omega = E(z_iz_i'e_i^2)$
\item (If we are willing to assume $W$ depends on $n$, thus $W_n$): $W_n\xrightarrow{p}W$, where $W$ is a positive definite weight matrix
\end{enumerate}
\end{assumption}
\end{mdframed}
Then we can say the following about the distribution of the GMM estimator
\begin{mdframed}[backgroundcolor=green!5] 
\begin{theorem}[Limiting Distribution of the GMM estimator] If the above assumptions are satisfied, the limiting distribution of the GMM estimator can be characterized by
\[
\sqrt{n}(\hat{\beta}_{GMM}-\beta)\xrightarrow{d}N(0,(Q'WQ)^{-1}(Q'W\Omega W'Q)(Q'WQ)^{-1})
\]
Where $(Q'WQ)^{-1}$ can be replaced with $A$, if we stick to the notation in class. 
\end{theorem}
\end{mdframed}\par
So far, we haven't said too much about $W$. We just assumed that they are fixed. Even if we suppose that $W$ depends on $n$ somehow, the above theorem still holds, provided that $W_n$ converges in probability to $W$. Then, the natural question should be ``What is the optimal selection of $W$?". This naturally moves us to...
\section{Efficient GMM}
To select an optimal $W$ matrix, it must be that the resulting variance should be the smallest. If we let $W=\Omega^{-1}$ and work with $(Q'WQ)^{-1}(Q'W\Omega W'Q)(Q'WQ)^{-1}-(Q'\Omega Q)^{-1}$, we can see that it is positive semidefinite\footnote{This is your homework, so I will not proceed further with any more explanations.}.
If we put $W=\Omega^{-1}$ and  recalculate the variance, we get that the efficient GMM has a limiting distribution characterized by
\[
\sqrt{n}(\hat{\beta}_{GMM}-\beta)\xrightarrow{d}N(0,(Q'WQ)^{-1})
\]\par
Note that this weighting matrix, which can be rewritten as
\[
W=\Omega^{-1}=E(z_iz_ie_i^2)^{-1}
\]
is not exactly same as the weighting matrix we used for deriving the 2SLS estimator from GMM, which is $\left(\frac{Z'Z}{n}\right)^{-1}$. In the $W=E(z_iz_ie_i^2)^{-1}$ setup, we allowed for heteroskedasticity. If we impose conditional homoskedasticity in the sense that $E(e_i^2|z_i)=\sigma^2$, we can rewrite $E(z_iz_i'e_i^2)$ as
\[
\begin{aligned}
E(z_iz_i'e_i^2)&=E(E(z_iz_i'e_i^2|z_i))=E(z_iz_i'E(e_i^2|z_i))\\
&=E(z_iz_i'\sigma^2)=E(z_iz_i')\sigma^2\\
\end{aligned}
\]
Thus, $E(z_iz_i'e_i^2)^{-1}=(Z'Z)^{-1}(\sigma^2)^{-1}$. However, scaling does not affect how the estimator is defined (by calculating $\hat{\beta}_{GMM}$ the scalar scales disappear). So $E(z_iz_i'e_i^2)^{-1}$ is effectively $(Z'Z)^{-1}$ and we get the conclusion that in a conditional homoskedastic setting, 2SLS estimator is the efficient GMM. 
\section{Two-step Optimal GMM}
Since we have no idea what the true value of $e_i$ is, the true value of $\Omega$ is naturally unknown. Therefore, we require a consistent estimator, denoted as $\widehat{W}$, for $W=\Omega^{-1}$. To do so, we need a preliminary consistent estimator for the true $\theta$, which I will denote with $\tilde{\theta}$. You can use any $W$ matrix for this step ($I$ or $(Z'Z)^{-1}$). Then, define
\[
\widehat{\Omega}=\frac{1}{n}\sum_{i=1}^n g(w_i.\tilde{\theta})g(w_i.\tilde{\theta})'
\]
since $\Omega = E[g(w_i,\theta)g(w_i,\theta)']$. Then we can identify what $\widehat{\Omega}^{-1}$ is. This will allow us to compute the efficient GMM estimator $\hat{\theta}_{GMM}$
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{definition}[Optimal Two-step GMM Estimator] We can compute Optimal Two-step GMM in these steps
\begin{enumerate}
\item Compute a preliminary, but consistent estimator for the true $\theta$. Denote this as $\tilde{\theta}$. 
\item Using $\Omega=E[g(w_i,\theta)g(w_i,\theta)']$, create a sample analogue of this, defined as $\widehat{\Omega}=\frac{1}{n}\sum_{i=1}^n g(w_i.\tilde{\theta})g(w_i.\tilde{\theta})'$. We can find our $\widehat{\Omega}^{-1}$ here. 
\item Using this $\widehat{\Omega}^{-1}$, construct an efficient GMM estimator $\hat{\theta}_{GMM}$
\end{enumerate}
\end{definition}
\end{mdframed}\par
There is another way to come up with a $\widehat{\Omega}^{-1}$ in this context. Define $\bar{g}(\theta)=\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})$. Then, an alternative definition of $\widehat{\Omega}$ can be written as
\[
\widehat{\Omega}^+=\frac{1}{n}\sum_{i=1}^n(g(w_i,\tilde{\theta})-\bar{g}(\theta))(g(w_i,\tilde{\theta})-\bar{g}(\theta))'
\]
So why use this? For one thing, both $\widehat{\Omega}$ and $\widehat{\Omega}^+$ converge in probability to $E[g(w_i,\theta)g(w_i,\theta)']$. This is because
\begin{gather*}
\frac{1}{n}\sum_{i=1}^n(g(w_i,\tilde{\theta})-\bar{g}(\theta))(g(w_i,\tilde{\theta})-\bar{g}(\theta))'\\
=\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})g(w_i,\tilde{\theta})'-\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})\bar{g}(\theta)'-\frac{1}{n}\sum_{i=1}^n\bar{g}(\theta)g(w_i,\tilde{\theta})'+\frac{1}{n}\sum_{i=1}^n \bar{g}(\theta)\bar{g}(\theta)'\\
=\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})g(w_i,\tilde{\theta})'- \bar{g}(\theta)\bar{g}(\theta)' \ \ \left(\because \bar{g}(\theta)=\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})\right)\\
\end{gather*}
Under the assumption that $E[g(w_i,\theta)]=0, (\text{or }E(z_ie_i)=0)$ is a correct specification, we have $\bar{g}(\theta)\xrightarrow{p}E(g(w_i,\theta))=0$. So $\widehat{\Omega}^+$ and $\widehat{\Omega}$ both converge to $E[g(w_i,\theta)g(w_i,\theta)']$.
\par
The difference? If it is the case that $E[g(w_i, \theta)]\neq0$ we view $\widehat{\Omega}^+$ as a robust estimator. Note that
\begin{itemize}
\item $\widehat{\Omega}=\frac{1}{n}\sum_{i=1}^n g(w_i.\tilde{\theta})g(w_i.\tilde{\theta})'\xrightarrow{p}E[g(w_i,\tilde{\theta})g(w_i,\tilde{\theta})']$
\item  $\widehat{\Omega}^+=\frac{1}{n}\sum_{i=1}^ng(w_i,\tilde{\theta})g(w_i,\tilde{\theta})'- \bar{g}(\theta)\bar{g}(\theta)' \xrightarrow{p} E[g(w_i,\tilde{\theta})g(w_i,\tilde{\theta})']-E[g(w_i,\tilde{\theta})]E[g(w_i,\tilde{\theta})]'=var[g(w_i,\tilde{\theta})]$
\end{itemize}
In other words, $\widehat{\Omega}$ is inconsistent in case where $E[g(w_i,\tilde{\theta})]=0$ is not guaranteed. Therefore, for tests, such as overidentification tests, it is much more desirable to use $\widehat{\Omega}^+$.\par
Since we know how to find the optimal $\widehat{W}$, we can estimate the asymptotic variance of the GMM estimators. This can be done by replacing matrices in the original variance with their sample counterparts. In general, we can estimate by
\[
\widehat{V}_{GMM}=\left(\widehat{Q}'\widehat{W}\widehat{Q}\right)^{-1}\left(\widehat{Q}'\widehat{W}\widehat{\Omega}\widehat{W}\widehat{Q}\right)\left(\widehat{Q}'\widehat{W}\widehat{Q}\right)^{-1}
\]
where $\widehat{Q}=\frac{1}{n}\sum_{i=1}^n z_ix_i' = \frac{Z'X}{n}$, $\widehat{W}$ is expressed by either $\widehat{\Omega}$ or $\widehat{\Omega}^+$. The residuals used in this estimation is defined as $\hat{e}_i = y_i- x_i'\hat{\beta}_{GMM}$ \par
One alternative to the two-step GMM estimator is contructed by letting weight matrix be an explicit function $\theta$. The criterion function is now
\[
J(\theta)=n\bar{g}_n(\theta)'\left(\frac{1}{n}\sum_{i=1}^n g(w_i,\theta)g(w_i,\theta)'\right)^{-1}\bar{g}_n(\theta)
\]
The $\hat{\theta}$ that minimizes this function is the continuously-updated GMM estimator. However, this setup is nonlinear, implying that acquiring this estimator requires numerical methods. 
%%%%%%%%%%%%%%%
\section{Wald Test Statistics}
Assume that you have found any kind of a GMM estimator. So we are not limiting our discussion to the efficient GMM. The GMM estimator $\hat{\beta}_{GMM}$ that you found has a limiting distribution that can be characterized as
\[
\sqrt{n}(\hat{\beta}_{GMM}-\beta)\xrightarrow{d}N(0,V_\beta)
\]\par
Now, we define a fuction $r(\beta):\mathbb{R}^k\xrightarrow{p} \Theta\in\mathbb{R}^q$ that characterizes the type of limitations we put on our parameter of interest $\beta$. I will write $\theta=r(\beta)$. The GMM estimator of $\theta$ would naturally be $\hat{\theta}_{GMM}=r(\hat{\beta}_{GMM})$. Then, by delta method, we can characterize the limiting distribution of $\hat{\theta}_{GMM}$ as
\[
\sqrt{n}(\hat{\theta}_{GMM}-\theta)\xrightarrow{d}R'N(0,V_\beta) = N(0,\underbrace{R'V_\beta R}_{=V_\theta})
\]
where $R = \frac{\partial r(\beta)'}{\partial \beta}\in\mathbb{R}^{k\times q}$. \par
The hypothesis test can be set up in the following manner
\[
H_0 : \theta= \theta_0, \ \ \ H_1 : \theta \neq \theta_0
\]
Here, we make use of the Wald test statistic, defined as
\[
W\equiv n(\hat{\theta}-\theta)'\widehat{V}_{\theta}^{-1} (\hat{\theta}-\theta)
\]
where $W\xrightarrow{d}\chi_q^2$ under $H_0$. If we conduct a test with a significance level $\alpha$, we need to find a critical value $C$ such that $\alpha=1-F(C)$ where $F$ is the CDF for $\chi^2_{q}$. In such test, we reject $H_0$ if $W>C$ and do not reject if otherwise. \par

\section{Restricted GMM}
There are many cases where we may want to impose restrictions on the coefficients (e.g. hypothesis testing). In this section, we will consider $r(\beta)=0$ as a constraint on $\beta$. Finding a restricted GMM (or constrained GMM) is not too different from the unconstrained GMM we have been solving so far. The only difference is that we are now solving a constrained minimization problem.  The constrained GMM estimator, in math, is the solution to the following problem
\[
\hat{\beta}_{CGMM}=\arg\min_\beta J_n(\beta) \ \text{s.t. } r(\beta)=0
\]
\par For instance, we may consider a linear constraint on $\beta$ coefficients in the form of $R'\beta=c$, where $R\in\mathbb{R}^{k\times q}$ with $q$ being number of constraints. We can write this out in a Lagrangian form.
\[
n\bar{g}(\beta)'W\bar{g}(\beta) + \lambda' [R'\beta-c]
\]
where $\lambda\in\mathbb{R}^q$ is the vector of Lagrange multipiers.\par
Suppose that the moment condition given is $E(z_ie_i)=0$. Then 
\[
\bar{g}(\beta)=\left(\frac{Z'y}{n}-\frac{Z'X\beta}{n}\right)
\] To find the optimal $\beta$, we differentiate the objective function with respect to $\beta$. This will become
\begin{gather*}
\frac{y'ZWz'Y}{n}-\frac{y'ZWZX\beta}{n}-\frac{\beta'X'ZWZ'y}{n}+\frac{\beta'X'ZWZ'X\beta}{n}+\lambda'R'\beta-\lambda'C\\
\xrightarrow{\partial\beta} -2\frac{X'ZWZ'y}{n}+2\frac{X'ZWZ'X\beta}{n}+R\lambda =0
\end{gather*}
We can use the knowledge that the unconstrained GMM can be written as \small{$\left(\frac{X'ZWZ'X}{n}\right)^{-1}\frac{X'ZWZ'y}{n}$}\normalsize. So the trick is to pre-multiply $-R'\left(\frac{X'ZWZ'X}{n}\right)^{-1}$. This would change the first order condition to
\[
2R'\hat{\beta}_{GMM}-2R'\beta-R'\left(\frac{X'ZWZ'X}{n}\right)^{-1}R\lambda =0
\]
Since $R'\beta=c$ under $H_0$, we write 
\[
\lambda = 2\left(R'\left(\frac{X'ZWZ'X}{n}\right)^{-1}R\right)^{-1}R'(\hat{\beta}_{GMM}-\beta)
\]
We can then put this back into the $\lambda$ in the first order condition to get
\[
-2\frac{X'ZWZ'y}{n}+2\frac{X'ZWZ'X\beta}{n}+2R\left(R'\left(\frac{X'ZWZ'X}{n}\right)^{-1}R\right)^{-1}R'(\hat{\beta}_{GMM}-\beta)=0
\]
Note that in this equation, $n$ is commonly divided in all terms and $2$ is multiplied in all terms. So we can practically get rid of them. So we can further rewrite the above as
\small{\[
(X'ZWZ'X)\beta-R(R'(X'ZWZ'X)^{-1}R)^{-1}\underbrace{R'\beta}_{=c}=X'ZWZ'y-R(R'(X'ZWZ'X)^{-1}R)^{-1}R'\hat{\beta}_{GMM}
\]}\normalsize
Or
\begin{gather*}
(X'ZWZ'X)\beta=X'ZWZ'y-R(R'(X'ZWZ'X)^{-1}R)^{-1}(R'\hat{\beta}_{GMM}-c)\\
\implies\hat{\beta}_{CGMM}=\hat{\beta}_{GMM}-(X'ZWZ'X)^{-1}R(R'(X'ZWZ'X)^{-1}R)^{-1}(R'\hat{\beta}_{GMM}-c)
\end{gather*}
\par 
Note that if we premultiply $R'$ to both sides, we can get
\[
\begin{aligned}
R'\hat{\beta}_{CGMM}&=R'\hat{\beta}_{GMM}-R'(X'ZWZ'X)^{-1}R(R'(X'ZWZ'X)^{-1}R)^{-1}(R'\hat{\beta}_{GMM}-c)\\
&=R'\hat{\beta}_{GMM}-R'\hat{\beta}_{GMM}+c=c\\
\end{aligned}
\]
which shows that the CGMM satisfies the constraint. 
\section{Distance Test}
When we looked at the Wald Test Statistics, we assumed a linear form of constraints for $r(\beta)=\theta$. If we are working with a possibly nonlinear form of $r(\beta)$, we can use an alternative criterion-based statistic. This is where the GMM Distance statistic comes in. The basic idea is to compare unrestricted and restricted estimators by contrasting the criterion functions. Again, we define
\[
J(\beta)=n\bar{g}_n(\beta)'\widehat{\Omega}^{-1}\bar{g}_n(\beta)
\]
where $\bar{g}_n(\beta)$ is $\frac{1}{n}\sum_{i=1}^n g(w_i,\beta)$ and $\widehat{\Omega}$ is the efficient weight matrix. With the unconstrained estimator and the constrained estimator, we can write
\begin{gather*}
J(\hat{\beta}_{GMM})=n\bar{g}_n(\hat{\beta}_{GMM})'\widehat{\Omega}^{-1}\bar{g}_n(\hat{\beta}_{GMM})\\
J(\hat{\beta}_{CGMM})=n\bar{g}_n(\hat{\beta}_{CGMM})'\widehat{\Omega}^{-1}\bar{g}_n(\hat{\beta}_{CGMM})\\
\end{gather*}\par
Now we have all the ingredients to define the distance statistic $D$, defined as
\[
D\equiv J(\hat{\beta}_{CGMM})-J(\hat{\beta}_{GMM})
\]
Note that $D\geq0$ by construction. This is because $J(\hat{\beta}_{CGMM})$ is from the minimization problem that is constrained and the other is an unconstrained problem, which usually results in lower values.\par
As we did for hypothesis testing on Wald test statistic, we can use $D$ for hypothesis tests of the following setup
\[
H_0: r(\beta)=\theta, \ \ H_1: r(\beta)\neq \theta
\]
Under $H_0$, $D$ converges in distribution to $\chi_q^2$. We can find a critical value $c$ s.t. $\alpha = 1-F(c)$, where $F$ is the CDF for $\chi_q^2$ and reject $H_0$ if $D>c$. The idea is that if $H_0$ is true, then imposing the restriction does not alter the moment equations greatly, making it a sensible restriction. Otherwise, the moment equation changes greatly, making it unreasonable constraint. \par
\par
 In fact, we can show that when $r(\beta)$ is linear, $D$ becomes identical to the Wald Test Statistic $W$. We have shown what $\hat{\beta}_{CGMM}$ is. Then we can write 
 \[
 \begin{aligned}
 \bar{g}_n(\hat{\beta}_{CGMM})&=\frac{Z'y-Z'X\hat{\beta}_{CGMM}}{n}\\
 & =\frac{\overbrace{Z'y-Z'X(\hat{\beta}_{GMM}}^{=\bar{g}_n(\hat{\beta}_{GMM})}-\overbrace{(X'ZWZ'X)^{-1}R(R'(X'ZWZ'X)^{-1}R)^{-1}(R'\hat{\beta}_{GMM}-c))}^{=\hat{\beta}_{GMM}-\hat{\beta}_{CGMM}}}{n}\\
 &=\bar{g}_n(\hat{\beta}_{GMM})+\frac{Z'X(\hat{\beta}_{GMM}-\hat{\beta}_{
 CGMM})}{n}\\
 \end{aligned}
 \]
 Thus,
 \small{\[
 \begin{aligned}
 J(\hat{\beta}_{CGMM})&=n\left(\bar{g}_n(\hat{\beta}_{GMM})+\frac{Z'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})}{n}\right)'W\left(\bar{g}_n(\hat{\beta}_{GMM})+\frac{Z'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})}{n}\right)\\
 &=n\bar{g}_n(\hat{\beta}_{GMM})'W\bar{g}_n(\hat{\beta}_{GMM})+\bar{g}_n(\hat{\beta}_{GMM})'WZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})\\
 &+(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})'X'ZW\bar{g}_n(\hat{\beta}_{GMM})+\frac{1}{n}(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})'X'ZWX'Z(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})
 \end{aligned}
 \]}\normalsize
 where $\bar{g}_n(\hat{\beta}_{GMM})'WZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})$ terms can be erased because
 \footnotesize{\[
 \begin{aligned}
 \bar{g}_n(\hat{\beta}_{GMM})'WZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})&=\frac{y'ZWZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})-\hat{\beta}_{GMM}'X'ZWZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})}{n}\\
 &=\frac{y'ZWZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})-y'ZWZ'X(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})}{n}=0\\\
 \end{aligned}
 \]}\normalsize
since $\hat{\beta}_{GMM}=(X'ZWZ'X)^{-1}X'ZWZ'y$. Therefore, 
\[
J(\hat{\beta}_{CGMM})=J(\hat{\beta}_{GMM})+\frac{1}{n}(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})'X'ZWX'Z(\hat{\beta}_{GMM}-\hat{\beta}_{CGMM})
\]
So $J(\hat{\beta}_{CGMM})- J(\hat{\beta}_{GMM})$ becomes a Wald Test Statistic.
\section{Overidentification Tests}
In this section, we generalize the overidentification test we learned in 2SLS setup to a GMM setting. It is generalized in the sense that we can allow for heteroskedasticity whereas for 2SLS overidentification test, we relied on the assumption of homoskedasticity. If $\dim(g_i)=l>k=\dim(\beta)$, it is possible that there exists no $\beta$ s.t. $E[g(w_i,\beta)]=0$ is satisfied. Therefore, the overidentifying restrictions become testable.  Effectively, we are testing the hypothesis
\[
H_0: E[g(w_i,\beta)]=0 \ \text{vs. } H_1: E[g(w_i,\beta)]\neq0
\]\par
Let $\beta_0$ be the true value for the parameter of interest. Then $\bar{g}_n(\beta_0)=\frac{Z'y-Z'X\beta_0}{n}=\frac{Z'e}{n}$ has a limiting distribution characterized by
\[
\sqrt{n}\bar{g}_n(\beta_0) \xrightarrow{d}N(0,\Omega),\ \ \ \Omega\in\mathbb{R}^{l\times l}
\]
and thus $J(\beta_0)\xrightarrow{d}\chi^2_l$. Since we are trying to estimate what $\beta_0$ is, we need to use a GMM estimator to build our test statistic $J=J(\hat{\beta}_{GMM})$, which has a limiting distribution $\chi^2_{l-k}$ under $H_0$. Like before, we can find $c$ s.t. $\alpha=1-F(c)$, and then reject $H_0$ when $J>c$. However, it should be noted that when the $H_0$ is rejected, we only know that some moment condition is violated. We cannot pick out which. Nevertheless, rejection of the $H_0$ is a bad sign. \par
We can apply overidentification test on a (strict) subset of instruments whose validity is uncertain. To do this, we can partition $z_i$ into two sets - $z_{ai}\in\mathbb{R}^{l_a}$ and $z_{bi}\in\mathbb{R}^{l_b}$. We are uncertain about $z_{bi}$ and want to test 
\[
H_0: E(z_{bi}e_i)=0, \ \text{vs. }H_1:E(z_{bi}e_i)\neq0
\]\par
We can construct the test statistic in a following manner. First, estimate the model by the efficient GMM with only the $z_{ai}$ set of instruments and obtain the GMM criterion. This will be denoted as $J_a$. Then, estimate the model with the full set of instruments and obtain a separate GMM criterion, denoted as $J_{a,b}$. Then, create a test statistic
\[
C=J_{a,b}-J_a\xrightarrow{d}\chi^2_{l-l_a=l_b}
\]
The idea is similar to that of a distance test. If the $C$ value is low, then the moment condition for the larger set of instruments are not too different from the moment condition from the smaller (but surer) set of instruments. Then, we find a critical value $c$ for a significance level $\alpha$ and reject the null hypothesis if $C>c$. 
\par
One example of a subset overidentification test is an endogeneity test. Assume a following data generating process
\[
y_i=x_{1i}'\beta_1+x_{2i}'\beta_2+e_i
\]
where $x_{1i}$ is exogenous but $x_{2i}$ may not be. Then, we want to test
\[
H_0: E(x_{2i}e_i)=0, \ \text{vs. }H_1:E(x_{2i}e_i)\neq0
\]\par
Let $z_i = (x_{1i} \ \ \ z_{2i})'\in\mathbb{R}^{l}$ and assume that $E(z_ie_i)=0$. Then we can create the test statistics as follows. First, estimate the efficient GMM by using $(x_{1i}, z_{2i})$ to instrument $(x_{1i}, x_{2i})$. Then obtain the GMM criterion $\tilde{J}$. Next, work with a larger set by using $(x_{1i}, x_{2i}, z_{2i})$ to instrument $(x_{1i}, x_{2i})$. After this, we can get the GMM criterion $\widehat{J}$. The relevant test statistic is
\[
C = \widehat{J}-\tilde{J} \xrightarrow{d} \chi^2_{k_2}
\]
As before, we find a relevant critical value $c$ for a significance level $\alpha$ and reject $H_0$ of exogeneity for $x_2$ if $C>c$. \par

Again, the idea is as follows. We know that $x_{1i}$ and $z_{2i}$ are exogenous. If $x_{2i}$ is also exogenous, then the GMM criterion obtained by using all three should not be too different from GMM criterion from variables $x_{1i}$ and $z_{2i}$\footnote{It can be slightly different, since the weighting matrix used in calculating the GMM criterion is usually different. This can lead to some discrepancies even if $H_0$ turns out to be true. }. If the difference between the two GMM criterion is sufficiently large, we have a reason to suspect that, given that $x_{1i}$ and $z_{2i}$ is exogenous for sure, that the moment condition involving $x_{2i}$ variables are vastly different from moment conditions from exogenous variables. Thus, $x_{2i}$ could be endogenous.  \par

\section{Conditional Moment Conditions}
In some cases, the moment condition for the data generating process can be expressed in terms of conditional moments. Assume that the data generating process and conditional moment condition of interest is
\[ 
y_i =m( x_i,\beta^0 )+ e_i \ \ \ E(e_i(\beta)|z_i)=0
\]
where $\beta^0$ is the true value of $\beta$, $z_i$ is the instrument and $m(x_i,\beta)$ can be either linear or nonlinear. Usually, these conditions are more stronger in the sense these sets of conditions imply that the unconditional moment conditions assumed throughout the discussion holds. In fact, we can show that if $E(e_i|z_i)=0$, then $E(h(z_i)e_i)=0$ for measurable function $h(\cdot)$. The other way may not hold.
\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{example} Suppose that $E(z_ie_i)=0$, then is it the case that $E(e_i|z_i)=0$ hold? There is this counterexample: Let $z_i$ and $e_i$ be characterized as
\[
z_i=\begin{cases} 1 & \Pr(z_i=1)=1/2 \\ 2 & \Pr(z_i=2)=1/2 \end{cases}, \ \  e_i = \begin{cases} 1 & (\text{if }z_i=1)\\ -1/2 & (\text{if }z_i=2)\\\end{cases}
\]
Then $E[z_ie_i]$  is $\frac{1}{2}(1\times 1) +  \frac{1}{2}(2\times (-1/2))= \frac{1}{2}-\frac{1}{2}=0$. As for $E[e_i|z_i]$, we get $E[e_i|z_i=1]=1$ and $E[e_i|z_i=2]=-1/2$. Therefore, the conditional mean is not 0. 
\end{example}
\end{mdframed} \par
So how do we solve when given conditional moments? One is to use the idea that $E[e_i|z_i]=0\implies E[h(z_i)e_i]=0$ to construct some number of instruments and solve a GMM with unconditional moments\footnote{There is no consensus on the optimal number of optimal instruments yet}.  The other is to construct an optimal instrument as Chamberlain (1987) found. For this, we need to define these equations
\begin{gather*}
R(z_i) = E\left[\frac{\partial e_i(\beta^0)}{\partial \beta}\mid z_i\right] \in\mathbb{R}^k \\
\sigma^2(z_i) = E[e_i(\beta^0)^2|z_i]
\end{gather*}
Then, the optimal instrument is defined by
\[
A_i = -\frac{R(z_i)}{\sigma^2(z_i)}\in\mathbb{R}^k
\]
yielding the optimal moment
\[
g^*_i(\beta) = A_i e_i(\beta)
\]
Thus, by working with the $E[g^*_i(\beta)]=0$ gives us the best GMM estimator possible (best being the lowest possible variance). In class, we have shown that if we actually knew the true $\sigma^2$, this gets us the GLS estimator.



