
%%%%%%%%%%%%%%%%%%
\chapter{Treatment Effects: Regression Discontinuity}
\section{Why do We Want Regression Discontinuity?}
\textbf{Regression discontinuity} (commonly abbreviated to RD or RDD) approach actually started from Psychology in 1960s and was brought into the mainstream of applied economics very recently. Whenever you learn applied microeconomics class in your second year, you are bound to read or present about a paper that uses this research design. \par
As its name suggests, this approach exploits the discontinuities in the treatment assignment. So the key difference between regression discontinuity and previous frameworks is that RDD does not require the overlap condition in the sense that $p(X_i)\in(0,1)$. In fact, one of the design has $p(X_i)=1$ for $X_i\geq c$ and $0$ otherwise. Another thing that RDD should satisfy is that for every other covariates, the distribution should remain continuous even through the discontinuity.\par
This is usually presented visually through graphs - one with the covariate that determines assignment to the treatment, usually called the forcing (running) variable $W_i$ and the treatment status $D_i$, the other with the $W_i$ and all other covariates $X_i$. For instance, if we are in the public finance context and study the effect of a particular welfare program with a means-tested benefit assignment mechanism, we could use the means-tested criteria as our $W_i$. If we are to study the effect of a specific educational program offered to those below a particular test score, that score could be the $W_i$. Here are some works that deal use RDD. 


\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{example}[Lee, 2007] This paper focuses on the elections won and lost by a very narrow margin. The logic is that in such case, winning and losing is effectively randomized. The running variable is the Democrat vote share $-$ Republican vote share, or a margin of victory/loss. One of the findings is that Democrats who barely win an election are much more likely to run for office and succeed in the next election, compared to Democrats who barely lose. In general this paper finds positive relationship between margin of victory and electoral outcomes, both in terms of vote shares and wins by individuals and parties. 
\end{example}


\begin{example}[Howell, 2017] This paper assess the impact of R\&D subsidies from the government to new ventures. The rank on the application to the R\&D is used as a running variable, (and this is a sharp RDD and a discrete running variable\footnote{Because of the discreteness, McCrary test is infeasible.}).  The paper finds that an early-stage award approximately doubles the probability that a firm receives subsequent venture capital (summarized in the figure below) and has large, positive impacts on patenting and revenue. These effects are stronger for more financially constrained firms.
\end{example}
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth, keepaspectratio]{RD_fig.png}
\caption{Reception of venture capital, before and after}
\end{figure}
\begin{example}[Ost, Pan \& Weber, 2018] This paper estimates the return to college using admin data on college enrollment and earnings from Ohio. The running variable here is a GPA, in that this paper uses the fact that colleges dismiss low-performing students on certain GPAs. RDD is used to measure effects of college on earnings. This paper finds that dismissal leads to a short-run increase in earnings and tuition savings, which are wiped out by gains from college graduates 8 years after. They also carry out the test that the result is not driven by manipulation.
\end{example}
\end{mdframed}


\section{Framework}
We will start with a \textbf{sharp RDD}. You can think of this case as a case where the assignment into the treatment is determined by
\[
D_i=1(W_i\geq c(X_i))
\]
where $c(X_i)$ can be a known function of $X_i$ or just a constant. We also keep the potential outcome framework from before, but we will slightly change this to
\begin{align*}
Y_i& = D_iY_i(1)+(1-D_i)Y_i(0)\\
&=Y_i(0)+D_i(Y_i(1)-Y_i(0))\\
&=Y_i(0)+1(W_i\geq c(X_i))\cdot(Y_i(1)-Y_i(0))
\end{align*}
We also keep the regressional framework for the potential outcomes, $Y_i(d)=\mu(X_i,W_i,d)+\epsilon_i(d)$. So we have
\[
E[Y_i(d)|X_i,W_i]=\mu(X_i.W_i.d)
\]
One key assumption that we will carry through this section is the continuity of the conditional expectation on potential outcomes at $W_i=c(X_i)$
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}[Continuity of $\mu(X_i,W_i,d)$ at $W_i=c$] If $W_i$ has a known cutoff at $c$, we assume that $E[Y_i(1)|X_i, W_i]$ and $E[Y_i(0)|X_i, W_i]$ are continuous around $W_i=c$. Put if differently, 
\[
E[Y_i(d)|X_i, W_i=c^+]=E[Y_i(d)|X_i, W_i=c^-] \ \text{for each } d\in\{0,1\}
\]
\end{assumption}
\end{mdframed}
The above condition also implies that $(\epsilon_i(1), \epsilon_i(0))$ also has a continuous distribution around $W_i=c(X_i)$. If this condition is not satisfied, this implies that there could be more than the $W_i$ variable that jumps around the cutoff, which is what you do not want in an RDD regression.  \par
There is another version of the RDD which is more general than the sharp RDD. A \textbf{fuzzy RDD} setup exploits the jump in the probability of being treated at $W_i=c$. This jump is not necessarily from 0 to 1. Formally, we say we look for discontinuities of 
\[
\Pr(D_i|X_i,W_i)
\]
at $W_i=c(X_i)$. 
\section{Identification in RDD}
We start with a sharp RDD case. Assume that $c(X_i)$ is a constant $c$. We can write
\begin{align*}
E[Y_i|X_i, W_i]&=E[Y_i(0)|X_i, W_i]+E[(Y_i(1)-Y_i(0))1(W_i\geq c(X_i))|X_i, W_i]
\end{align*}
And we can separate the above into two. One with $W_i=c^+$ and the other with $W_i=c^-$.
\begin{align*}
E[Y_i|X_i, W_i=c^+]&=E[Y_i(0)|X_i, W_i=c^+]+E[(Y_i(1)-Y_i(0))|X_i, W_i=c^+]\\
E[Y_i|X_i, W_i=c^-]&=E[Y_i(0)|X_i, W_i=c^-]
\end{align*}
This is because $1(W_i\geq c(X_i))=1$ if $W_i=c^+$ and 0 otherwise. Therefore, 
\begin{align*}
E[Y_i|X_i, W_i=c^+] - E[Y_i|X_i, W_i=c^-]&=E[Y_i(0)|X_i, W_i=c^+]-E[Y_i(0)|X_i, W_i=c^-]\\ &+E[(Y_i(1)-Y_i(0))|X_i, W_i=c^+]
\\&=E[(Y_i(1)-Y_i(0))|X_i, W_i=c^+]
\end{align*}
where the first line on the right hand side vanishes due to the continuity assumptions. Reusing the continuity argument that $E[Y_i(d)|X_i, W_i=c^+]=E[Y_i(d)|X_i, W_i=c^-] \ \text{for each } d\in\{0,1\}$, we can write  
\[
E[(Y_i(1)-Y_i(0))|X_i, W_i=c^+]=E[(Y_i(1)-Y_i(0))|X_i, W_i=c]
\]
Therefore, we have backed out
\[
E[(Y_i(1)-Y_i(0))|X_i, W_i=c]=E[Y_i|X_i, W_i=c^+] - E[Y_i|X_i, W_i=c^-]
\]
which is the average treatment effect at $W_i=c$ and a given $X_i$. This is similar to the LATE in the sense if we let $Z_i=1(W_i\geq c)$, we can get back what is effectively equivalent to the LATE estimator (with the difference in the probability of treatment exactly at 1). Also, we are looking at the treatment effect on the compliers, albeit a very narrow group of people.  \par
For the fuzzy RDD, we can write as follows 
\begin{align*}
E[Y_i|X_i, W_i]&=E[Y_i(0)|X_i, W_i]+E[(Y_i(1)-Y_i(0))\Pr(D_i=1|X_i, W_i)|X_i, W_i]
\end{align*}
So we can do the similar approach as in sharp RDD and get 
\footnotesize{\begin{align*}
E[Y_i|X_i=x, W_i=c^+]&=E[Y_i(0)|X_i=x, W_i=c^+]+\Pr(D_i=1|X_i=x, W_i=c^+)E[Y_i(1)-Y_i(0)|X_i=x, W_i=c^+]\\
E[Y_i|X_i=x, W_i=c^-]&=E[Y_i(0)|X_i=x, W_i=c^-]+\Pr(D_i=1|X_i=x, W_i=c^-)E[Y_i(1)-Y_i(0)|X_i=x, W_i=c^-]
\end{align*}}\normalsize
So using the continuity assumption, the difference in the left hand side can be written as
\footnotesize{\begin{align*}
E[Y_i|X_i=x, W_i=c^+]-E[Y_i|X_i=x, W_i=c^-]&=[\Pr(D_i=1|X_i=x, W_i=c^+)-\Pr(D_i=1|X_i=x, W_i=c^-)]\\
&\times E[Y_i(1)-Y_i(0)|X_i, W_i=c]
\end{align*}}\normalsize
Thus, we also identify an average treatment effect at $W_i=c$ and $X_i=x$, which is now divided by something that is not necessarily 1. 
\[
E[Y_i(1)-Y_i(0)|X_i=x, W_i=c]=\frac{E[Y_i|X_i=x, W_i=c^+]-E[Y_i|X_i=x, W_i=c^-]}{\Pr(D_i=1|X_i=x, W_i=c^+)-\Pr(D_i=1|X_i=x, W_i=c^-)}
\]
\section{Implementation of RDD}
So what can we do in terms of implementing the RDD in practice? One way to implement RDD in practice is to make use of nonparametric regression. The bandwidth, however, should be bounded at $c$. If you are doing a nonparametric regression from the left of the bandwidth, you should apply this on $[c-\epsilon, c)$. For the right hand side, $[c, c+\epsilon]$. If the domain reaches beyond $c$ from either side, there is a huge risk of misfitting the data. While you can do local constant regression in theory, it comes at a huge cost. With this, you can only pin down the level of $Y_i$ from either side. You lose out the rate in which outcome changes within both sides of the boundary. Therefore, if carrying out nonparametric regression, you should do at least a local linear regression. \par
Since we are doing a nonparametric regression, we also run into similar problems as in the typical nonparametric regressions. If you have many other variables to consider, you risk running into the curse of dimensionality. There is also an issue of selecting an optimal bandwidth. We still minimize the AMISE, but at $W_i=c$.  $MSE(h)$ is defined as 
\[
E[(\hat{\mu}(x,c,0)-\mu(x,c,0))^2+(\hat{\mu}(x,c,1)-\mu(x,c,1))^2]
\]
Calonico, Cattaneo, and Tituinik (2014) suggests the use of local quadratic estimator with the optimal bandwidth selected by minimizing above $MSE(h)$ which is typically known as IK bandwidth (Imbens, Kalyanaraman, 2012). Also note that the same trade-off between variance and bias is still in play. \par
There is also a parametric way to implement this as well. One way is to run a separate regression for both left and right of the cutoff. That is, run
\begin{gather*}
Y_i = X_i\beta^- +(c-w_i)X_i\gamma^-+\epsilon_i^-\ \ \text{for }w_i<c\\
Y_i = X_i\beta^+ +(w_i-c)X_i\gamma^++\epsilon_i^+\ \ \text{for }w_i\geq c
\end{gather*}
This setup allows us to fit a different trend on the left and the right of the cutoff $c$. There is also a way in which we make use of both sides of the cutoff. Specifically, we run
\[
Y_i = X_i\beta+ X_i \cdot 1(W_i\geq c)\gamma+X_i\cdot(c-W_i)\delta+X_i\cdot (c-W_i)\cdot 1(W_i\geq c) \mu +\epsilon_i
\]
Here, what happens is that
\begin{itemize}
\item $W_i\geq c$: $Y_i=X_i(\beta+\gamma)+X_i\cdot(c-W_i)(\delta+\mu)$
\item $W_i< c$: $Y_i=X_i(\beta)+X_i\cdot(c-W_i)(\delta)$
\end{itemize}
To make this simple, suppose that $X_i$ is just a vector of 1's. Then, on the $(Y_i, W_i)$ plane,  the jump of the outcome at $W_i=c$ would be captured by the $\gamma$ parameter. Using $\delta, \mu$, we can allow the slopes to differ on either side of the equation. \par
There are some graphical tests that should be carried out. First, we plot $Y_i$ as a function of $W_i$ for a given $X_i$ and see if there is a mean shift. Also, we plot the propensity score $\Pr(D_i=1|X_i, W_i)$ and look for a shift. One thing that should not shift is the distribution of $X_i$'s. So plot $X_i$'s as function of $W_i$ and confirm that there is no shift. Another thing to worry regarding $W_i$ is the possibility of manipulating or gaming. Maybe it might be possible to get into the treatment when you should not be treated (e.g. Ost et al. (2018) checks for this). Therefore, use a McCrary test to confirm that the distribution of $W_i$ do not change at $W_i=c$. \par
Last but not least, what if we do not know the exact cutoff? That is actually not much to worry about, since we can figure out what they might be based on the distribution of $W_i$. Chay, McEwan, and Urquiola (2005) conducts and RDD in the context where the exact cutoffs are unknown using visual evidence. Also, since we lose a lot of data in selecting the optimal bandwidth around the cutoff, RDD is usually poor in terms of external validity.


