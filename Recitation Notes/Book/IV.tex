

%%%%%%%%%%%%%%%%%%
\chapter{Instrumental Variables}
\section{Classical Linear Models: Ordinary Least Squares}
We will assume a data generating process that looks like
\[
y_i = x_i'\beta+e_i, \ x_i = \begin{pmatrix} x_{i1} \\ ... \\ x_{ik}\end{pmatrix}, \ i=1,...,n
\]
where $x_i\text{ and }\beta $ are both in $\mathbb{R}^k$ and $y_i$ and $e_i$ are scalars. In a matrix notation, this can be written as
\[
y=X\beta+e, \ y = \begin{pmatrix} y_{1} \\ ... \\ y_{n}\end{pmatrix}\in\mathbb{R}^n, X = \begin{pmatrix} x_{1}' \\ ... \\ x_{n}'\end{pmatrix} \in\mathbb{R}^{n\times k}, e = \begin{pmatrix} e_{1} \\ ... \\ e_{n}\end{pmatrix}\in\mathbb{R}^n
\]\par
To demonstrate the consistency and the limiting distribution of the OLS estimators, I will use some of these assumptions
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}[Assumptions for Classical Linear Models]
\item The following assumptions are used in showing consistency and the limiting distribution of OLS estimators
\begin{description}
\item[A1] $(y_i, x_i)$ are IID across $i$'s
\item[A2] $E(x_ie_i)=0$
\begin{description}
\item[A2'] $E(e_i|x_i)=0$ (Problem set 1 includes a question that asks you to derive A2 from A2')
\end{description}
\item[A3] $E(x_ix_i')=Q$ is a positive definite matrix (hereafter PD matrix)
\item[A4] $E||x_i^4||<\infty, E||y_i^4||<\infty$
\end{description}
\end{assumption}
\end{mdframed} \par
The OLS estimator can be found by minimizing the sum of squared errors. In other words
\[
\hat{\beta}=\min_b\sum_{i=1}^n (y_i-x_i'b)^2 = \left(\sum_{i=1}^nx_ix_i'\right)^{-1}\left(\sum_{i=1}^nx_iy_i\right)
\]
or in matrix notation, $(X'X)^{-1}(X'y)$. The consistency and the limiting distribution of OLS estimators can be demonstrated as follows
\begin{mdframed}[backgroundcolor=green!5] 
\begin{theorem}[Consistency of $\hat{\beta}$]
Under assumptions \textbf{A1-A3}, $\hat{\beta}\xrightarrow{p}\beta$
\begin{proof}
Rewrite $\hat{\beta}$ as $\beta+\left(\sum_{i=1}^nx_ix_i'\right)^{-1}\left(\sum_{i=1}^nx_ie_i\right)$. To carry out the asymptotic analysis on the summation terms, multiply $\frac{1}{n}$ to both. By \textbf{A1}, we can deduce that $x_i$ and $e_i$ are IID. Then, we can apply weak law of large numbers and continuous mapping theorem to show that
\[
\begin{aligned}
\frac{1}{n}\sum_{i=1}^nx_ix_i'&\xrightarrow{p}E\left(x_ix_i'\right) \\
\left(\frac{1}{n}\sum_{i=1}^nx_ix_i'\right)^{-1}&\xrightarrow{p}E\left(x_ix_i'\right)^{-1} (\because CMT)\\
\frac{1}{n}\sum_{i=1}^nx_ie_i'&\xrightarrow{p}E\left(x_ie_i'\right) \\
\end{aligned}
\]
By assumptions \textbf{A2,A3},$\left(\frac{1}{n}\sum_{i=1}^nx_ix_i'\right)^{-1}\xrightarrow{p}Q^{-1}$ and $\frac{1}{n}\sum_{i=1}^nx_ie_i'\xrightarrow{p}0$. By Slutsky's theorem, $\left(\sum_{i=1}^nx_ix_i'\right)^{-1}\left(\sum_{i=1}^nx_ie_i\right)\xrightarrow{p}0$. Thus, $\hat{\beta}\xrightarrow{p}\beta$.
\end{proof}
\end{theorem}
\begin{theorem}[Limiting distribution of $\hat{\beta}$]
Under assumptions \textbf{A1-A4}, the limiting distribution of $\hat{\beta}$ is characterized by $\sqrt{n}(\hat{\beta}-\beta)\xrightarrow{d}N(0,Q^{-1}\Omega Q^{-1})$ , where $\Omega = E(x_ix_i'e_i^2)$
\begin{proof}
We can write $\sqrt{n}(\hat{\beta}-\beta)=\left(\frac{1}{n}\sum_{i=1}^nx_ix_i'\right)^{-1}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^nx_ie_i\right)$. We know from the previous theorem that $\left(\frac{1}{n}\sum_{i=1}^nx_ix_i'\right)^{-1}\xrightarrow{p}Q^{-1}$. So we need to work on $\left(\frac{1}{\sqrt{n}}\sum_{i=1}^nx_ie_i\right)$. From the central limit theorem, we can obtain the limiting distribution of  $\left(\frac{1}{\sqrt{n}}\sum_{i=1}^nx_ie_i\right)$ 
\[
\frac{1}{\sqrt{n}}\sum_{i=1}^nx_ie_i\xrightarrow{d} N(0,\Omega)
\]
since $E(x_ie_i)=0$ by \textbf{A2} and $var(x_ie_i)= E(x_ie_ie_ix_i')-(E(x_ie_i))^2 = E(x_ix_i'e_i^2)=\Omega$ (In using CLT, we need assumption \textbf{A4} so that the variance-covariance matrix obtained from here is finite.) Then, by Slutsky's theorem, 
\[
\sqrt{n}(\hat{\beta}-\beta)=\left(\frac{1}{n}\sum_{i=1}^nx_ix_i'\right)^{-1}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^nx_ie_i\right)\xrightarrow{d}Q^{-1}N(0,\Omega)=N(0,\underbrace{Q^{-1}\Omega Q^{-1}}_{V})
\]
\end{proof}
\end{theorem}
\end{mdframed} \par
If we are interested in a particular element of $\beta$, namely $\beta_j$, we will need to work on the following limiting distribution
\[
\sqrt{n}(\hat{\beta}_j-\beta_j) \xrightarrow{d} N(0,V_{jj}) \ (V_{jj} \text{ is the $(j,j)$th element of V})
\]
\section{Hypothesis Tests on OLS}
\begin{itemize}
\item \textbf{Single element}: Consider the following setting
\[
H_0: \beta_j = \beta_j^0, \ \ H_1:\beta_j \neq \beta_j^0
\]
From the limiting distribution of $\beta_j$, we can show that the test statistic is distributed as a standard normal under $H_0$.  It is characterized as
\[
\frac{\hat{\beta}_j-\beta_j^0}{\sqrt{\widehat{V}_{jj}/n}}\xrightarrow{d}N(0,1)
\]
where $\widehat{V}$ is estimated version of the variance (more on that later). 
\item \textbf{Multiple element}: Sometimes, we may be interested in the features of the linear combinations of the elements of $\beta$, or even multiple restrictions, Let $R\in\mathbb{R}^{k\times r}$ characterize such restrictions. Then we can write
\[
H_0 : R'\beta = c, \ \ H_1: \lnot H_0
\]
Then from the limiting distribution of $\hat{\beta}$, we can apply Slutsky's theorem to get the necessary limiting distribution
\[
\sqrt{n}(R'\hat{\beta}-R'\beta)=\sqrt{n}R'(\hat{\beta}-\beta)\xrightarrow{d}N(0,R'VR)
\]
Since $ R'\beta = c$ under $H_0$,  we can obtain the following Wald test statistic. (For convenience, we also assume that $V$ is known)
\[
n(R'\hat{\beta}-c)'(R'VR)^{-1}(R'\hat{\beta}-c)\xrightarrow{d}\chi^2_r
\]
To see why we ended up with a $\chi_r^2$ distribution, we need to understand the following. 
\begin{mdframed}[backgroundcolor=green!5] 
\begin{theorem}[Chi-squared distribution]
If $\eta\sim N(0,A)$, where $A$ is PD, then 
\[
\eta'A^{-1}\eta\sim \chi^2_{\text{rank}(A)}
\]
\end{theorem}
\end{mdframed} \par
\item \textbf{Notes on $\widehat{V}$}: The definition of $\widehat{V}$ is $\widehat{V}\equiv\widehat{Q}^{-1}\widehat{\Omega}\widehat{Q}^{-1}$, where
\begin{itemize}
\item $\widehat{Q}$ is a sample analogue of $Q$, written as $\frac{1}{n}\sum_{i=1}^nx_ix_i'$
\item $\widehat{\Omega}$ is a sample analogue of $E(x_ix_i'e_i^2)$, also with the consideration that the true value of $e_i$ is replaced with the residual $\hat{e}_i$. Therefore, we can write $\frac{1}{n}\sum_{i=1}^nx_ix_i'\hat{e}_i^2$
\end{itemize}
\end{itemize}

\section{Source of Endogeneity}
We still work with the data generating process $y_i = x_i'\beta+e_i$, but now with the possibility that $E(x_ie_i)\neq0$. In other words, the error term and the regressor can now be correlated (or the regressors are endogenous). Since we required \textbf{A2} assumption in showing that OLS estimators are consistent, the fact that $E(x_ie_i)$ is not necessarily zero implies that OLS may no longer be consistent. In this section, we study cases in which regressors can become endogenous and how instrumental variables allow us to address this problem. 
\begin{itemize}
\item \textbf{Measurement Error in Regressors: }  Suppose that the linear model we want to estimate is as follows
\[
y_i = {x_i}^{*'}\beta+e_i \ (\text{We assume }E({x_i}^{*}e_i)=0)
\]
However, we cannot observe $x_i^*$. Instead, we can observe $x_i=x_i^*+v_i$, where $v_i$ has mean zero and independent of both $x_i^*$ and $e_i$. So we have a classical measurement error in which $x_i$ is unbiased, but noisy measure of $x_i^*$. If we use $x_i$ instead, 
\[
\begin{aligned}
y_i &= (x_i-v_i)'\beta+e_i &=x_i'\beta \underbrace{-v_i'\beta+e_i}_{=u_i} \\
\end{aligned}
\]
Then $E(x_iu_i)$ is as follows
\[
E(x_iu_i)=E[x_i(-v_i'\beta+e_i)]=E[(x_i^{*}+v_i)(-v_i'\beta+e_i)]=-E(v_iv_i')\beta
\]
So unless $\beta=0$, or $E(v_iv_i')=0$, $E(x_iu_i)\neq0$.  When we use OLS on this context, the probability limit of the OLS estimator would be
\[
\begin{aligned}
\hat{\beta}_{OLS} &=\beta+E(x_ix_i')^{-1}E(x_iu_i)\\
&=\beta-E(x_ix_i')^{-1}E(v_iv_i)\beta\\
&=\beta-E[(x_i^*+v_i)(x_i^*+v_i)']^{-1}E(v_iv_i)\beta\\
&=\beta-[E(x_i^*x_i^{*'})+E(v_iv_i')]^{-1}E(v_iv_i)\beta\\
&=\frac{E(x_i^*x_i^{*'})}{E(x_i^*x_i^{*'})+E(v_iv_i')}\beta \ (\leq\beta)\\
\end{aligned}
\]
The only time that $\frac{E(x_i^*x_i^{*'})}{E(x_i^*x_i^{*'})+E(v_iv_i')}\beta$ would equal $\beta$ is when $\beta$ itself is zero or when $E(v_iv_i')=0$. The latter, however, implies that $var(v_i)=0$ and the noise $v_i$ has mean 0 and has a point mass at 0 - so no measurement error exists. In usual cases, the OLS estimator has a probability limit of something less than $\beta$.  This is what is also known as \textbf{attenuation bias}. 
\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{comment}[Comment on Measurement Errors]
So how do we address the endogeneity problem?
\begin{itemize}
\item If there exists another noisy, but unbiased measure of $x_i^*$, namely $w_i=x_i^*+\delta_i$, we can use $w_i$ to instrument for $x_i$. The condition is that $\eta_i$ has mean zero and uncorrelated with $(x_i^*, e_i. v_i)$. Try verifying that this satisfies all IV conditions. 
\item If there is a measurement error in $y_i$, the only this it does is to change the component of $e_i$. Assuming all the old assumptions hold, this does not pose as much problem as having a measurement error in the regressor. 
\end{itemize}
\end{comment}
\end{mdframed}
\item \textbf{Simultaneity Bias: } A classic example of this would be a supply and demand system type of setting:
\begin{gather*}
q_i = \beta_1p_i+u_i \tag{Supply}\\
q_i = -\beta_2p_i+v_i \tag{Demand}
\end{gather*}
I will assume $e_i = (u_i \ v_i)'$ is IID, $E(e_i)=0, E(e_ie_i')=I_{2}$
When you do some algebra, the equilibrium of this system is 
\[
p_i = \frac{v_i-u_i}{\beta_1+\beta_2}, q_i = \frac{\beta_1v_i + \beta_2u_i}{\beta_1+\beta_2}
\]
So for both supply and demand equations, we have $E(p_iu_i)\neq0$ and $E(p_iv_i)\neq0$. When naively applying OLS to this equation, the result is as follows. 
\[
q_i=\beta^*p_i+\eta_i, \ E(p_i\eta_i)=0 \implies \hat{\beta}^*=\frac{E(p_iq_i)}{E(p_i^2)}=\frac{\beta_1-\beta_2}{2}
\]
Thus, OLS estimators does not converge to either one of $\beta_1$ or $\beta_2$, resulting in a \textbf{simultaneity bias}.  
\item \textbf{Omitted Variable Bias (OVB): } Suppose that we are interested in the determinant of wages ($y_i$). Also assume that education, $x_i$, and innate ability,  $a_i$, determine wages in the following manner
\[
y_ i = x_i\beta_1+a_i\beta_2+e_i, \ \ E(x_ie_i)=0, E(a_ie_i)=0
\]
However, instead of observing $(y_i,x_i,a_i)$, we can only observe $(y_i, x_i)$. the best we can do at the moment is to estimate the following equation
\[
y_i=x_i\beta_1+u_i,\text{ where } u_i=a_i\beta_2 + e_i
\] 
Then $E(x_iu_i)$ becomes
\[
E(x_iu_i)= E(x_i(a_i\beta_2+e_i))=E(x_ia_i)\beta_2+0 = E(x_ia_i)\beta_2
\]
Therefore, when 1)$x_i$ and $a_i$ are correlated and 2)$\beta_2\neq0$, $x_i$ is endogenous with respect to $u_i$. On the flip side, if either one of the condition is not met, $E(x_iu_i)=0$ again. Moreover, the OLS estimator acquired here has a probability limit of
\[
\hat{\beta}_{OLS}=\beta_1+E(x_i^2)^{-1}E(x_iu_i)=\beta_1+E(x_i^2)^{-1}E(x_ia_i)\beta_2
\]
So if 1) and 2) occurs, the above does not converge in probability to $\beta_1$. Also note that we can determine the direction of the bias by the sign of $E(x_ia_i)$ and $\beta_2$. 
\end{itemize}

\section{Instrumental Variable Estimators}
Assume that the data generating process is as follows
\[
y_i = x_{1i}'\beta_1+x_{2i}'\beta_2+e_i
\]
where $E(x_{1i}e_i)=0, E(x_{2i}e_i)\neq0$, and $\dim(x_{1i})=k_1, \dim(x_{2i})=k_2,\ k_1+k_2=k$. In our case, $x_{2i}$ is the collection of endogenous variables. It can be shown that consistency of the OLS estimators of $\beta_2$ and $\beta_1$ will not be guaranteed under this situation.
\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{example}[When $k_1=k_2=1$]
In this case, we can write $\hat{\beta}_1$ as
\[
\hat{\beta}_1=\frac{\sum_{i=1}^n x_{2i}^2\sum_{i=1}^n x_{1i}y_i-\sum_{i=1}^n x_{1i}x_{2i}\sum_{i=1}^nx_{2i}y_i}{\sum_{i=1}^n x_{1i}^2\sum_{i=1}^nx_{2i}^2-(\sum_{i=1}^nx_{1i}x_{2i})^2}
\]
When we replace $y_i$ with $x_{1i}\beta_1+x_{2i}\beta_2+e_i$, we end up with
\[
\hat{\beta}_1=\beta_1+\frac{\sum_{i=1}^n x_{2i}^2\sum_{i=1}^n x_{1i}e_i-\sum_{i=1}^n x_{1i}x_{2i}\sum_{i=1}^nx_{2i}e_i}{\sum_{i=1}^n x_{1i}^2\sum_{i=1}^nx_{2i}^2-(\sum_{i=1}^nx_{1i}x_{2i})^2}
\]
Since $E(x_{2i}e_i)\neq0$, then $\frac{1}{n}\sum_{i=1}^nx_{2i}e_i$ converges to something that is not zero (whereas $\frac{1}{n}\sum_{i=1}^nx_{1i}e_i$ does converge in probability to 0). So the whole fraction term does not converge in probability to 0 and even $\hat{\beta}_1$ is not consistent. 
\end{example}
\end{mdframed}
\par
Let $z_i\in\mathbb{R}^l = \begin{pmatrix}z_{1i} \\ z_{2i}\end{pmatrix}=\begin{pmatrix}x_{1i} \\ z_{2i}\end{pmatrix}$, where $\dim(z_{2i})=l-k_1$ and $l-k_1\geq k_2$. For $z_i$ to be a valid IV, the following conditions must be satisfied
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{definition}[IV conditions]
$z_i$ is a valid IV if
\begin{enumerate}
\item \textbf{Exogeneity}: $E(z_ie_i)=0$
\begin{enumerate}
\item \textbf{Exclusion}: $E(z_iy_i)=\beta_1E(z_ix_{1i})+\beta_2E(z_ix_{2i})$, in other words, $z_i$ should impact $y_i$ through $x_{1i}$ and $x_{2i}$
\end{enumerate}
\item \textbf{Relevancy}: $rank[E(z_ix_i')]=\dim(x_i)=k$
\item \textbf{PD}: $E(z_iz_i')>0$
\end{enumerate}
\end{definition}
\end{mdframed}
We will derive IV estimators in two ways: Reduced form approach and 2SLS approach
\begin{itemize}
\item \textbf{Reduced form}: In this approach, we assume that $z_i$ is a least squares projection. So we can write
\begin{gather*}
x_i = \Gamma'z_i+u_i \ (\Gamma\in\mathbb{R}^{l\times k})\\
\implies z_ix_i'=z_iz_i'\Gamma+z_iu_i'\\
\implies E(z_ix_i')=E(z_iz_i')\Gamma+E(z_iu_i')
\end{gather*}
Since $z_i$ is a least squares projection, $E(z_iu_i)=0$. So
\[
\Gamma= E(z_iz_i')^{-1}E(z_ix_i')
\]
and the estimator for $\Gamma$ would be its sample analogue, $\widehat{\Gamma}=\left(\frac{1}{n}\sum_{i=1}^nz_iz_i'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^nz_ix_i'\right)=(Z'Z)^{-1}(Z'X)$.\par
Then we get to the structural equation 
\[
y_i=x_i'\beta+e_i \iff y_i =(z_i'\Gamma+u_i')\beta+e_i \iff y_i = z_i'\underbrace{\Gamma\beta}_{=\lambda}+\underbrace{u_i'\beta+e_i}_{=v_i}
\]
From the assumptions, we can show that $E(z_iv_i)=0$. As such, 
\[
\lambda=E(z_iz_i')^{-1}E(z_iy_i)
\]
with its sample analogue being $\hat{\lambda}=\left(\frac{1}{n}\sum_{i=1}^n z_iz_i'\right)^{-1} \left(\frac{1}{n}\sum_{i=1}^n z_iy_i\right)=(Z'Z)^{-1}Z'y$\par
In case where $k=l$, $Z$ itself becomes invertible. Then we can show that $\beta=\Gamma^{-1}\lambda$ and thus 
\[
\hat{\beta}_{IV}=(Z'X)^{-1}Z'y
\]
If otherwise, We note that $\widehat{\Gamma}\beta+\text{error}=\hat{\lambda}$ and show that 
\[
\hat{\beta}_{IV}=(\widehat{\Gamma}'\widehat{\Gamma})^{-1}\widehat{\Gamma}'\hat{\lambda}
\]
\item \textbf{2SLS}:  Suppose the structural equation and the first-stage regression is as follows.
\begin{gather*}
y=X\beta+e \tag{Structural}\\
X=Z\Gamma+u \tag{First Stage}
\end{gather*}
where $Z\in\mathbb{R}^{n\times l},\Gamma\in\mathbb{R}^{l\times k}$. $Z$ is our IV and we still maintain the least square projection assumption. We proceed as follows
\begin{enumerate}
\item Regress the first stage and obtain $\widehat{\Gamma}=(Z'Z)^{-1}Z'X$. Then the predicted value of $X$, denoted as $\widehat{X}=Z(Z'Z)^{-1}Z'X=P_ZX$.
\begin{mdframed}[backgroundcolor=green!5] 
\begin{property}[Properties of Projection Matrix $P_Z$] Note the following 
\begin{itemize} 
\item Symmetric: $P_Z'=(Z(Z'Z)^{-1}Z')'=Z(Z'Z)^{-1}Z'=P_Z$
\item Idempotent: $P_Z^2=Z(Z'Z)^{-1}Z'Z(Z'Z)^{-1}Z'=Z(Z'Z)^{-1}Z'=P_Z$
\end{itemize}
\end{property}
\end{mdframed}
\item In the structural equation, replace $X$ with $\widehat{X}$ and obtain
\[
\begin{aligned}
\hat{\beta}_{2SLS}&=(\widehat{X}'\widehat{X})^{-1}\widehat{X}'y=(X'P_Z'P_ZX)^{-1}(X'P_Z'y)\\
&=(X'P_ZX)^{-1}X'P_Zy =(\widehat{X}'X)^{-1}\widehat{X}'y\\
\end{aligned}
\]
which is effectively replacing $Z$ in the previous approach with $\widehat{X}$. 
\end{enumerate}
\end{itemize}
\section{Consistency and Limiting Distribution of 2SLS}
To show the asymptotic properties of the 2SLS estimators, we need the following set of assumptions
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{assumption}[2SLS Assumptions] This is a list of required assumptions. 
\begin{description}
\item[T1] $(y_i, x_i, z_i)$ are IID
\item[T2] Finite second moments: $E||y_i^2||<\infty, E||x_i^2||<\infty, E||z_i^2||<\infty$
\item[T3] $E(z_iz_i')>0$
\item[T4] $rank[E(z_ix_i')]=k$
\item[T5] $E(z_ie_i)=0$
\item[T6] Finite fourth moments: $E||y_i^4||<\infty, E||x_i^4||<\infty, E||z_i^4||<\infty$
\item[T7] $E(z_iz_i'e_i^2)=\Omega>0$
\end{description}
\end{assumption}
\end{mdframed}\par
Then, we can show the consistency and asymptotic normality of 2SLS estimators. 
\begin{mdframed}[backgroundcolor=green!5] 
\begin{theorem}[Consistency of 2SLS] Under assumptions \textbf{T1-T5}, $\hat{\beta}_{2SLS}\xrightarrow{p}\beta$
\begin{proof}
We can rewrite 2SLS estimators as $\hat{\beta}_{2SLS}=\beta+(X'P_ZX)^{-1}X'P_Ze$, which becomes
\small{\[
\beta+\left[\frac{X'Z}{n}\left(\frac{Z'Z}{n}\right)^{-1} \frac{Z'X}{n}\right]^{-1}\left[\frac{X'Z}{n}\left(\frac{Z'Z}{n}\right)^{-1} \frac{Z'e}{n}\right]
\]}\normalsize
Define $Q_{ZX}=E(z_ix_i'), Q_{XX}=E(x_ix_i')$. Then by weak law of large numbers,
\small{\[
\frac{Z'X}{n}\xrightarrow{p}Q_{ZX}, \frac{X'X}{n}\xrightarrow{p}Q_{XX}, \frac{Z'e}{n}\xrightarrow{p}0
\]}\normalsize
By applying Slutsky's theorem and continuous mapping theorem, all the terms right of $\beta$ converge in probability to 0. Thus, $\hat{\beta}_{2SLS}\xrightarrow{p}\beta$
\end{proof}
\end{theorem}
\begin{theorem}[Limiting Distribution of 2SLS] Under assumptions \textbf{T1-T7}, the limiting distribution of the 2SLS estimator is characterized by $\sqrt{n}(\hat{\beta}_{2SLS}-\beta)\xrightarrow{d}N(0,V_\beta)$
\begin{proof}
Note that
\small{\[
\sqrt{n}(\hat{\beta}_{2SLS}-\beta)=\left[\frac{X'Z}{n}\left(\frac{Z'Z}{n}\right)^{-1} \frac{Z'X}{n}\right]^{-1}\left[\frac{X'Z}{n}\left(\frac{Z'Z}{n}\right)^{-1} \frac{Z'e}{\sqrt{n}}\right]
\]}\normalsize
By CLT, $\frac{Z'e}{\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{i=1}^nz_ie_i\xrightarrow{d}N(0,\Omega)$. Then apply Slutsky theorem and continuous mapping theorem to obtain
\small{\[
\sqrt{n}(\hat{\beta}_{2SLS}-\beta)\xrightarrow{d}N(0,\underbrace{A_n^{-1}Q_{ZX}'Q_{ZZ}^{-1}\Omega Q_{ZZ}^{-1}Q_{ZX}A_n^{-1}}_{=V_\beta})
\]}\normalsize
where $A_n=\left[\frac{X'Z}{n}\left(\frac{Z'Z}{n}\right)^{-1} \frac{Z'X}{n}\right]$
\end{proof}
\end{theorem}
\end{mdframed}

\section{Some words of Caution}
\begin{itemize}
\item \textbf{Correct Standard Errors} When estimating $\Omega=E(z_iz_i'e_i^2)$, we are not aware of the true error. So we need to estimate this as well. Note that we need to use a proper residual. Namely, we must use
\[
\hat{e}_i = y_i - x_i'\hat{\beta}_{2SLS}
\]
This is correct, as $\hat{\beta}_{2SLS}\xrightarrow{p}\beta$. However, we frequently make a mistake of using
\[
\tilde{e}_i = y_i - \hat{x}_i'\hat{\beta}_{2SLS}
\]
Since $\tilde{x}_i$ is not exactly $x_i$, this converges in probability to something else. 
\begin{mdframed}[backgroundcolor=yellow!5] 
\begin{comment}[on STATA]
To see this, compare the standard errors from doing \textnormal{\texttt{ivregress 2sls y (x=z)}} and 'hard-coding' 2SLS by using \textnormal{\texttt{reg x z}}, then \textnormal{\texttt{predict xhat}}, and then coding \textnormal{\texttt{reg y xhat}}. 
\end{comment}
\end{mdframed}
\item \textbf{Nonlinear Extension}: If we are instead interested in the properties of $g(\hat{\beta}_{2SLS})$, where $g(\cdot)$ is not necessarily nonlinear, we can apply delta method here. 
\item \textbf{Too Many IV?}: In practice, when we have too many IV's (large dimension for $z_i$), it may be possible for the instruments to 'perfectly' predict $x_i$. In other words, $\hat{x}_i$ becomes nearly identical to $x_i$. This can become a problem because the endogeneity bias that plagued original structural equation may not be mitigated. \\
One way of getting around this is to use a regression method that involves penalty mechanism - like LASSO, for instance. We will also formally treat this issue when we learn over-identification tests in the near future. 
\end{itemize}

\section{Limited Information Maximum Likelihood: Setup and Motivation}
Assume a data generating process
\[
y_ i = \beta_1' x_{1i}+ \beta_2' x_{2i}+e_i,\  (x_{1i}\in\mathbb{R}^{k_1}, x_{1i}\in\mathbb{R}^{k_2})
\]
where $E(x_{1i}e_i)=0$, but $E(x_{2i}e_i)\neq0$. A \textbf{limited information maximum likelihood (LIML) estimator} derives the maximum likelihood estimator for the joint distribution of $(y_i, x_{2i})$ using structural equation of $y_i$ and the reduced form equation for $x_{2i}$. This differs from the full information maximum likelihood (FIML) in the sense that the FIML requires structural equation for $x_{2i}$ as well. \par
Briefly speaking, the reason we prefer LIML is as follows. When the number of the instruments are fixed, 2SLS and LIML have the same asymptotic distribution, with the former not requiring the assumption of normality (Greene, 2012). However, when there is a problem of weak instrument variable or too many instrumental variables, it can be shown that 2SLS becomes biased towards OLS. Others have shown that LIML estimators perform better in the presence of weak IVs and/or too many IVs.\footnote{I refer further explanation to Jorn-Steffen Pischke's notes, found at \url{http://econ.lse.ac.uk/staff/spischke/ec533/Weak\%20IV.pdf}}. \par
\section{Limited Information Maximum Likelihood: Derivation}
The structural equation for $y_i$ is introduced in the previous page. Assume that there is a $z_i = \begin{pmatrix} x_{1i}\\ z_{2i}\end{pmatrix}\in\mathbb{R}^l$ where $E(z_ie_i)=0$. I now define a reduced form equation for $x_{2i}$, written as
\[
x_{2i}=\Gamma_{12}'x_{1i}+\Gamma_{22}'z_{2i}+u_{2i},\  (\Gamma_{12}'\in\mathbb{R}^{k_2\times k_1}, \Gamma_{22}'\in\mathbb{R}^{k_2\times (l-k_1)},z_{2i}\in\mathbb{R}^{l-k_1})
\]
By putting the structural and reduced form equation together in a matrix form, we get
\[
\underbrace{\begin{pmatrix}1 &- \beta_2' \\ 0  & I_{k_2} \end{pmatrix}}_{=A}\underbrace{\begin{pmatrix} y_i \\ x_{2i} \end{pmatrix}}_{=w_i}
= \underbrace{\begin{pmatrix}\beta_1' &0 \\ \Gamma_{12}'  & \Gamma_{22}' \end{pmatrix}}_{=B}\underbrace{\begin{pmatrix} x_{1i} \\ z_{2i} \end{pmatrix}}_{z_i} + \underbrace{\begin{pmatrix} e_i \\ u_{2i} \end{pmatrix}}_{=\eta_i}
\]
This now puts all endogenous variables on the left hand side and the exogenous variables on the right. To use the maximum likelihood approach, we need some assumptions on $\eta_i$. Specifically, we assume that $\eta_i$ is conditionally IID normal and
\[
\eta_i |z_i\sim N(0,\Sigma_\eta)
\]
If we apply these facts, then we can derive the log-likelihood function for $\eta_i$
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{lemma}[Multivariate Normal Density]
Let $X\in\mathbb{R}^k$ be a multivariate normal distribution s.t. $X\sim N(\mu, \Sigma)$. Then the density function can be written as
\[
f_X(x)=(2\pi)^{-\frac{k}{2}}\det(\Sigma)^{-\frac{1}{2}}\times e^{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)}
\]
\end{lemma}
\begin{lemma}[Jacobian Transformation]
If $X\in\mathbb{R}^k$ is a continuous random vector and $g:\mathbb{R}^k \to\mathbb{R}^k$ is one-to-one and onto function, the density of $Y=g(X)$ is given by
\[
f_Y(y) = f_X(g^{-1}(y))\left|J\right|
\]
where $J = \det\begin{pmatrix}\frac{\partial x_1}{\partial y_1} & ... & \frac{\partial x_1}{\partial y_k} \
\\ ... &... &...\\ \frac{\partial x_k}{\partial y_1} & ... &\frac{\partial x_k}{\partial y_k}\end{pmatrix}$. Alternatively, we can write
\[
f_X(x) = f_Y(g(x))|H|, H=\det\begin{pmatrix}\frac{\partial y_1}{\partial x_1} & ... & \frac{\partial y_1}{\partial x_k} \
\\ ... &... &...\\ \frac{\partial y_k}{\partial x_1} & ... &\frac{\partial y_k}{\partial x_k}\end{pmatrix}
\]
\end{lemma}
\end{mdframed}
So to work through the likelihood function, we take these steps
\begin{enumerate}
\item \textbf{Re-express the PDF}: The PDF pf $\eta_i$ is
\[
(2\pi)^{-\frac{k_2+1}{2}}\det(\Sigma)^{-\frac{1}{2}}\times e^{-\frac{1}{2}(\eta_i)'\Sigma_\eta^{-1}(\eta_i)}
\]
Note that $\eta_i = Aw_i - Bz_i$, Then we can write the pdf of $w_i$ in terms of $A$ and $B$ as \footnote{Note that $\chi_i=w_i-A^{-1}Bz_i$ is also another normal distribution. You can notice that $\eta_j = A\chi_i$. Try matching this with the second version of the Jacobian transformation.  }
\[
(2\pi)^{-\frac{k_2+1}{2}}\det(\Sigma)^{-\frac{1}{2}}\times e^{-\frac{1}{2}(Aw_i-Bz_i)'\Sigma_\eta^{-1}(Aw_i-Bz_i)}|A|
\]
by Applying a Jacobian Transformation. Note that the determinant of the $A$ matrix is 1. So this term would be dropped. By IID assumption, the log likelihood for $i=1,...,n$ becomes
\[
C - \frac{n}{2}\log(\det(\Sigma_\eta)) - \frac{1}{2}\sum_{i=1}^n(Aw_i-Bz_i)'\Sigma_\eta^{-1}(Aw_i-Bz_i)
\]
\item \textbf{Concentrating out}: To find $A,B$ that maximizes the log - likelihood, we concentrate out the $\Sigma_\eta$ term by using its estimator,
\[
\widehat{\Sigma}_\eta=\frac{1}{n}\sum_{i=1}^n(Aw_i-Bz_i)(Aw_i-Bz_i)'
\]
Then plug this estimator into the log-likelihood function to get
\[
C - \frac{n}{2}\log(\det(\widehat{\Sigma}_\eta)) - \frac{1}{2}\sum_{i=1}^n(Aw_i-Bz_i)'\widehat{\Sigma}_\eta^{-1}(Aw_i-Bz_i)
\]
Since the last term is a scalar, we can take
\[
(Aw_i-Bz_i)'\widehat{\Sigma}_\eta^{-1}(Aw_i-Bz_i)=tr\left((Aw_i-Bz_i)'\widehat{\Sigma}_\eta^{-1}(Aw_i-Bz_i)\right)
\]
Also, using the fact that $tr(AB)=tr(BA)$, 
\[
tr\left((Aw_i-Bz_i)'\widehat{\Sigma}_\eta^{-1}(Aw_i-Bz_i)\right)=tr\left(\widehat{\Sigma}_\eta^{-1}(Aw_i-Bz_i)(Aw_i-Bz_i)'\right)
\]
This reduced the likelihood function to
\small{\[
C - \frac{n}{2}\log(\det(\widehat{\Sigma}_\eta)) - \frac{1}{2}tr\left(\widehat{\Sigma}_\eta^{-1}\sum_{i=1}^n(Aw_i-Bz_i)(Aw_i-Bz_i)'\right) = C-\frac{n}{2}\log(\det(\widehat{\Sigma}_\eta))-\frac{n(k_2+1)}{2}
\]}\normalsize
\item \textbf{Maximize}: The only thing left to maximize over now is $-\frac{n}{2}\log(\det(\widehat{\Sigma}_\eta))$. This is equivalent to maximizing
\footnotesize{\[
-\det\left(\frac{1}{n}\sum_{i=1}^n(Aw_i-Bz_i)'(Aw_i-Bz_i)\right)
=
-\det\left(\frac{1}{n}\sum_{i=1}^n\begin{pmatrix} y_ i - \beta_1'x_{1i}-\beta_2x_{2i} \\ x_{2i}-\Gamma_{12}'x_{1i}-\Gamma_{22}'z_{2i} \end{pmatrix}\begin{pmatrix} y_ i - \beta_1'x_{1i}-\beta_2x_{2i} \\ x_{2i}-\Gamma_{12}'x_{1i}-\Gamma_{22}'z_{2i} \end{pmatrix}'\right)
\]}\normalsize
The combination of $\beta$ and $\Gamma$ that maximizes this is the LIML estimator. \footnote{Further steps involve heavy amounts of algebra involving multi-dimensional matrices. For details, I will have you refer to the recitation notes from Dong Woo last year, which I will upload.}
\end{enumerate}
\section{$k$-class Estimators}
Another way to compute the LIML estimator is to use a $k$-class estimator with a particular choice for $k$. In general, $k$-class estimator is defined as 
\[
\hat{\beta}_k=\arg\min_\beta(y-X\beta)'(I_n-kM_Z)(y-X\beta)
\]
where $M_Z = I-Z(Z'Z)^{-1}Z'=I-P_Z$. The other way to express this, after some matrix differentiation, is 
\begin{gather*}
-2X'y+2kX'M_Zy+2(X'X)\beta-2k(X'M_ZX)\beta=0\\
\iff (X'(I_n-kM_Z)X)\beta=X'(I_n-kM_Z)y\\
\implies \hat{\beta}_k= (X'(I_n-kM_Z)X)^{-1}(X'(I_n-kM_Z)y)
\end{gather*}
In fact, we can show that OLS $(k=0)$ and 2SLS $(k=1)$ are $k$-class estimators. LIML is a $k$-class estimator with a parameter choice of some $k>1$. What we need to do find the associated value of $k$. To do so, define
\[
W=(y\ \ X_2)\in \mathbb{R}^{n\times (k_2+1)}, \ M_1 = I- X_1(X_1'X_1)^{-1}X_1'
\]
Then we compute the minimum eigenvalue of \footnote{This is shown in Anderson and Rubin (1949) and Amemiya (1985)}
\[
(W'M_1W)(W'M_ZW)^{-1}
\]
This minimum eigenvalue will be our choice of $k$, which will be denoted as $\hat{k}$, and the LIML estimator would be
\[
\hat{\beta}_{LIML}=(X'(I_n-\hat{k}M_Z)X)^{-1}(X'(I_n-\hat{k}M_Z)y)
\]
From here, we can also find that LIML is also a type of an IV estimator. We can see that by rewriting above equation as
\[
\hat{\beta}_{LIML}=(\tilde{X}'X)^{-1}(\tilde{X}'y)
\]
where $\tilde{X}=(I_n-\hat{k}M_Z)X$. This also hints that the asymptotic properties of LIML and some other types of IV estimators are similar. 
\section{Asymptotics of LIML}
Given the above definition of $\hat{\beta}_{LIML}=(X'(I_n-\hat{k}M_Z)X)^{-1}(X'(I_n-\hat{k}M_Z)y)$, we can use this to study the asymptotic properties of LIML estimators. Note that
\[
\begin{aligned}
\hat{\beta}_{LIML}-\beta &= (X'(I_n-\hat{k}M_Z)X)^{-1}(X'(I_n-\hat{k}M_Z)e)\\
& =(X'(P_Z-(\hat{k}-1))M_Z)X)^{-1}(X'(P_Z-(\hat{k}-1))e) \\
& (\because I-M_Z=P_Z, \implies I-\hat{k}M_Z=P_Z-(\hat{k}-1)M_Z)\\
 \implies \sqrt{n}(\hat{\beta}_{LIML}-\beta)&=\left(\frac{X'P_ZX}{n}-(\hat{k}-1)\frac{X'M_ZX}{n}\right)^{-1}\left(\frac{X'P_ZX}{\sqrt{n}}-(\hat{k}-1)\frac{X'M_Ze}{\sqrt{n}}\right)\\
\end{aligned}
\]
Note that
\begin{itemize}
\item Anderson and Rubin (1949) shows that $\hat{k}-1\xrightarrow{p}0$, which we accept as given. 
\item $\frac{X'M_ZX}{n}=\frac{X'X}{n}-\frac{X'P_ZX}{n}=\frac{X'X}{n}-\frac{X'P_Z'P_ZX}{n}=\frac{X'X}{n}-\frac{(P_ZX)'(P_ZX)}{n}\leq\frac{X'X}{n}\xrightarrow{p}E(x_ix_i')$. Therefore, $\frac{X'M_ZX}{n}$ is bounded (and thus $O_p(1)$)
\item $\frac{X'M_Ze}{\sqrt{n}}$ converges in distribution to a normal distribution (CLT), so it is $O_p(1)$
\item From the third and first points, we can infer that $(\hat{k}-1)\frac{X'M_Ze}{\sqrt{n}}=o_p(1)$
\end{itemize}\par
Combining these leads to the result that 
\[
 \sqrt{n}(\hat{\beta}_{LIML}-\beta) =\left(\frac{X'P_ZX}{n}\right)^{-1}\frac{X'P_ZX}{\sqrt{n}} +o_p(1)
\]
which is equivalent to $ \sqrt{n}(\hat{\beta}_{2SLS}-\beta) +o_p(1)$. So for a fixed $n$, the 2SLS and LIML converge to a similar limiting distribution. However, when either instrument is weakly related to $X$ or large in numbers, 2SLS bias becomes large, giving advantage to the LIML\footnote{This is the key takeaway from Dr. Pischke's slides. }.
\section{Control Function Approach}
This is another way to derive a 2SLS estimator. As before, I will assume $E(x_{1i}e_i)=0, E(x_{2i}e_i)\neq0$ and write the structural and reduced form regression as
\begin{gather*}
y_i = x_{1i}'\beta_1 + x_{2i}'\beta_2+e_i \tag{Structural}\\
x_{2i}=\Gamma_{12}'x_{1i}+\Gamma_{22}'z_{2i}+u_{2i} \tag{Reduced Form}\\
\end{gather*}
Also, we have a $z_i\in( x_{1i}\  z_{2i})'\in\mathbb{R}^l$ that satisfies $E(z_ie_i)=0$. The key driving idea for the control function method is that we can write $E(x_{2i}e_i)$ differently. Specifically, 
\[
\begin{aligned}
E[x_{2i}e_i]&= E[(\Gamma_{12}'x_{1i}+\Gamma_{22}'\beta_2+u_{2i})e_i]\\
&= \Gamma_{12}'E(x_{1i}e_i)+\Gamma_{22}'E(z_{2i}e_i)+E(u_{2i}e_i)\\
&= E(u_{2i}e_i) \ (\because \text{The first by exogeneity, the second by IV conditions})\\
&\therefore E[x_{2i}e_i]\neq 0 \iff E[u_{2i}e_i]\neq 0 
\end{aligned}
\]\par
From here, we consider a linear projection of $e_i$ onto $u_{2i}$, which we write as
\[
e_i  = u_{2i}'\alpha+\epsilon_i \tag{LP}
\]
where $E(u_{2i}\epsilon_i)=0$ and the population analogue of $\alpha=E(u_{2i}u_{2i}')^{-1}E(u_{2i}e_i)$. What we now do is to substitute the $e_i$ term in the (Structural) equation with the (LP) equation to get
\[
y_i = x_{1i}'\beta_1 + x_{2i}'\beta_2+ u_{2i}'\alpha+\epsilon_i   \tag{CFA}
\]
where the following are satisfied
\[
\begin{aligned}
E[x_{1i}\epsilon_i]&=E[x_{1i}(e_i-u_{2i}'\alpha)]=0-0=0 \\
E[u_{2i}\epsilon_i]&=E[u_{2i}(e_i-u_{2i}'\alpha)]\\
&=E[u_{2i}e_i]-E[u_{2i}u_{2i}']E[u_{2i}u_{2i}']^{-1}E[u_{2i}e_i]=0 \\
E[x_{2i}\epsilon_i]&=E[x_{2i}(e_i-u_{2i}'\alpha)]=E[x_{2i}e_i]-E[x_{2i}u_{2i}']\alpha \\
&=E[u_{2i}e_i]-\Gamma_{12}'E[x_{1i}u_{2i}']\alpha-\Gamma_{22}'E[z_{2i}u_{2i}']\alpha-E[u_{2i}u_{2i}']\alpha\\
&=E[u_{2i}e_i]-0-0-E[u_{2i}e_i]=0 \\
&(\because\text{LS assumption from Reduced Form, Recitation 2})\\
\end{aligned}
\]
So the key takeaway is that now $x_{2i}$ is exogenous with $\epsilon_i$. In words, $x_{2i}$ is correlated with $e_i$ through $u_{2i}$ and $\epsilon_i$ is the error after $e_i$ has been projected onto $u_{2i}$.  \par
If we know the true $u_{2i}$, the (CFA) equation can be regressed with an OLS straight away. This is not usually the case, so we need to use the residual $\hat{u}_{2i}$ which we can get from the (Reduced Form) equation. We work in these steps
\begin{enumerate}
\item \textbf{Obtain $\hat{u}_{2i}$}: This is done by regressing (Reduced Form) equation. 
\item \textbf{Work with (CFA)}: However, instead of $u_{2i}$, use $\hat{u}_{2i}$. Then we can run an OLS on the REWRITTEN (CFA) equation. 
\end{enumerate}\par
Once this is done, we can show that the estimates from control function approach is numerically identical to 2SLS.\footnote{A more detailed explanation can be found in Hansen (2019) pages 426-427.}. 
One thing to note is that with this setup, we can conduct a test of endogeneity. Formally we want to test
\[
H_0: E(x_{2i}e_i)=0, \ H_1:E(x_{2i}e_i)\neq0
\]
If it is the case that $E(x_{2i}e_i)=0$, then $E(u_{2i}e_i)=0$. Then, by how we constructed $\alpha$, this implies that $\alpha=0$. We can then show that the Wald statistics, under $H_0$, is distributed as
\[
\hat{\alpha}'(var(\hat{\alpha}))^{-1}\hat{\alpha}\xrightarrow{d}\chi^2_{k_2}
\]
To test the null against the alternative hypothesis, define $C_{1-\alpha}$ as $\Pr(\chi_{k_2}^2\leq C_{1-\alpha})=1-\alpha$. Then, we can reject the $H_0$ hypothesis if $\hat{\alpha}'(var(\hat{\alpha}))^{-1}\hat{\alpha}>C_{1-\alpha}$.

\section{Hausmann Tests for Specification}
Hausmann Test can be utilized as a general test of specification. In this context, we are interested in using Hausmann Tests for exogeneity / endogeneity of the regressors. Assume the following data generating process
\[
y_i = x_{1i}'\beta_1 + x_{2i}'\beta_2+e_i
\]
and we are interested in checking the exogeneity / endogeneity of $x_{2i}$. So we test $H_0: E(x_{2i}e_i)=0$ against $H_1:E(x_{2i}e_i)\neq0$. Consider these properties of 2SLS and OLS estimators
\begin{itemize}
\item $\hat{\beta}_{OLS}$: Consistent and minimal variance under $H_0$, inconsistent under $H_1$
\item $\hat{\beta}_{2SLS}$:  Consistent in either $H_0$ or $H_1$. Inefficient under $H_0$. 
\end{itemize}
Under $H_0$, $\sqrt{n}(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS})$ converges in distribution to $N(0, var(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS}))$. Hausmann also shows that in $H_0$, $var(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS})=var(\hat{\beta}_{2SLS})-var(\hat{\beta}_{OLS})$ holds in general (you are asked to show this for a scalar case). Given this, we can write the Hausmann test statistic as 
\[
(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS})'(var(\hat{\beta}_{2SLS})-var(\hat{\beta}_{OLS}))^-(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS})\xrightarrow{d}\chi_{k_2}^2
\]
where $(var(\hat{\beta}_{2SLS})-var(\hat{\beta}_{OLS}))^-$ indicates a generalized inverse. We can use the test statistic, construct a critical value and reject/not reject $H_0$ based on comparison.\par
Note that  generalized inverse is required since $(var(\hat{\beta}_{2SLS})-var(\hat{\beta}_{OLS}))$ is usually not a full (column) rank matrix. The following trick is useful in showing this.
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{lemma}[Breakdown of OLS coefficients, from Hansen (2019): pp. 80-82]
The OLS estimator for $\beta_1, \beta_2$ in $y=X_1\beta_1 + X_2\beta_2+e$ can be written as
\[
\hat{\beta}_1=(X_1'M_2X_1)^{-1}(X_1'M_2y),\ \hat{\beta}_2=(X_2'M_1X_2)^{-1}(X_2'M_1y)
\]
where $M_1 = I-X_1(X_1'X_1)^{-1}X_1'$ and $M_2$ is defined similarly.
\begin{proof}
I will provide some sketches of this proof. Suppose that we want to find $\beta_1$'s OLS estimate. Then the problem is identical to
\[
\hat{\beta}_1 = \arg\min_{\beta_1}(\min_{\beta_2}SSE(\beta_1, \beta_2))
\]
In words, we concentrate out $\beta_2$ and express it as a function of $\beta_1$. We put this expression back into the original equation and find $\beta_1$ from there. As a result, we can get that
\[
\beta_2 = (X_2'X_2)^{-1}(X_2'(y-X_1\beta_1))
\]
Put this back in to obtain the OLS estimation for $\beta_1$, which is
\[
\hat{\beta}_1=(X_1'M_2X_1)^{-1}(X_1'M_2y)
\]
It is also useful to note that $M_1$ is symmetric and idempotent. 
\end{proof}
\end{lemma}
\end{mdframed}
In our context, $\hat{\beta}_{1,OLS}$ would be $(X_1'X_1)^{-1}(X_1'(y-X_2\hat{\beta}_{2,OLS}))$ and its 2SLS estimator is \\ $(\widehat{X}_1'\widehat{X}_1)^{-1}(\widehat{X}_1'(y-\widehat{X}_2\hat{\beta}_{2,2SLS}))$. Since $Z=[X_1 \ Z_2]'\in\mathbb{R}^{n\times l }$, we can write that $X_1=Z\Gamma$ with $\Gamma=\begin{pmatrix}I_{k_1}\\ 0\end{pmatrix}\in\mathbb{R}^{l \times k_1}$ . So we can write $\widehat{X}_1 = Z(Z'Z)^{-1}Z'X_1=Z\Gamma=X_1$. Therefore, 
\[
\begin{aligned}
\hat{\beta}_{1,OLS}-\hat{\beta}_{1,2SLS}& =(X_1'X_1)^{-1}X_1'(\widehat{X}_2\hat{\beta}_{2,2SLS}-X_2\hat{\beta}_{2,OLS})\\
&=(X_1'X_1)^{-1}X_1'(P_ZX_2\hat{\beta}_{2,2SLS}-X_2\hat{\beta}_{2,OLS})\\
&=(X_1'X_1)^{-1}X_1'P_ZX_2\hat{\beta}_{2,2SLS}-(X_1'X_1)^{-1}X_1'X_2\hat{\beta}_{2,OLS})\\
&=(X_1'X_1)^{-1}X_1'X_2\hat{\beta}_{2,2SLS}-(X_1'X_1)^{-1}X_1'X_2\hat{\beta}_{2,OLS} \ (\because X_1'P_Z=(P_ZX_1)'=X_1')\\
&=(X_1'X_1)^{-1}X_1'X_2(\hat{\beta}_{2,2SLS}-\hat{\beta}_{2,OLS})\\
\end{aligned}
\]
So $\hat{\beta}_{1,OLS}-\hat{\beta}_{1,2SLS}$ is merely obtained by linearly transforming $\hat{\beta}_{2,OLS}-\hat{\beta}_{2,2SLS}$. Therefore, $var(\hat{\beta}_{2SLS}-\hat{\beta}_{OLS})$ is not full rank. 
\section{Subset Endogeneity Test}
This section can be seen as an extension to the endogeneity test we showed earlier. Instead, we break down $x_{2i}$ into two parts - one that is `potentially' endogenous ($x_{2i}$) and one that is endogenous for sure ($x_{3i}$). Then, we again test for  $H_0: E(x_{2i}e_i)=0$ against $H_1:E(x_{2i}e_i)\neq0$. We now go over how control function approach can be utilized in this context. We have the set of required equations as follows
\begin{gather*}
y_i = x_{1i}'\beta_1 + x_{2i}'\beta_2+x_{3i}'\beta_3+e_i \tag{Structural}\\
x_{2i}=\Gamma_{2}'z_{i}+u_{2i} \tag{Reduced Form 2}\\
x_{3i}=\Gamma_{3}'z_{i}+u_{3i} \tag{Reduced Form 3}\\
\end{gather*}
We then project $e_i$ onto $u_{2i}, u_{3i}$ to obtain
\[
e_i = u_{2i}'\alpha_2 + u_{3i}'\alpha_3 + \epsilon_i \tag{LP2}
\]
where the following is satisfied
\[
\begin{pmatrix}E(u_{2i}u_{2i}') & E(u_{2i}u_{3i}')\\E(u_{3i}u_{2i}') & E(u_{3i}u_{3i}')\end{pmatrix}\begin{pmatrix} \alpha_2 \\ \alpha_3\end{pmatrix} = \begin{pmatrix} E(u_{2i}e_i) \\ E(u_{3i}e_i)\end{pmatrix}
\]
then the control function approach would require us to apply an OLS to the following
\[
y_i = x_{1i}'\beta_1 + x_{2i}'\beta_2+x_{3i}'\beta_3+ u_{2i}'\alpha_2 + u_{3i}'\alpha_3 + \epsilon_i \tag{CFA2}
\]
As before, $E(x_{2i}e_i)=0$ implies $E(u_{2i}e_i)=0$. This allows us to replace the $H_0$ with
\[
H_0: E[u_{2i}u_{2i}']\alpha_2+E[u_{2i}u_{3i}']\alpha_3=0
\]
This is a special case of $R\beta=c$ type of hypothesis test studied in Recitation 1. However, since we cannot usually know what the true value of $u_{2i}, u_{3i}$ are, we use the residuals, and replace $R$ with $\widehat{R}$ constructed from the sample analogue using residuals. 
\section{Overidentification Test}
Consider the following setup
\[
y_i = x_i'\beta+e_i, \ (\dim(x_i)=k<\dim(z_i)=l) 
\]
where we have an overidentified model, with number of instruments being larger than endogenous variables. We can test for the validity of the instruments by testing $H_0: E(z_ie_i)=0$ vs. $H_1: E(z_ie_i)\neq0$. To do this, we construct a linear projection equation by projecting $e_i$ onto $z_i$, obtaining
\[
e_i=z_i'\alpha+\epsilon_i
\]
Thus, $\alpha=E(z_iz_i')^{-1}E(z_ie_i)$. Because of this, the null and alternative hypotheses can be expressed by $H_0: \alpha=0$ vs. $H_1: \alpha\neq0$. This leaves us with the challenge of estimating $\alpha$.  Since we do not know what the true value of the error terms are, we take the following steps.
\begin{enumerate}
\item \textbf{Obtain $\hat{e}_i$}: This can be done using 2SLS estimates of $\beta$. As a result, 
\[
\hat{e}_i = y_i-x_i'\hat{\beta}_{2SLS} \implies \hat{e}=y-X\hat{\beta}_{2SLS}
\]
\item \textbf{Obtain $\hat{\alpha}$}: Replace $e$ with $\hat{e}$ to get
\[
\hat{\alpha} = (Z'Z)^{-1}Z'\hat{e}
\]
\item \textbf{Sargan Test}: We will assume homoskedasticity (otherwise, the test statistic does not converge to $\chi^2$ distribution). Then we make use of the following test statistic
\[
S=\hat{\alpha}'(var(\hat{\alpha}))^{-}\hat{\alpha}=\frac{\hat{e}'Z(Z'Z)^{-1}Z'\hat{e}}{\hat{\sigma}^2}
\]
where $\hat{\sigma}^2$ can be obtained from two paths - one using $\frac{1}{n}\hat{e}'\hat{e}$ and the other using $\frac{1}{n}\hat{\epsilon}'\hat{\epsilon}=\frac{1}{n}(\hat{e}-Z\hat{\alpha})'(\hat{e}-Z\hat{\alpha})$. While they are slightly different in that the first estimate has larger variances\footnote{It can be shown that $\hat{\epsilon}'\hat{\epsilon}=\hat{e}'\hat{e}-\hat{e}'Z(Z'Z)^{-1}Z'\hat{e}$ Since $\hat{e}\xrightarrow{p}e$ and $E(z_ie_i)=0$, it can be shown that $\hat{e}'Z(Z'Z)^{-1}Z'\hat{e}$ goes to 0.}, they are asymptotically equal. Under the null, $S\xrightarrow{d}\chi_{l-k}^2$
\end{enumerate}
Some note of caution include:
\begin{itemize}
\item Even if the null hypothesis is rejected, the test alone is not enough to pick up which variable is violating the exclusion restriction.
\item Since $E(z_ie_i)=0\iff E(z_iy_i)-E(z_ix_i')\beta=0$, we are effectively solving a system of $l\times 1$ restrictions, despite $\beta$ being a $k$-vector. Depending on the subset of $z_i$ vectors used, the $\beta$ may differ in finite samples. If the overidentification hypothesis is correct, the estimates that are different in finite samples are consistent for true $\beta$. You can think of the overidentification test as an attempt to see whether different $\beta$ estimates are converging to a same parameter as $n\to\infty$ 
\end{itemize}
\section{Weak Instrument Test}
Assume that the structural and reduced form equations are (we are working with a scalar regressors)
\begin{gather*}
y_i = x_i\beta+ e_i \\ x_i=z_i\gamma+u_i
\end{gather*}
We say that there is a problem of weak instrument if $\gamma\simeq0$. This can cause the 2SLS estimates to be biased and the distribution to be affected. Specifically, let $\gamma=\frac{\mu}{\sqrt{n}}$. For simplicity, I will assume
\begin{itemize}
\footnotesize{\item $var\left(\begin{pmatrix}e_i \\ u_i \end{pmatrix}| z_i\right)=\begin{pmatrix}1 & \rho \\ \rho &1\end{pmatrix}=\Sigma$
\item $E(z_i^2)=1$
\item $\frac{1}{\sqrt{n}}\sum_{i=1}^n\begin{pmatrix}z_ie_i \\ z_iu_i\end{pmatrix}\xrightarrow{d}\begin{pmatrix}\xi_1 \\ \xi_2\end{pmatrix}=N(0,\Sigma)$}\normalsize
\end{itemize}
Now I will show that neither OLS nor IV estimator is not consistent. 
\begin{itemize}
\item OLS: Note that $\hat{\beta}_{OLS}-\beta$ can be written as $\frac{n^{-1}\sum_{i=1}^n x_ie_i}{n^{-1}\sum_{i=1}^nx_i^2}$, equivalent to
\footnotesize{\[
\begin{aligned}
\frac{n^{-1}\sum_{i=1}^n x_ie_i}{n^{-1}\sum_{i=1}^nx_i^2}&=\frac{n^{-1}\sum_{i=1}^n (z_i\gamma+u_i)e_i}{n^{-1}\sum_{i=1}^n(z_i\gamma+u_i)^2}\\
&=\frac{n^{-1}\sum_{i=1}^n (z_ie_i\gamma+u_ie_i)}{n^{-1}\sum_{i=1}^n(z_i^2\gamma^2+u_i^2+2z_iu_i\gamma)}\\
&=\frac{n^{-1}\sum_{i=1}^n u_ie_i}{n^{-1}\sum_{i=1}^nu_i^2}+o_p(1)\\
&\xrightarrow{p}E(u_ie_i)/E(u_i^2)=\rho\\
\end{aligned}
\]}\normalsize
\item IV: For $\hat{\beta}_{IV}-\beta$, we can write $\frac{n^{-1/2}\sum_{i=1}^n z_ie_i}{n^{-1/2}\sum_{i=1}^nz_ix_i}$ or equivalently
\footnotesize{\[
\begin{aligned}
\hat{\beta}_{IV}-\beta&=\frac{n^{-1/2}\sum_{i=1}^n z_ie_i}{n^{-1/2}\sum_{i=1}^nz_i(z_i\gamma+u_i)}\\
&=\frac{n^{-1/2}\sum_{i=1}^n z_ie_i}{n^{-1/2}\sum_{i=1}^nz_iu_i+n^{-1}\sum_{i=1}^n z_i^2\mu}\xrightarrow{d}\frac{\xi_1}{\xi_2+\mu}
\end{aligned}
\]}\normalsize
which is not centered at 0, making it inconsistent. 
\end{itemize}\par
In STATA, such test can be implemented by typing in \texttt{estat firststage} after 2SLS or LIML regression. 

\section{Problem with Many IVs}
In some instances, we may work with ``many" IVs. This indicates a situation where the number of IV is large relative to the sample size. To formalize the idea, Bekker (1994)\footnote{Bekker, Paul A. (1994),  ``Alternative Approximations to the Distributions of Instrumental Variable Estimators", Econometrica 62(3), 657-681} introduces an asymptotic approximation which treats the number of instruments $l$ as proportional to the sample size, so that $l=\alpha n$. This is equivalent to 
\[
l/n \to \alpha 
\]
When $\alpha$ is not zero, then it can be said that this setup has many IVs. This could cause the 2SLS estimators to be inconsistent as well. \par
Consider the setup where $x_i$ is endogenous and is a scalar. 
\begin{gather*}
y_ i = x_i'\beta+e_i \iff Y=X\beta+e \\
x_i = z_i'\beta+u_i \iff X=Z\Gamma+u \ \ (z_i \in \mathbb{R}^l)
\end{gather*}
To make the analysis more simpler, I assume that $z_i$ is still a valid IV (relevant, exogenous) and that $var\begin{pmatrix} e_i \\ u_i \end{pmatrix} = \begin{pmatrix}1 & \rho \\ \rho & 1 \end{pmatrix} = \Sigma$. In addition, note that $var(x_i) = var(z_i'\gamma)+var(u_i)$ and assume that
\[
\frac{1}{n}\sum_{i=1}^n \gamma'z_iz_i'\gamma\xrightarrow{p}c>0
\]
and that variance of $x_i$ and $u_i$ are unchanging with respect to $l$. This implies that the variance of $var(z_i'\gamma)$ is not changing as well an that that $R^2$ of the reduced form\footnote{In this context, it would be $\frac{var(z_i'\gamma)}{var(x_i)}$}  converges to a constant. \par
To understand the behavior of some of the estimators in many IV situations, I will first start with OLS as a benchmark.
\begin{itemize}
\item OLS: We know that $\hat{\beta}_{OLS}$ can be written as
\[
\hat{\beta}_{OLS}-\beta=\left(\frac{1}{n}\sum_{i=1}^n x_ix_i'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n x_ie_i\right)
\]
Applying our setup, we can re-write this as
\[
\begin{aligned}
\frac{1}{n}\sum_{i=1}^n x_ix_i'&=\frac{1}{n}\sum_{i=1}^n \gamma'z_iz_i'\gamma+\frac{1}{n}\sum_{i=1}^n u_iu_i'+\frac{2}{n}\sum_{i=1}^n \gamma'z_iu_i'\\
&\xrightarrow{p} c+1\\
\left(\frac{1}{n}\sum_{i=1}^n x_ix_i'\right)^{-1}&\xrightarrow{p} (c+1)^{-1}\\
\frac{1}{n}\sum_{i=1}^n x_ie_i&\xrightarrow{p}\rho
\end{aligned}
\]
Therefore, 
\[
\hat{\beta}_{OLS}-\beta\xrightarrow{p}\frac{\rho}{c+1}
\]
This also captures the idea that if $x_i$ is endogenous (regardless of size), OLS estimators fail to be consistent
\item 2SLS: The 2SLS estimator can be characterized by
\[
\begin{aligned}
\hat{\beta}_{2SLS}-\beta&=(X'P_ZX)^{-1}(X'P_Ze)\\
&=[(\Gamma'Z'+u')Z(Z'Z)^{-1}Z'(Z\Gamma+u)]^{-1}[(\Gamma'Z'+u')Z(Z'Z)^{-1}Z'e]\\
&=[\frac{\Gamma'Z'Z\Gamma}{n}+\frac{\Gamma'Z'u}{n}+\frac{u'Z\Gamma}{n}+\frac{u'P_Zu}{n}]^{-1}[\frac{\Gamma'Z'e}{n}+\frac{u'P_Ze}{n}]
\end{aligned}
\]
We know from the above discussion about $\frac{1}{n}\sum_{i=1}^n \gamma'z_iz_i'\gamma$ and the properties of $Z$ that  $\frac{\Gamma'Z'Z\Gamma}{n}\xrightarrow{p}c, \frac{\Gamma'Z'u}{n}\xrightarrow{p}0, \frac{\Gamma'Z'e}{n}\xrightarrow{p}0$. We need to know what happens to the rest of them. Using the fact that $u'P_Ze \text{ and }u'P_Zu$ are scalars and the properties of the trace operators, 
\begin{gather*}
E\left[\frac{1}{n}u'P_Ze\right]=\frac{1}{n}E[tr(u'P_Ze)]=\frac{1}{n}E[tr(P_Zeu')]=\frac{1}{n}tr[E(P_Zeu')]\\
=\frac{1}{n}tr[E(P_Z)\rho]=\frac{1}{n}E[tr(P_Z)]\rho=\frac{l}{n}\rho\\
\end{gather*}
where I use the fact that $tr(E(X))=E(tr(X))$ and $tr(AB)=tr(BA)$ multiple times. In a similar fashion, 
\[
E\left[\frac{1}{n}u'P_Zu\right] = \frac{l}{n}
\]
Based on the two facts and $l/n\to\alpha$, I can make use of Markov inequality to show that $\frac{1}{n}u'P_Zu\xrightarrow{p} \frac{l}{n}\text{ and }\frac{1}{n}u'P_Ze\xrightarrow{p}\frac{l}{n}\rho$. Since $l/n\to\alpha$, I can write
\[
\frac{u'P_Ze}{n}\xrightarrow{p} \alpha\rho, \frac{u'P_Zu}{n}\xrightarrow{p} \alpha
\]
Therefore, 
\[
\hat{\beta}_{2SLS}-\beta\xrightarrow{p}\frac{\alpha\rho}{c+\alpha}
\]
If we do not have many IVs, $\alpha=0$ and 2SLS estimator is consistent. Otherwise, inconsistency of the above form occurs. 
\end{itemize}
\begin{mdframed}[backgroundcolor=blue!5] 
\begin{theorem}[Inconsistency in Large Scale IVs]
If above assumptions hold, together with $E(e_i^2|z_i)<\infty, E(u_i^4|z_i)<\infty$. Then 
\[
\hat{\beta}_{OLS}-\beta\xrightarrow{p}\frac{\rho}{c+1},\   \hat{\beta}_{2SLS}-\beta\xrightarrow{p}\frac{\alpha\rho}{c+\alpha}
\]
\end{theorem}
\end{mdframed}
Hansen (2019, pp. 447-448) shows why LIML is immune to this problem. 

